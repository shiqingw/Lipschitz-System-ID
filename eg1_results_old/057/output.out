==> torch device:  cuda:0
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (M): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 1.00
==> Start training...
Epoch: 001 | Loss: 2.6188948 | L2 loss: 2.6186949 | Lip loss: 0.0001999 | Grad norm: 15.232815 | Time: 50s261ms
Epoch: 001 | Test Loss: 0.0386471 | Time: 554ms
==> Save the model at epoch 001 with test loss 0.0386471
Epoch: 002 | Loss: 0.0227585 | L2 loss: 0.0225528 | Lip loss: 0.0002057 | Grad norm: 5.001874 | Time: 50s692ms
Epoch: 002 | Test Loss: 0.0184201 | Time: 548ms
==> Save the model at epoch 002 with test loss 0.0184201
Epoch: 003 | Loss: 0.0262323 | L2 loss: 0.0260271 | Lip loss: 0.0002052 | Grad norm: 4.268390 | Time: 51s185ms
Epoch: 003 | Test Loss: 0.1085297 | Time: 559ms
Epoch: 004 | Loss: 0.0195967 | L2 loss: 0.0193920 | Lip loss: 0.0002047 | Grad norm: 3.072307 | Time: 50s163ms
Epoch: 004 | Test Loss: 0.0155543 | Time: 564ms
==> Save the model at epoch 004 with test loss 0.0155543
Epoch: 005 | Loss: 0.0152220 | L2 loss: 0.0150178 | Lip loss: 0.0002043 | Grad norm: 2.516408 | Time: 51s2ms
Epoch: 005 | Test Loss: 0.0102005 | Time: 549ms
==> Save the model at epoch 005 with test loss 0.0102005
Epoch: 006 | Loss: 0.0077957 | L2 loss: 0.0075927 | Lip loss: 0.0002030 | Grad norm: 0.440416 | Time: 49s193ms
Epoch: 006 | Test Loss: 0.0080371 | Time: 553ms
==> Save the model at epoch 006 with test loss 0.0080371
Epoch: 007 | Loss: 0.0077376 | L2 loss: 0.0075346 | Lip loss: 0.0002030 | Grad norm: 0.457710 | Time: 46s421ms
Epoch: 007 | Test Loss: 0.0075768 | Time: 543ms
==> Save the model at epoch 007 with test loss 0.0075768
Epoch: 008 | Loss: 0.0077654 | L2 loss: 0.0075626 | Lip loss: 0.0002029 | Grad norm: 0.496882 | Time: 46s413ms
Epoch: 008 | Test Loss: 0.0081424 | Time: 540ms
Epoch: 009 | Loss: 0.0077730 | L2 loss: 0.0075704 | Lip loss: 0.0002026 | Grad norm: 0.514471 | Time: 46s966ms
Epoch: 009 | Test Loss: 0.0077306 | Time: 551ms
Epoch: 010 | Loss: 0.0077524 | L2 loss: 0.0075499 | Lip loss: 0.0002025 | Grad norm: 0.509807 | Time: 47s529ms
Epoch: 010 | Test Loss: 0.0075663 | Time: 622ms
==> Save the model at epoch 010 with test loss 0.0075663
Epoch: 011 | Loss: 0.0074386 | L2 loss: 0.0072359 | Lip loss: 0.0002027 | Grad norm: 0.292409 | Time: 47s821ms
Epoch: 011 | Test Loss: 0.0073713 | Time: 550ms
==> Save the model at epoch 011 with test loss 0.0073713
Epoch: 012 | Loss: 0.0074329 | L2 loss: 0.0072306 | Lip loss: 0.0002023 | Grad norm: 0.294514 | Time: 46s438ms
Epoch: 012 | Test Loss: 0.0073993 | Time: 540ms
Epoch: 013 | Loss: 0.0074352 | L2 loss: 0.0072329 | Lip loss: 0.0002023 | Grad norm: 0.296552 | Time: 49s93ms
Epoch: 013 | Test Loss: 0.0073950 | Time: 551ms
Epoch: 014 | Loss: 0.0074339 | L2 loss: 0.0072315 | Lip loss: 0.0002024 | Grad norm: 0.295237 | Time: 50s934ms
Epoch: 014 | Test Loss: 0.0074409 | Time: 573ms
Epoch: 015 | Loss: 0.0074394 | L2 loss: 0.0072370 | Lip loss: 0.0002024 | Grad norm: 0.312046 | Time: 51s102ms
Epoch: 015 | Test Loss: 0.0074399 | Time: 551ms
Epoch: 016 | Loss: 0.0073938 | L2 loss: 0.0071915 | Lip loss: 0.0002023 | Grad norm: 0.262506 | Time: 51s54ms
Epoch: 016 | Test Loss: 0.0073640 | Time: 615ms
==> Save the model at epoch 016 with test loss 0.0073640
Epoch: 017 | Loss: 0.0073905 | L2 loss: 0.0071884 | Lip loss: 0.0002021 | Grad norm: 0.255750 | Time: 47s205ms
Epoch: 017 | Test Loss: 0.0073614 | Time: 545ms
==> Save the model at epoch 017 with test loss 0.0073614
Epoch: 018 | Loss: 0.0073907 | L2 loss: 0.0071884 | Lip loss: 0.0002023 | Grad norm: 0.263839 | Time: 49s235ms
Epoch: 018 | Test Loss: 0.0073652 | Time: 558ms
Epoch: 019 | Loss: 0.0073890 | L2 loss: 0.0071867 | Lip loss: 0.0002024 | Grad norm: 0.261124 | Time: 48s132ms
Epoch: 019 | Test Loss: 0.0073558 | Time: 548ms
==> Save the model at epoch 019 with test loss 0.0073558
Epoch: 020 | Loss: 0.0073913 | L2 loss: 0.0071889 | Lip loss: 0.0002024 | Grad norm: 0.259081 | Time: 48s597ms
Epoch: 020 | Test Loss: 0.0073592 | Time: 563ms
Epoch: 021 | Loss: 0.0073852 | L2 loss: 0.0071828 | Lip loss: 0.0002024 | Grad norm: 0.253799 | Time: 48s208ms
Epoch: 021 | Test Loss: 0.0073547 | Time: 554ms
==> Save the model at epoch 021 with test loss 0.0073547
Epoch: 022 | Loss: 0.0073845 | L2 loss: 0.0071823 | Lip loss: 0.0002023 | Grad norm: 0.252157 | Time: 47s670ms
Epoch: 022 | Test Loss: 0.0073553 | Time: 546ms
Epoch: 023 | Loss: 0.0073842 | L2 loss: 0.0071819 | Lip loss: 0.0002023 | Grad norm: 0.245618 | Time: 47s172ms
Epoch: 023 | Test Loss: 0.0073547 | Time: 545ms
==> Save the model at epoch 023 with test loss 0.0073547
Epoch: 024 | Loss: 0.0073843 | L2 loss: 0.0071820 | Lip loss: 0.0002023 | Grad norm: 0.256950 | Time: 47s392ms
Epoch: 024 | Test Loss: 0.0073556 | Time: 552ms
Epoch: 025 | Loss: 0.0073842 | L2 loss: 0.0071820 | Lip loss: 0.0002023 | Grad norm: 0.246719 | Time: 50s863ms
Epoch: 025 | Test Loss: 0.0073543 | Time: 550ms
==> Save the model at epoch 025 with test loss 0.0073543
Epoch: 026 | Loss: 0.0073837 | L2 loss: 0.0071812 | Lip loss: 0.0002024 | Grad norm: 0.262691 | Time: 50s811ms
Epoch: 026 | Test Loss: 0.0073544 | Time: 557ms
Epoch: 027 | Loss: 0.0073834 | L2 loss: 0.0071812 | Lip loss: 0.0002022 | Grad norm: 0.250765 | Time: 50s706ms
Epoch: 027 | Test Loss: 0.0073545 | Time: 548ms
Epoch: 028 | Loss: 0.0073835 | L2 loss: 0.0071811 | Lip loss: 0.0002023 | Grad norm: 0.252356 | Time: 47s382ms
Epoch: 028 | Test Loss: 0.0073545 | Time: 539ms
Epoch: 029 | Loss: 0.0073834 | L2 loss: 0.0071812 | Lip loss: 0.0002022 | Grad norm: 0.251376 | Time: 49s869ms
Epoch: 029 | Test Loss: 0.0073546 | Time: 553ms
Epoch: 030 | Loss: 0.0073835 | L2 loss: 0.0071811 | Lip loss: 0.0002023 | Grad norm: 0.255367 | Time: 47s935ms
Epoch: 030 | Test Loss: 0.0073545 | Time: 555ms
Epoch: 031 | Loss: 0.0073833 | L2 loss: 0.0071810 | Lip loss: 0.0002023 | Grad norm: 0.256710 | Time: 50s538ms
Epoch: 031 | Test Loss: 0.0073545 | Time: 550ms
Epoch: 032 | Loss: 0.0073833 | L2 loss: 0.0071810 | Lip loss: 0.0002022 | Grad norm: 0.252520 | Time: 49s797ms
Epoch: 032 | Test Loss: 0.0073545 | Time: 553ms
Epoch: 033 | Loss: 0.0073832 | L2 loss: 0.0071810 | Lip loss: 0.0002022 | Grad norm: 0.253638 | Time: 52s896ms
Epoch: 033 | Test Loss: 0.0073545 | Time: 559ms
Epoch: 034 | Loss: 0.0073833 | L2 loss: 0.0071810 | Lip loss: 0.0002023 | Grad norm: 0.249566 | Time: 50s523ms
Epoch: 034 | Test Loss: 0.0073545 | Time: 557ms
Epoch: 035 | Loss: 0.0073834 | L2 loss: 0.0071810 | Lip loss: 0.0002023 | Grad norm: 0.252410 | Time: 50s274ms
Epoch: 035 | Test Loss: 0.0073545 | Time: 544ms
Epoch: 036 | Loss: 0.0073834 | L2 loss: 0.0071810 | Lip loss: 0.0002023 | Grad norm: 0.249267 | Time: 50s736ms
Epoch: 036 | Test Loss: 0.0073545 | Time: 551ms
Epoch: 037 | Loss: 0.0073834 | L2 loss: 0.0071810 | Lip loss: 0.0002024 | Grad norm: 0.252698 | Time: 50s73ms
Epoch: 037 | Test Loss: 0.0073545 | Time: 549ms
Epoch: 038 | Loss: 0.0073834 | L2 loss: 0.0071810 | Lip loss: 0.0002024 | Grad norm: 0.249547 | Time: 47s12ms
Epoch: 038 | Test Loss: 0.0073545 | Time: 565ms
Epoch: 039 | Loss: 0.0073833 | L2 loss: 0.0071810 | Lip loss: 0.0002023 | Grad norm: 0.253531 | Time: 51s702ms
Epoch: 039 | Test Loss: 0.0073545 | Time: 559ms
Epoch: 040 | Loss: 0.0073832 | L2 loss: 0.0071810 | Lip loss: 0.0002021 | Grad norm: 0.244170 | Time: 50s52ms
Epoch: 040 | Test Loss: 0.0073545 | Time: 609ms
Total time: 33m13s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
