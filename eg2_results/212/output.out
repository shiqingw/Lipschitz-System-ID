==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 2.5791194 | L2 loss: 2.5790187 | Lip loss: 0.0001007 | Grad norm: 4.413305 | Time: 8s393ms
Epoch: 001 | Test Loss: 0.1961161 | Time: 434ms
==> Save the model at epoch 001 with test loss 0.1961161
Epoch: 002 | Loss: 0.1749674 | L2 loss: 0.1748168 | Lip loss: 0.0001506 | Grad norm: 5.078546 | Time: 8s650ms
Epoch: 002 | Test Loss: 0.1893777 | Time: 534ms
==> Save the model at epoch 002 with test loss 0.1893777
Epoch: 003 | Loss: 0.1228014 | L2 loss: 0.1226497 | Lip loss: 0.0001517 | Grad norm: 5.625373 | Time: 8s311ms
Epoch: 003 | Test Loss: 0.0758008 | Time: 467ms
==> Save the model at epoch 003 with test loss 0.0758008
Epoch: 004 | Loss: 0.0825571 | L2 loss: 0.0824053 | Lip loss: 0.0001518 | Grad norm: 4.856551 | Time: 8s396ms
Epoch: 004 | Test Loss: 0.0747162 | Time: 439ms
==> Save the model at epoch 004 with test loss 0.0747162
Epoch: 005 | Loss: 0.0552875 | L2 loss: 0.0551361 | Lip loss: 0.0001514 | Grad norm: 4.613204 | Time: 8s243ms
Epoch: 005 | Test Loss: 0.0496935 | Time: 450ms
==> Save the model at epoch 005 with test loss 0.0496935
Epoch: 006 | Loss: 0.0261042 | L2 loss: 0.0259573 | Lip loss: 0.0001469 | Grad norm: 1.496245 | Time: 8s304ms
Epoch: 006 | Test Loss: 0.0208983 | Time: 465ms
==> Save the model at epoch 006 with test loss 0.0208983
Epoch: 007 | Loss: 0.0216714 | L2 loss: 0.0215241 | Lip loss: 0.0001473 | Grad norm: 1.200345 | Time: 8s271ms
Epoch: 007 | Test Loss: 0.0181790 | Time: 450ms
==> Save the model at epoch 007 with test loss 0.0181790
Epoch: 008 | Loss: 0.0196003 | L2 loss: 0.0194546 | Lip loss: 0.0001456 | Grad norm: 1.376037 | Time: 8s229ms
Epoch: 008 | Test Loss: 0.0164044 | Time: 460ms
==> Save the model at epoch 008 with test loss 0.0164044
Epoch: 009 | Loss: 0.0179248 | L2 loss: 0.0177758 | Lip loss: 0.0001490 | Grad norm: 1.325229 | Time: 8s674ms
Epoch: 009 | Test Loss: 0.0156464 | Time: 437ms
==> Save the model at epoch 009 with test loss 0.0156464
Epoch: 010 | Loss: 0.0161951 | L2 loss: 0.0160537 | Lip loss: 0.0001414 | Grad norm: 1.175256 | Time: 8s562ms
Epoch: 010 | Test Loss: 0.0138969 | Time: 448ms
==> Save the model at epoch 010 with test loss 0.0138969
Epoch: 011 | Loss: 0.0145815 | L2 loss: 0.0144295 | Lip loss: 0.0001520 | Grad norm: 0.675495 | Time: 8s428ms
Epoch: 011 | Test Loss: 0.0133778 | Time: 440ms
==> Save the model at epoch 011 with test loss 0.0133778
Epoch: 012 | Loss: 0.0143829 | L2 loss: 0.0142359 | Lip loss: 0.0001470 | Grad norm: 0.673480 | Time: 8s430ms
Epoch: 012 | Test Loss: 0.0133777 | Time: 441ms
==> Save the model at epoch 012 with test loss 0.0133777
Epoch: 013 | Loss: 0.0142313 | L2 loss: 0.0140823 | Lip loss: 0.0001490 | Grad norm: 0.649743 | Time: 8s283ms
Epoch: 013 | Test Loss: 0.0132204 | Time: 507ms
==> Save the model at epoch 013 with test loss 0.0132204
Epoch: 014 | Loss: 0.0140700 | L2 loss: 0.0139220 | Lip loss: 0.0001480 | Grad norm: 0.654130 | Time: 8s305ms
Epoch: 014 | Test Loss: 0.0130039 | Time: 446ms
==> Save the model at epoch 014 with test loss 0.0130039
Epoch: 015 | Loss: 0.0139332 | L2 loss: 0.0137919 | Lip loss: 0.0001413 | Grad norm: 0.656483 | Time: 8s348ms
Epoch: 015 | Test Loss: 0.0129628 | Time: 442ms
==> Save the model at epoch 015 with test loss 0.0129628
Epoch: 016 | Loss: 0.0137888 | L2 loss: 0.0136439 | Lip loss: 0.0001448 | Grad norm: 0.610124 | Time: 8s277ms
Epoch: 016 | Test Loss: 0.0128747 | Time: 522ms
==> Save the model at epoch 016 with test loss 0.0128747
Epoch: 017 | Loss: 0.0137567 | L2 loss: 0.0136129 | Lip loss: 0.0001439 | Grad norm: 0.571240 | Time: 8s388ms
Epoch: 017 | Test Loss: 0.0128570 | Time: 437ms
==> Save the model at epoch 017 with test loss 0.0128570
Epoch: 018 | Loss: 0.0138811 | L2 loss: 0.0137365 | Lip loss: 0.0001446 | Grad norm: 0.581950 | Time: 8s437ms
Epoch: 018 | Test Loss: 0.0128525 | Time: 439ms
==> Save the model at epoch 018 with test loss 0.0128525
Epoch: 019 | Loss: 0.0137379 | L2 loss: 0.0135923 | Lip loss: 0.0001455 | Grad norm: 0.595324 | Time: 8s373ms
Epoch: 019 | Test Loss: 0.0128383 | Time: 440ms
==> Save the model at epoch 019 with test loss 0.0128383
Epoch: 020 | Loss: 0.0137312 | L2 loss: 0.0135758 | Lip loss: 0.0001554 | Grad norm: 0.582453 | Time: 8s229ms
Epoch: 020 | Test Loss: 0.0128264 | Time: 445ms
==> Save the model at epoch 020 with test loss 0.0128264
Epoch: 021 | Loss: 0.0137030 | L2 loss: 0.0135540 | Lip loss: 0.0001490 | Grad norm: 0.585797 | Time: 8s237ms
Epoch: 021 | Test Loss: 0.0128219 | Time: 438ms
==> Save the model at epoch 021 with test loss 0.0128219
Epoch: 022 | Loss: 0.0136939 | L2 loss: 0.0135447 | Lip loss: 0.0001492 | Grad norm: 0.590690 | Time: 8s503ms
Epoch: 022 | Test Loss: 0.0128205 | Time: 451ms
==> Save the model at epoch 022 with test loss 0.0128205
Epoch: 023 | Loss: 0.0137004 | L2 loss: 0.0135515 | Lip loss: 0.0001489 | Grad norm: 0.598289 | Time: 8s828ms
Epoch: 023 | Test Loss: 0.0128196 | Time: 454ms
==> Save the model at epoch 023 with test loss 0.0128196
Epoch: 024 | Loss: 0.0136884 | L2 loss: 0.0135432 | Lip loss: 0.0001452 | Grad norm: 0.583959 | Time: 8s459ms
Epoch: 024 | Test Loss: 0.0128177 | Time: 519ms
==> Save the model at epoch 024 with test loss 0.0128177
Epoch: 025 | Loss: 0.0136940 | L2 loss: 0.0135434 | Lip loss: 0.0001506 | Grad norm: 0.576309 | Time: 8s343ms
Epoch: 025 | Test Loss: 0.0128167 | Time: 469ms
==> Save the model at epoch 025 with test loss 0.0128167
Epoch: 026 | Loss: 0.0136899 | L2 loss: 0.0135428 | Lip loss: 0.0001471 | Grad norm: 0.555545 | Time: 8s477ms
Epoch: 026 | Test Loss: 0.0128167 | Time: 435ms
==> Save the model at epoch 026 with test loss 0.0128167
Epoch: 027 | Loss: 0.0136805 | L2 loss: 0.0135353 | Lip loss: 0.0001451 | Grad norm: 0.580106 | Time: 8s373ms
Epoch: 027 | Test Loss: 0.0128166 | Time: 518ms
==> Save the model at epoch 027 with test loss 0.0128166
Epoch: 028 | Loss: 0.0136867 | L2 loss: 0.0135400 | Lip loss: 0.0001467 | Grad norm: 0.560317 | Time: 8s588ms
Epoch: 028 | Test Loss: 0.0128165 | Time: 439ms
==> Save the model at epoch 028 with test loss 0.0128165
Epoch: 029 | Loss: 0.0136877 | L2 loss: 0.0135434 | Lip loss: 0.0001444 | Grad norm: 0.583776 | Time: 8s574ms
Epoch: 029 | Test Loss: 0.0128164 | Time: 438ms
==> Save the model at epoch 029 with test loss 0.0128164
Epoch: 030 | Loss: 0.0136860 | L2 loss: 0.0135396 | Lip loss: 0.0001463 | Grad norm: 0.604074 | Time: 8s617ms
Epoch: 030 | Test Loss: 0.0128164 | Time: 468ms
==> Save the model at epoch 030 with test loss 0.0128164
Epoch: 031 | Loss: 0.0136818 | L2 loss: 0.0135359 | Lip loss: 0.0001459 | Grad norm: 0.564942 | Time: 8s286ms
Epoch: 031 | Test Loss: 0.0128164 | Time: 454ms
Epoch: 032 | Loss: 0.0136966 | L2 loss: 0.0135484 | Lip loss: 0.0001482 | Grad norm: 0.589901 | Time: 8s549ms
Epoch: 032 | Test Loss: 0.0128164 | Time: 443ms
==> Save the model at epoch 032 with test loss 0.0128164
Epoch: 033 | Loss: 0.0136847 | L2 loss: 0.0135363 | Lip loss: 0.0001484 | Grad norm: 0.586565 | Time: 8s537ms
Epoch: 033 | Test Loss: 0.0128164 | Time: 467ms
Epoch: 034 | Loss: 0.0136874 | L2 loss: 0.0135428 | Lip loss: 0.0001446 | Grad norm: 0.574262 | Time: 8s943ms
Epoch: 034 | Test Loss: 0.0128164 | Time: 454ms
Epoch: 035 | Loss: 0.0136899 | L2 loss: 0.0135406 | Lip loss: 0.0001493 | Grad norm: 0.536934 | Time: 8s420ms
Epoch: 035 | Test Loss: 0.0128164 | Time: 519ms
==> Save the model at epoch 035 with test loss 0.0128164
Epoch: 036 | Loss: 0.0138043 | L2 loss: 0.0136572 | Lip loss: 0.0001471 | Grad norm: 0.584772 | Time: 8s638ms
Epoch: 036 | Test Loss: 0.0128164 | Time: 436ms
Epoch: 037 | Loss: 0.0136884 | L2 loss: 0.0135398 | Lip loss: 0.0001486 | Grad norm: 0.577014 | Time: 8s336ms
Epoch: 037 | Test Loss: 0.0128164 | Time: 441ms
Epoch: 038 | Loss: 0.0137014 | L2 loss: 0.0135562 | Lip loss: 0.0001451 | Grad norm: 0.578986 | Time: 8s303ms
Epoch: 038 | Test Loss: 0.0128164 | Time: 514ms
Epoch: 039 | Loss: 0.0137590 | L2 loss: 0.0136127 | Lip loss: 0.0001463 | Grad norm: 0.565601 | Time: 8s503ms
Epoch: 039 | Test Loss: 0.0128164 | Time: 445ms
Epoch: 040 | Loss: 0.0136835 | L2 loss: 0.0135382 | Lip loss: 0.0001453 | Grad norm: 0.577538 | Time: 8s788ms
Epoch: 040 | Test Loss: 0.0128164 | Time: 439ms
Total time: 5m56s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
