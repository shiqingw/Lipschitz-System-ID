==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 1.1132940 | L2 loss: 1.1132939 | Lip loss: 0.0000001 | Grad norm: 2.173121 | Time: 14s795ms
Epoch: 001 | Test Loss: 0.0079643 | Time: 726ms
==> Save the model at epoch 001 with test loss 0.0079643
Epoch: 002 | Loss: 0.0050183 | L2 loss: 0.0050182 | Lip loss: 0.0000001 | Grad norm: 0.658928 | Time: 14s171ms
Epoch: 002 | Test Loss: 0.0053187 | Time: 731ms
==> Save the model at epoch 002 with test loss 0.0053187
Epoch: 003 | Loss: 0.0048158 | L2 loss: 0.0048156 | Lip loss: 0.0000001 | Grad norm: 0.677130 | Time: 13s614ms
Epoch: 003 | Test Loss: 0.0061427 | Time: 807ms
Epoch: 004 | Loss: 0.0044083 | L2 loss: 0.0044082 | Lip loss: 0.0000001 | Grad norm: 0.582796 | Time: 13s717ms
Epoch: 004 | Test Loss: 0.0059917 | Time: 744ms
Epoch: 005 | Loss: 0.0044062 | L2 loss: 0.0044061 | Lip loss: 0.0000001 | Grad norm: 0.634284 | Time: 14s30ms
Epoch: 005 | Test Loss: 0.0126011 | Time: 746ms
Epoch: 006 | Loss: 0.0033083 | L2 loss: 0.0033082 | Lip loss: 0.0000001 | Grad norm: 0.252601 | Time: 14s1ms
Epoch: 006 | Test Loss: 0.0029836 | Time: 730ms
==> Save the model at epoch 006 with test loss 0.0029836
Epoch: 007 | Loss: 0.0029471 | L2 loss: 0.0029470 | Lip loss: 0.0000001 | Grad norm: 0.133591 | Time: 14s171ms
Epoch: 007 | Test Loss: 0.0029802 | Time: 729ms
==> Save the model at epoch 007 with test loss 0.0029802
Epoch: 008 | Loss: 0.0029406 | L2 loss: 0.0029405 | Lip loss: 0.0000001 | Grad norm: 0.143257 | Time: 14s80ms
Epoch: 008 | Test Loss: 0.0029651 | Time: 750ms
==> Save the model at epoch 008 with test loss 0.0029651
Epoch: 009 | Loss: 0.0029432 | L2 loss: 0.0029431 | Lip loss: 0.0000001 | Grad norm: 0.156370 | Time: 13s941ms
Epoch: 009 | Test Loss: 0.0029493 | Time: 735ms
==> Save the model at epoch 009 with test loss 0.0029493
Epoch: 010 | Loss: 0.0029262 | L2 loss: 0.0029261 | Lip loss: 0.0000001 | Grad norm: 0.146444 | Time: 13s803ms
Epoch: 010 | Test Loss: 0.0029264 | Time: 810ms
==> Save the model at epoch 010 with test loss 0.0029264
Epoch: 011 | Loss: 0.0028660 | L2 loss: 0.0028659 | Lip loss: 0.0000001 | Grad norm: 0.095000 | Time: 14s28ms
Epoch: 011 | Test Loss: 0.0028901 | Time: 733ms
==> Save the model at epoch 011 with test loss 0.0028901
Epoch: 012 | Loss: 0.0028620 | L2 loss: 0.0028619 | Lip loss: 0.0000001 | Grad norm: 0.091855 | Time: 14s521ms
Epoch: 012 | Test Loss: 0.0028911 | Time: 737ms
Epoch: 013 | Loss: 0.0028623 | L2 loss: 0.0028621 | Lip loss: 0.0000001 | Grad norm: 0.093992 | Time: 13s995ms
Epoch: 013 | Test Loss: 0.0028930 | Time: 736ms
Epoch: 014 | Loss: 0.0028613 | L2 loss: 0.0028612 | Lip loss: 0.0000001 | Grad norm: 0.095668 | Time: 13s963ms
Epoch: 014 | Test Loss: 0.0028874 | Time: 732ms
==> Save the model at epoch 014 with test loss 0.0028874
Epoch: 015 | Loss: 0.0028596 | L2 loss: 0.0028595 | Lip loss: 0.0000001 | Grad norm: 0.095970 | Time: 14s174ms
Epoch: 015 | Test Loss: 0.0028908 | Time: 806ms
Epoch: 016 | Loss: 0.0028514 | L2 loss: 0.0028513 | Lip loss: 0.0000001 | Grad norm: 0.091587 | Time: 13s915ms
Epoch: 016 | Test Loss: 0.0028830 | Time: 744ms
==> Save the model at epoch 016 with test loss 0.0028830
Epoch: 017 | Loss: 0.0028516 | L2 loss: 0.0028515 | Lip loss: 0.0000001 | Grad norm: 0.091491 | Time: 14s116ms
Epoch: 017 | Test Loss: 0.0028831 | Time: 759ms
Epoch: 018 | Loss: 0.0028514 | L2 loss: 0.0028513 | Lip loss: 0.0000001 | Grad norm: 0.087626 | Time: 13s942ms
Epoch: 018 | Test Loss: 0.0028834 | Time: 735ms
Epoch: 019 | Loss: 0.0028563 | L2 loss: 0.0028562 | Lip loss: 0.0000001 | Grad norm: 0.087898 | Time: 14s66ms
Epoch: 019 | Test Loss: 0.0028834 | Time: 721ms
Epoch: 020 | Loss: 0.0028546 | L2 loss: 0.0028545 | Lip loss: 0.0000001 | Grad norm: 0.087912 | Time: 13s933ms
Epoch: 020 | Test Loss: 0.0028825 | Time: 738ms
==> Save the model at epoch 020 with test loss 0.0028825
Epoch: 021 | Loss: 0.0028582 | L2 loss: 0.0028581 | Lip loss: 0.0000001 | Grad norm: 0.086012 | Time: 13s962ms
Epoch: 021 | Test Loss: 0.0028825 | Time: 727ms
==> Save the model at epoch 021 with test loss 0.0028825
Epoch: 022 | Loss: 0.0028489 | L2 loss: 0.0028488 | Lip loss: 0.0000001 | Grad norm: 0.090868 | Time: 14s37ms
Epoch: 022 | Test Loss: 0.0028825 | Time: 809ms
Epoch: 023 | Loss: 0.0028493 | L2 loss: 0.0028492 | Lip loss: 0.0000001 | Grad norm: 0.086105 | Time: 14s218ms
Epoch: 023 | Test Loss: 0.0028826 | Time: 733ms
Epoch: 024 | Loss: 0.0028527 | L2 loss: 0.0028526 | Lip loss: 0.0000001 | Grad norm: 0.085047 | Time: 14s284ms
Epoch: 024 | Test Loss: 0.0028826 | Time: 743ms
Epoch: 025 | Loss: 0.0028505 | L2 loss: 0.0028504 | Lip loss: 0.0000001 | Grad norm: 0.088611 | Time: 14s59ms
Epoch: 025 | Test Loss: 0.0028825 | Time: 737ms
Epoch: 026 | Loss: 0.0028498 | L2 loss: 0.0028497 | Lip loss: 0.0000001 | Grad norm: 0.090501 | Time: 14s528ms
Epoch: 026 | Test Loss: 0.0028825 | Time: 727ms
Epoch: 027 | Loss: 0.0028494 | L2 loss: 0.0028492 | Lip loss: 0.0000001 | Grad norm: 0.088694 | Time: 14s72ms
Epoch: 027 | Test Loss: 0.0028825 | Time: 808ms
Epoch: 028 | Loss: 0.0028526 | L2 loss: 0.0028525 | Lip loss: 0.0000001 | Grad norm: 0.084595 | Time: 14s227ms
Epoch: 028 | Test Loss: 0.0028825 | Time: 723ms
Epoch: 029 | Loss: 0.0028507 | L2 loss: 0.0028506 | Lip loss: 0.0000001 | Grad norm: 0.089635 | Time: 14s151ms
Epoch: 029 | Test Loss: 0.0028825 | Time: 737ms
Epoch: 030 | Loss: 0.0028494 | L2 loss: 0.0028493 | Lip loss: 0.0000001 | Grad norm: 0.084724 | Time: 13s994ms
Epoch: 030 | Test Loss: 0.0028825 | Time: 754ms
Epoch: 031 | Loss: 0.0028620 | L2 loss: 0.0028619 | Lip loss: 0.0000001 | Grad norm: 0.086278 | Time: 14s72ms
Epoch: 031 | Test Loss: 0.0028825 | Time: 734ms
Epoch: 032 | Loss: 0.0028503 | L2 loss: 0.0028502 | Lip loss: 0.0000001 | Grad norm: 0.090361 | Time: 14s193ms
Epoch: 032 | Test Loss: 0.0028825 | Time: 922ms
Epoch: 033 | Loss: 0.0028512 | L2 loss: 0.0028511 | Lip loss: 0.0000001 | Grad norm: 0.088971 | Time: 14s591ms
Epoch: 033 | Test Loss: 0.0028825 | Time: 753ms
Epoch: 034 | Loss: 0.0028489 | L2 loss: 0.0028488 | Lip loss: 0.0000001 | Grad norm: 0.088737 | Time: 14s77ms
Epoch: 034 | Test Loss: 0.0028825 | Time: 821ms
Epoch: 035 | Loss: 0.0028505 | L2 loss: 0.0028504 | Lip loss: 0.0000001 | Grad norm: 0.087716 | Time: 14s65ms
Epoch: 035 | Test Loss: 0.0028825 | Time: 738ms
Epoch: 036 | Loss: 0.0028481 | L2 loss: 0.0028480 | Lip loss: 0.0000001 | Grad norm: 0.091254 | Time: 13s991ms
Epoch: 036 | Test Loss: 0.0028825 | Time: 738ms
Epoch: 037 | Loss: 0.0028510 | L2 loss: 0.0028508 | Lip loss: 0.0000001 | Grad norm: 0.088880 | Time: 14s279ms
Epoch: 037 | Test Loss: 0.0028825 | Time: 736ms
Epoch: 038 | Loss: 0.0028511 | L2 loss: 0.0028510 | Lip loss: 0.0000001 | Grad norm: 0.091352 | Time: 14s166ms
Epoch: 038 | Test Loss: 0.0028825 | Time: 733ms
Epoch: 039 | Loss: 0.0028495 | L2 loss: 0.0028494 | Lip loss: 0.0000001 | Grad norm: 0.085605 | Time: 13s807ms
Epoch: 039 | Test Loss: 0.0028825 | Time: 805ms
Epoch: 040 | Loss: 0.0028501 | L2 loss: 0.0028500 | Lip loss: 0.0000001 | Grad norm: 0.088144 | Time: 13s920ms
Epoch: 040 | Test Loss: 0.0028825 | Time: 739ms
Total time: 9m53s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
