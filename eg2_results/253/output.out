==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 1.4229810 | L2 loss: 1.3031078 | Lip loss: 0.1198732 | Grad norm: 4.064487 | Time: 17s543ms
Epoch: 001 | Test Loss: 0.1426364 | Time: 465ms
==> Save the model at epoch 001 with test loss 0.1426364
Epoch: 002 | Loss: 0.2534099 | L2 loss: 0.1099361 | Lip loss: 0.1434738 | Grad norm: 5.339309 | Time: 16s696ms
Epoch: 002 | Test Loss: 0.0732848 | Time: 453ms
==> Save the model at epoch 002 with test loss 0.0732848
Epoch: 003 | Loss: 0.2153633 | L2 loss: 0.0726387 | Lip loss: 0.1427246 | Grad norm: 5.369536 | Time: 17s968ms
Epoch: 003 | Test Loss: 0.0455223 | Time: 518ms
==> Save the model at epoch 003 with test loss 0.0455223
Epoch: 004 | Loss: 0.1863202 | L2 loss: 0.0496951 | Lip loss: 0.1366251 | Grad norm: 3.701723 | Time: 17s531ms
Epoch: 004 | Test Loss: 0.0335294 | Time: 441ms
==> Save the model at epoch 004 with test loss 0.0335294
Epoch: 005 | Loss: 0.1786935 | L2 loss: 0.0429700 | Lip loss: 0.1357235 | Grad norm: 4.183341 | Time: 17s653ms
Epoch: 005 | Test Loss: 0.0300781 | Time: 449ms
==> Save the model at epoch 005 with test loss 0.0300781
Epoch: 006 | Loss: 0.1562645 | L2 loss: 0.0180750 | Lip loss: 0.1381895 | Grad norm: 1.354602 | Time: 17s319ms
Epoch: 006 | Test Loss: 0.0172635 | Time: 441ms
==> Save the model at epoch 006 with test loss 0.0172635
Epoch: 007 | Loss: 0.1577688 | L2 loss: 0.0165454 | Lip loss: 0.1412235 | Grad norm: 1.370205 | Time: 17s646ms
Epoch: 007 | Test Loss: 0.0168600 | Time: 455ms
==> Save the model at epoch 007 with test loss 0.0168600
Epoch: 008 | Loss: 0.1549918 | L2 loss: 0.0162674 | Lip loss: 0.1387244 | Grad norm: 1.321323 | Time: 17s875ms
Epoch: 008 | Test Loss: 0.0157378 | Time: 459ms
==> Save the model at epoch 008 with test loss 0.0157378
Epoch: 009 | Loss: 0.1526404 | L2 loss: 0.0154117 | Lip loss: 0.1372286 | Grad norm: 1.326663 | Time: 17s415ms
Epoch: 009 | Test Loss: 0.0160787 | Time: 442ms
Epoch: 010 | Loss: 0.1539279 | L2 loss: 0.0156454 | Lip loss: 0.1382825 | Grad norm: 1.347228 | Time: 17s596ms
Epoch: 010 | Test Loss: 0.0152050 | Time: 520ms
==> Save the model at epoch 010 with test loss 0.0152050
Epoch: 011 | Loss: 0.1569277 | L2 loss: 0.0141812 | Lip loss: 0.1427465 | Grad norm: 1.123453 | Time: 17s696ms
Epoch: 011 | Test Loss: 0.0141080 | Time: 447ms
==> Save the model at epoch 011 with test loss 0.0141080
Epoch: 012 | Loss: 0.1533375 | L2 loss: 0.0139620 | Lip loss: 0.1393756 | Grad norm: 1.134740 | Time: 17s548ms
Epoch: 012 | Test Loss: 0.0140338 | Time: 449ms
==> Save the model at epoch 012 with test loss 0.0140338
Epoch: 013 | Loss: 0.1518566 | L2 loss: 0.0139996 | Lip loss: 0.1378569 | Grad norm: 1.085844 | Time: 17s75ms
Epoch: 013 | Test Loss: 0.0139891 | Time: 445ms
==> Save the model at epoch 013 with test loss 0.0139891
Epoch: 014 | Loss: 0.1534923 | L2 loss: 0.0137014 | Lip loss: 0.1397909 | Grad norm: 1.147420 | Time: 17s360ms
Epoch: 014 | Test Loss: 0.0137070 | Time: 461ms
==> Save the model at epoch 014 with test loss 0.0137070
Epoch: 015 | Loss: 0.1525953 | L2 loss: 0.0136728 | Lip loss: 0.1389226 | Grad norm: 1.120259 | Time: 17s730ms
Epoch: 015 | Test Loss: 0.0137441 | Time: 454ms
Epoch: 016 | Loss: 0.1517553 | L2 loss: 0.0135182 | Lip loss: 0.1382370 | Grad norm: 1.131495 | Time: 17s216ms
Epoch: 016 | Test Loss: 0.0135432 | Time: 451ms
==> Save the model at epoch 016 with test loss 0.0135432
Epoch: 017 | Loss: 0.1533284 | L2 loss: 0.0134375 | Lip loss: 0.1398909 | Grad norm: 1.093461 | Time: 17s514ms
Epoch: 017 | Test Loss: 0.0134801 | Time: 517ms
==> Save the model at epoch 017 with test loss 0.0134801
Epoch: 018 | Loss: 0.1474774 | L2 loss: 0.0133342 | Lip loss: 0.1341433 | Grad norm: 1.011291 | Time: 17s291ms
Epoch: 018 | Test Loss: 0.0133702 | Time: 443ms
==> Save the model at epoch 018 with test loss 0.0133702
Epoch: 019 | Loss: 0.1525310 | L2 loss: 0.0133287 | Lip loss: 0.1392023 | Grad norm: 1.178150 | Time: 17s558ms
Epoch: 019 | Test Loss: 0.0134705 | Time: 454ms
Epoch: 020 | Loss: 0.1534194 | L2 loss: 0.0134433 | Lip loss: 0.1399762 | Grad norm: 1.132528 | Time: 17s71ms
Epoch: 020 | Test Loss: 0.0135988 | Time: 442ms
Epoch: 021 | Loss: 0.1475628 | L2 loss: 0.0134907 | Lip loss: 0.1340721 | Grad norm: 1.033155 | Time: 17s495ms
Epoch: 021 | Test Loss: 0.0135507 | Time: 441ms
Epoch: 022 | Loss: 0.1533263 | L2 loss: 0.0134827 | Lip loss: 0.1398436 | Grad norm: 1.122411 | Time: 17s433ms
Epoch: 022 | Test Loss: 0.0135374 | Time: 447ms
Epoch: 023 | Loss: 0.1502366 | L2 loss: 0.0134431 | Lip loss: 0.1367935 | Grad norm: 1.044282 | Time: 17s163ms
Epoch: 023 | Test Loss: 0.0135093 | Time: 460ms
Epoch: 024 | Loss: 0.1489160 | L2 loss: 0.0134141 | Lip loss: 0.1355019 | Grad norm: 1.018069 | Time: 17s128ms
Epoch: 024 | Test Loss: 0.0134796 | Time: 524ms
Epoch: 025 | Loss: 0.1483631 | L2 loss: 0.0134569 | Lip loss: 0.1349062 | Grad norm: 1.040879 | Time: 17s19ms
Epoch: 025 | Test Loss: 0.0134531 | Time: 444ms
Epoch: 026 | Loss: 0.1551872 | L2 loss: 0.0133914 | Lip loss: 0.1417958 | Grad norm: 1.145334 | Time: 17s353ms
Epoch: 026 | Test Loss: 0.0134534 | Time: 467ms
Epoch: 027 | Loss: 0.1534054 | L2 loss: 0.0133743 | Lip loss: 0.1400311 | Grad norm: 1.095694 | Time: 17s201ms
Epoch: 027 | Test Loss: 0.0134534 | Time: 436ms
Epoch: 028 | Loss: 0.1467161 | L2 loss: 0.0133701 | Lip loss: 0.1333460 | Grad norm: 0.973403 | Time: 17s284ms
Epoch: 028 | Test Loss: 0.0134521 | Time: 454ms
Epoch: 029 | Loss: 0.1504519 | L2 loss: 0.0133706 | Lip loss: 0.1370814 | Grad norm: 1.061450 | Time: 16s715ms
Epoch: 029 | Test Loss: 0.0134520 | Time: 442ms
Epoch: 030 | Loss: 0.1515226 | L2 loss: 0.0133717 | Lip loss: 0.1381510 | Grad norm: 1.145490 | Time: 17s177ms
Epoch: 030 | Test Loss: 0.0134529 | Time: 466ms
Epoch: 031 | Loss: 0.1539847 | L2 loss: 0.0133734 | Lip loss: 0.1406113 | Grad norm: 1.050486 | Time: 17s409ms
Epoch: 031 | Test Loss: 0.0134529 | Time: 435ms
Epoch: 032 | Loss: 0.1518300 | L2 loss: 0.0133688 | Lip loss: 0.1384612 | Grad norm: 1.101175 | Time: 17s429ms
Epoch: 032 | Test Loss: 0.0134529 | Time: 443ms
Epoch: 033 | Loss: 0.1491688 | L2 loss: 0.0133763 | Lip loss: 0.1357925 | Grad norm: 1.016336 | Time: 17s113ms
Epoch: 033 | Test Loss: 0.0134529 | Time: 607ms
Epoch: 034 | Loss: 0.1504947 | L2 loss: 0.0133768 | Lip loss: 0.1371179 | Grad norm: 1.095408 | Time: 17s209ms
Epoch: 034 | Test Loss: 0.0134529 | Time: 481ms
Epoch: 035 | Loss: 0.1502562 | L2 loss: 0.0133716 | Lip loss: 0.1368847 | Grad norm: 1.118976 | Time: 17s398ms
Epoch: 035 | Test Loss: 0.0134529 | Time: 532ms
Epoch: 036 | Loss: 0.1489495 | L2 loss: 0.0133690 | Lip loss: 0.1355805 | Grad norm: 1.055920 | Time: 16s839ms
Epoch: 036 | Test Loss: 0.0134529 | Time: 528ms
Epoch: 037 | Loss: 0.1509227 | L2 loss: 0.0133784 | Lip loss: 0.1375443 | Grad norm: 1.069968 | Time: 17s249ms
Epoch: 037 | Test Loss: 0.0134529 | Time: 448ms
Epoch: 038 | Loss: 0.1510333 | L2 loss: 0.0133726 | Lip loss: 0.1376606 | Grad norm: 1.094435 | Time: 16s752ms
Epoch: 038 | Test Loss: 0.0134529 | Time: 445ms
Epoch: 039 | Loss: 0.1514886 | L2 loss: 0.0133681 | Lip loss: 0.1381205 | Grad norm: 1.080527 | Time: 17s40ms
Epoch: 039 | Test Loss: 0.0134529 | Time: 463ms
Epoch: 040 | Loss: 0.1552333 | L2 loss: 0.0133745 | Lip loss: 0.1418588 | Grad norm: 1.125501 | Time: 17s57ms
Epoch: 040 | Test Loss: 0.0134529 | Time: 443ms
Total time: 11m51s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
