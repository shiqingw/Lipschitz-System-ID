==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Lipschitz constant: 2.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5123476 0.5241365]
==> Ouput transform to be applied to the neural network:
[1.908  1.9349]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 7.3690110 | Grad norm: 3.155137 | Time: 18s917ms
Epoch: 001 | Test Loss: 7.3730142 | Time: 636ms
==> Save the model at epoch 001 with test loss 7.3730142
Epoch: 002 | Train Loss: 0.6048365 | Grad norm: 2.164248 | Time: 18s637ms
Epoch: 002 | Test Loss: 0.0507673 | Time: 628ms
==> Save the model at epoch 002 with test loss 0.0507673
Epoch: 003 | Train Loss: 0.0368853 | Grad norm: 1.352506 | Time: 18s914ms
Epoch: 003 | Test Loss: 0.0222437 | Time: 630ms
==> Save the model at epoch 003 with test loss 0.0222437
Epoch: 004 | Train Loss: 0.0266771 | Grad norm: 2.347022 | Time: 18s386ms
Epoch: 004 | Test Loss: 0.0229637 | Time: 722ms
Epoch: 005 | Train Loss: 0.0287262 | Grad norm: 3.704712 | Time: 18s664ms
Epoch: 005 | Test Loss: 0.0150215 | Time: 632ms
==> Save the model at epoch 005 with test loss 0.0150215
Epoch: 006 | Train Loss: 0.0255560 | Grad norm: 3.264883 | Time: 18s891ms
Epoch: 006 | Test Loss: 0.0127208 | Time: 768ms
==> Save the model at epoch 006 with test loss 0.0127208
Epoch: 007 | Train Loss: 0.0278316 | Grad norm: 3.784512 | Time: 18s969ms
Epoch: 007 | Test Loss: 0.0196210 | Time: 630ms
Epoch: 008 | Train Loss: 0.0219703 | Grad norm: 2.852688 | Time: 18s320ms
Epoch: 008 | Test Loss: 0.0129641 | Time: 711ms
Epoch: 009 | Train Loss: 0.0149634 | Grad norm: 2.295535 | Time: 18s905ms
Epoch: 009 | Test Loss: 0.0139460 | Time: 629ms
Epoch: 010 | Train Loss: 0.0125463 | Grad norm: 1.930753 | Time: 19s59ms
Epoch: 010 | Test Loss: 0.0090655 | Time: 726ms
==> Save the model at epoch 010 with test loss 0.0090655
Epoch: 011 | Train Loss: 0.0111763 | Grad norm: 1.762250 | Time: 18s600ms
Epoch: 011 | Test Loss: 0.0075924 | Time: 626ms
==> Save the model at epoch 011 with test loss 0.0075924
Epoch: 012 | Train Loss: 0.0109169 | Grad norm: 1.754000 | Time: 19s134ms
Epoch: 012 | Test Loss: 0.0150867 | Time: 635ms
Epoch: 013 | Train Loss: 0.0139720 | Grad norm: 2.172384 | Time: 18s513ms
Epoch: 013 | Test Loss: 0.0047780 | Time: 635ms
==> Save the model at epoch 013 with test loss 0.0047780
Epoch: 014 | Train Loss: 0.0070323 | Grad norm: 1.082360 | Time: 18s303ms
Epoch: 014 | Test Loss: 0.0061371 | Time: 641ms
Epoch: 015 | Train Loss: 0.0118831 | Grad norm: 1.919294 | Time: 18s418ms
Epoch: 015 | Test Loss: 0.0116357 | Time: 714ms
Epoch: 016 | Train Loss: 0.0076618 | Grad norm: 1.136189 | Time: 19s21ms
Epoch: 016 | Test Loss: 0.0091463 | Time: 634ms
Epoch: 017 | Train Loss: 0.0055695 | Grad norm: 0.830114 | Time: 19s392ms
Epoch: 017 | Test Loss: 0.0054927 | Time: 639ms
Epoch: 018 | Train Loss: 0.0043993 | Grad norm: 0.548403 | Time: 19s27ms
Epoch: 018 | Test Loss: 0.0047031 | Time: 642ms
==> Save the model at epoch 018 with test loss 0.0047031
Epoch: 019 | Train Loss: 0.0047327 | Grad norm: 0.680249 | Time: 18s700ms
Epoch: 019 | Test Loss: 0.0032855 | Time: 628ms
==> Save the model at epoch 019 with test loss 0.0032855
Epoch: 020 | Train Loss: 0.0047489 | Grad norm: 0.705763 | Time: 18s790ms
Epoch: 020 | Test Loss: 0.0118218 | Time: 635ms
Epoch: 021 | Train Loss: 0.0046762 | Grad norm: 0.703112 | Time: 18s624ms
Epoch: 021 | Test Loss: 0.0044575 | Time: 634ms
Epoch: 022 | Train Loss: 0.0040461 | Grad norm: 0.552554 | Time: 18s738ms
Epoch: 022 | Test Loss: 0.0041313 | Time: 718ms
Epoch: 023 | Train Loss: 0.0039338 | Grad norm: 0.517424 | Time: 18s913ms
Epoch: 023 | Test Loss: 0.0040606 | Time: 635ms
Epoch: 024 | Train Loss: 0.0036384 | Grad norm: 0.455151 | Time: 18s989ms
Epoch: 024 | Test Loss: 0.0032097 | Time: 628ms
==> Save the model at epoch 024 with test loss 0.0032097
Epoch: 025 | Train Loss: 0.0035802 | Grad norm: 0.444916 | Time: 18s661ms
Epoch: 025 | Test Loss: 0.0037373 | Time: 632ms
Epoch: 026 | Train Loss: 0.0035538 | Grad norm: 0.436743 | Time: 18s481ms
Epoch: 026 | Test Loss: 0.0033546 | Time: 635ms
Epoch: 027 | Train Loss: 0.0034868 | Grad norm: 0.431075 | Time: 18s831ms
Epoch: 027 | Test Loss: 0.0027929 | Time: 636ms
==> Save the model at epoch 027 with test loss 0.0027929
Epoch: 028 | Train Loss: 0.0033278 | Grad norm: 0.395896 | Time: 18s793ms
Epoch: 028 | Test Loss: 0.0031620 | Time: 636ms
Epoch: 029 | Train Loss: 0.0031633 | Grad norm: 0.338219 | Time: 18s447ms
Epoch: 029 | Test Loss: 0.0027921 | Time: 709ms
==> Save the model at epoch 029 with test loss 0.0027921
Epoch: 030 | Train Loss: 0.0031427 | Grad norm: 0.355157 | Time: 18s873ms
Epoch: 030 | Test Loss: 0.0028527 | Time: 628ms
Epoch: 031 | Train Loss: 0.0030027 | Grad norm: 0.295326 | Time: 19s128ms
Epoch: 031 | Test Loss: 0.0027614 | Time: 639ms
==> Save the model at epoch 031 with test loss 0.0027614
Epoch: 032 | Train Loss: 0.0029522 | Grad norm: 0.286611 | Time: 18s639ms
Epoch: 032 | Test Loss: 0.0024649 | Time: 660ms
==> Save the model at epoch 032 with test loss 0.0024649
Epoch: 033 | Train Loss: 0.0028855 | Grad norm: 0.254840 | Time: 18s892ms
Epoch: 033 | Test Loss: 0.0028021 | Time: 709ms
Epoch: 034 | Train Loss: 0.0028343 | Grad norm: 0.252379 | Time: 18s967ms
Epoch: 034 | Test Loss: 0.0026433 | Time: 647ms
Epoch: 035 | Train Loss: 0.0027779 | Grad norm: 0.221779 | Time: 18s872ms
Epoch: 035 | Test Loss: 0.0024396 | Time: 638ms
==> Save the model at epoch 035 with test loss 0.0024396
Epoch: 036 | Train Loss: 0.0027223 | Grad norm: 0.203964 | Time: 18s499ms
Epoch: 036 | Test Loss: 0.0026224 | Time: 647ms
Epoch: 037 | Train Loss: 0.0026829 | Grad norm: 0.187166 | Time: 18s519ms
Epoch: 037 | Test Loss: 0.0024326 | Time: 640ms
==> Save the model at epoch 037 with test loss 0.0024326
Epoch: 038 | Train Loss: 0.0026429 | Grad norm: 0.164162 | Time: 18s866ms
Epoch: 038 | Test Loss: 0.0023924 | Time: 629ms
==> Save the model at epoch 038 with test loss 0.0023924
Epoch: 039 | Train Loss: 0.0026132 | Grad norm: 0.145077 | Time: 19s63ms
Epoch: 039 | Test Loss: 0.0023395 | Time: 633ms
==> Save the model at epoch 039 with test loss 0.0023395
Epoch: 040 | Train Loss: 0.0025955 | Grad norm: 0.134155 | Time: 18s10ms
Epoch: 040 | Test Loss: 0.0023385 | Time: 714ms
==> Save the model at epoch 040 with test loss 0.0023385
Total time: 12m56s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5123476 0.5241365]
==> Output transform to be applied to the neural network (trained):
[1.908  1.9349]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
