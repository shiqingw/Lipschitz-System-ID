==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 6.9094615 | L2 loss: 6.9083514 | Lip loss: 0.0011101 | Grad norm: 2.339204 | Time: 17s718ms
Epoch: 001 | Test Loss: 3.2607795 | Time: 450ms
==> Save the model at epoch 001 with test loss 3.2607795
Epoch: 002 | Loss: 0.6447289 | L2 loss: 0.6322035 | Lip loss: 0.0125254 | Grad norm: 3.078523 | Time: 17s407ms
Epoch: 002 | Test Loss: 0.2102099 | Time: 440ms
==> Save the model at epoch 002 with test loss 0.2102099
Epoch: 003 | Loss: 0.2129786 | L2 loss: 0.1986991 | Lip loss: 0.0142795 | Grad norm: 1.976685 | Time: 17s357ms
Epoch: 003 | Test Loss: 0.1482183 | Time: 525ms
==> Save the model at epoch 003 with test loss 0.1482183
Epoch: 004 | Loss: 0.1632595 | L2 loss: 0.1485304 | Lip loss: 0.0147291 | Grad norm: 2.088810 | Time: 17s581ms
Epoch: 004 | Test Loss: 0.1149686 | Time: 443ms
==> Save the model at epoch 004 with test loss 0.1149686
Epoch: 005 | Loss: 0.1312954 | L2 loss: 0.1160185 | Lip loss: 0.0152768 | Grad norm: 2.257860 | Time: 17s59ms
Epoch: 005 | Test Loss: 0.0956146 | Time: 459ms
==> Save the model at epoch 005 with test loss 0.0956146
Epoch: 006 | Loss: 0.1165130 | L2 loss: 0.1009624 | Lip loss: 0.0155506 | Grad norm: 1.643687 | Time: 16s977ms
Epoch: 006 | Test Loss: 0.0879987 | Time: 448ms
==> Save the model at epoch 006 with test loss 0.0879987
Epoch: 007 | Loss: 0.1133789 | L2 loss: 0.0983010 | Lip loss: 0.0150779 | Grad norm: 1.483351 | Time: 17s146ms
Epoch: 007 | Test Loss: 0.0853906 | Time: 554ms
==> Save the model at epoch 007 with test loss 0.0853906
Epoch: 008 | Loss: 0.1111226 | L2 loss: 0.0957603 | Lip loss: 0.0153623 | Grad norm: 1.435559 | Time: 17s155ms
Epoch: 008 | Test Loss: 0.0839710 | Time: 436ms
==> Save the model at epoch 008 with test loss 0.0839710
Epoch: 009 | Loss: 0.1086970 | L2 loss: 0.0936252 | Lip loss: 0.0150718 | Grad norm: 1.495407 | Time: 17s383ms
Epoch: 009 | Test Loss: 0.0814573 | Time: 447ms
==> Save the model at epoch 009 with test loss 0.0814573
Epoch: 010 | Loss: 0.1067068 | L2 loss: 0.0915065 | Lip loss: 0.0152003 | Grad norm: 1.573089 | Time: 17s278ms
Epoch: 010 | Test Loss: 0.0794104 | Time: 509ms
==> Save the model at epoch 010 with test loss 0.0794104
Epoch: 011 | Loss: 0.1051177 | L2 loss: 0.0897741 | Lip loss: 0.0153436 | Grad norm: 1.432196 | Time: 17s788ms
Epoch: 011 | Test Loss: 0.0790473 | Time: 474ms
==> Save the model at epoch 011 with test loss 0.0790473
Epoch: 012 | Loss: 0.1049265 | L2 loss: 0.0895783 | Lip loss: 0.0153482 | Grad norm: 1.373261 | Time: 17s263ms
Epoch: 012 | Test Loss: 0.0788150 | Time: 457ms
==> Save the model at epoch 012 with test loss 0.0788150
Epoch: 013 | Loss: 0.1046203 | L2 loss: 0.0893771 | Lip loss: 0.0152431 | Grad norm: 1.346472 | Time: 16s848ms
Epoch: 013 | Test Loss: 0.0786423 | Time: 459ms
==> Save the model at epoch 013 with test loss 0.0786423
Epoch: 014 | Loss: 0.1043233 | L2 loss: 0.0891368 | Lip loss: 0.0151865 | Grad norm: 1.381177 | Time: 17s671ms
Epoch: 014 | Test Loss: 0.0783956 | Time: 452ms
==> Save the model at epoch 014 with test loss 0.0783956
Epoch: 015 | Loss: 0.1051496 | L2 loss: 0.0897511 | Lip loss: 0.0153985 | Grad norm: 1.375303 | Time: 17s303ms
Epoch: 015 | Test Loss: 0.0781967 | Time: 449ms
==> Save the model at epoch 015 with test loss 0.0781967
Epoch: 016 | Loss: 0.1041424 | L2 loss: 0.0887126 | Lip loss: 0.0154299 | Grad norm: 1.326129 | Time: 16s726ms
Epoch: 016 | Test Loss: 0.0781835 | Time: 456ms
==> Save the model at epoch 016 with test loss 0.0781835
Epoch: 017 | Loss: 0.1046088 | L2 loss: 0.0890265 | Lip loss: 0.0155822 | Grad norm: 1.344076 | Time: 17s173ms
Epoch: 017 | Test Loss: 0.0781661 | Time: 648ms
==> Save the model at epoch 017 with test loss 0.0781661
Epoch: 018 | Loss: 0.1040683 | L2 loss: 0.0887155 | Lip loss: 0.0153528 | Grad norm: 1.358094 | Time: 16s894ms
Epoch: 018 | Test Loss: 0.0781429 | Time: 470ms
==> Save the model at epoch 018 with test loss 0.0781429
Epoch: 019 | Loss: 0.1038782 | L2 loss: 0.0887230 | Lip loss: 0.0151552 | Grad norm: 1.297291 | Time: 16s509ms
Epoch: 019 | Test Loss: 0.0781259 | Time: 458ms
==> Save the model at epoch 019 with test loss 0.0781259
Epoch: 020 | Loss: 0.1038494 | L2 loss: 0.0886386 | Lip loss: 0.0152109 | Grad norm: 1.348895 | Time: 16s936ms
Epoch: 020 | Test Loss: 0.0781043 | Time: 451ms
==> Save the model at epoch 020 with test loss 0.0781043
Epoch: 021 | Loss: 0.1040249 | L2 loss: 0.0886268 | Lip loss: 0.0153981 | Grad norm: 1.350453 | Time: 17s272ms
Epoch: 021 | Test Loss: 0.0781033 | Time: 459ms
==> Save the model at epoch 021 with test loss 0.0781033
Epoch: 022 | Loss: 0.1039430 | L2 loss: 0.0886341 | Lip loss: 0.0153089 | Grad norm: 1.368424 | Time: 17s58ms
Epoch: 022 | Test Loss: 0.0781026 | Time: 442ms
==> Save the model at epoch 022 with test loss 0.0781026
Epoch: 023 | Loss: 0.1041046 | L2 loss: 0.0886600 | Lip loss: 0.0154446 | Grad norm: 1.355821 | Time: 17s150ms
Epoch: 023 | Test Loss: 0.0781015 | Time: 450ms
==> Save the model at epoch 023 with test loss 0.0781015
Epoch: 024 | Loss: 0.1037135 | L2 loss: 0.0888717 | Lip loss: 0.0148418 | Grad norm: 1.329583 | Time: 17s71ms
Epoch: 024 | Test Loss: 0.0781008 | Time: 506ms
==> Save the model at epoch 024 with test loss 0.0781008
Epoch: 025 | Loss: 0.1039581 | L2 loss: 0.0886668 | Lip loss: 0.0152913 | Grad norm: 1.319905 | Time: 17s91ms
Epoch: 025 | Test Loss: 0.0780991 | Time: 454ms
==> Save the model at epoch 025 with test loss 0.0780991
Epoch: 026 | Loss: 0.1039861 | L2 loss: 0.0889976 | Lip loss: 0.0149885 | Grad norm: 1.317550 | Time: 16s647ms
Epoch: 026 | Test Loss: 0.0780991 | Time: 481ms
==> Save the model at epoch 026 with test loss 0.0780991
Epoch: 027 | Loss: 0.1052504 | L2 loss: 0.0897276 | Lip loss: 0.0155227 | Grad norm: 1.356111 | Time: 17s150ms
Epoch: 027 | Test Loss: 0.0780991 | Time: 451ms
Epoch: 028 | Loss: 0.1043797 | L2 loss: 0.0887021 | Lip loss: 0.0156776 | Grad norm: 1.417455 | Time: 17s231ms
Epoch: 028 | Test Loss: 0.0780991 | Time: 443ms
==> Save the model at epoch 028 with test loss 0.0780991
Epoch: 029 | Loss: 0.1038899 | L2 loss: 0.0886814 | Lip loss: 0.0152085 | Grad norm: 1.381338 | Time: 17s20ms
Epoch: 029 | Test Loss: 0.0780991 | Time: 467ms
Epoch: 030 | Loss: 0.1038985 | L2 loss: 0.0886349 | Lip loss: 0.0152636 | Grad norm: 1.357413 | Time: 17s427ms
Epoch: 030 | Test Loss: 0.0780991 | Time: 462ms
==> Save the model at epoch 030 with test loss 0.0780991
Epoch: 031 | Loss: 0.1037547 | L2 loss: 0.0886358 | Lip loss: 0.0151189 | Grad norm: 1.370543 | Time: 16s884ms
Epoch: 031 | Test Loss: 0.0780991 | Time: 454ms
==> Save the model at epoch 031 with test loss 0.0780991
Epoch: 032 | Loss: 0.1038444 | L2 loss: 0.0886712 | Lip loss: 0.0151732 | Grad norm: 1.361234 | Time: 16s854ms
Epoch: 032 | Test Loss: 0.0780991 | Time: 453ms
Epoch: 033 | Loss: 0.1040044 | L2 loss: 0.0886190 | Lip loss: 0.0153854 | Grad norm: 1.343709 | Time: 17s469ms
Epoch: 033 | Test Loss: 0.0780991 | Time: 446ms
Epoch: 034 | Loss: 0.1044970 | L2 loss: 0.0889548 | Lip loss: 0.0155423 | Grad norm: 1.380934 | Time: 17s179ms
Epoch: 034 | Test Loss: 0.0780991 | Time: 445ms
Epoch: 035 | Loss: 0.1036521 | L2 loss: 0.0886349 | Lip loss: 0.0150173 | Grad norm: 1.333715 | Time: 17s198ms
Epoch: 035 | Test Loss: 0.0780991 | Time: 440ms
Epoch: 036 | Loss: 0.1038163 | L2 loss: 0.0886180 | Lip loss: 0.0151984 | Grad norm: 1.384586 | Time: 16s732ms
Epoch: 036 | Test Loss: 0.0780991 | Time: 512ms
Epoch: 037 | Loss: 0.1035443 | L2 loss: 0.0886644 | Lip loss: 0.0148799 | Grad norm: 1.275407 | Time: 17s477ms
Epoch: 037 | Test Loss: 0.0780991 | Time: 511ms
Epoch: 038 | Loss: 0.1039344 | L2 loss: 0.0886255 | Lip loss: 0.0153089 | Grad norm: 1.334347 | Time: 17s76ms
Epoch: 038 | Test Loss: 0.0780991 | Time: 452ms
Epoch: 039 | Loss: 0.1039342 | L2 loss: 0.0887901 | Lip loss: 0.0151441 | Grad norm: 1.373392 | Time: 17s436ms
Epoch: 039 | Test Loss: 0.0780991 | Time: 451ms
Epoch: 040 | Loss: 0.1038908 | L2 loss: 0.0886290 | Lip loss: 0.0152617 | Grad norm: 1.378762 | Time: 17s996ms
Epoch: 040 | Test Loss: 0.0780991 | Time: 446ms
Total time: 11m46s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
