==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Lipschitz constant: 1.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5123476 0.5241365]
==> Ouput transform to be applied to the neural network:
[1.908  1.9349]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 7.4068932 | Grad norm: 1.582289 | Time: 4s668ms
Epoch: 001 | Test Loss: 7.3801768 | Time: 629ms
==> Save the model at epoch 001 with test loss 7.3801768
Epoch: 002 | Train Loss: 4.1449299 | Grad norm: 4.219683 | Time: 4s460ms
Epoch: 002 | Test Loss: 2.0728599 | Time: 625ms
==> Save the model at epoch 002 with test loss 2.0728599
Epoch: 003 | Train Loss: 1.7063959 | Grad norm: 3.475192 | Time: 4s572ms
Epoch: 003 | Test Loss: 1.4026700 | Time: 692ms
==> Save the model at epoch 003 with test loss 1.4026700
Epoch: 004 | Train Loss: 1.1061601 | Grad norm: 2.850754 | Time: 4s406ms
Epoch: 004 | Test Loss: 0.8356710 | Time: 627ms
==> Save the model at epoch 004 with test loss 0.8356710
Epoch: 005 | Train Loss: 0.6122435 | Grad norm: 2.088369 | Time: 4s568ms
Epoch: 005 | Test Loss: 0.4323251 | Time: 626ms
==> Save the model at epoch 005 with test loss 0.4323251
Epoch: 006 | Train Loss: 0.3788776 | Grad norm: 2.773394 | Time: 4s478ms
Epoch: 006 | Test Loss: 0.3374990 | Time: 637ms
==> Save the model at epoch 006 with test loss 0.3374990
Epoch: 007 | Train Loss: 0.3115647 | Grad norm: 3.027385 | Time: 4s605ms
Epoch: 007 | Test Loss: 0.2852129 | Time: 627ms
==> Save the model at epoch 007 with test loss 0.2852129
Epoch: 008 | Train Loss: 0.2801366 | Grad norm: 2.968065 | Time: 4s609ms
Epoch: 008 | Test Loss: 0.2475362 | Time: 628ms
==> Save the model at epoch 008 with test loss 0.2475362
Epoch: 009 | Train Loss: 0.2674878 | Grad norm: 3.340268 | Time: 4s799ms
Epoch: 009 | Test Loss: 0.2994484 | Time: 643ms
Epoch: 010 | Train Loss: 0.2749057 | Grad norm: 5.372660 | Time: 4s498ms
Epoch: 010 | Test Loss: 0.2455832 | Time: 694ms
==> Save the model at epoch 010 with test loss 0.2455832
Epoch: 011 | Train Loss: 0.2590794 | Grad norm: 4.751495 | Time: 4s698ms
Epoch: 011 | Test Loss: 0.2435792 | Time: 628ms
==> Save the model at epoch 011 with test loss 0.2435792
Epoch: 012 | Train Loss: 0.2489003 | Grad norm: 4.262479 | Time: 4s552ms
Epoch: 012 | Test Loss: 0.2190889 | Time: 627ms
==> Save the model at epoch 012 with test loss 0.2190889
Epoch: 013 | Train Loss: 0.2371808 | Grad norm: 3.282172 | Time: 4s627ms
Epoch: 013 | Test Loss: 0.2188908 | Time: 632ms
==> Save the model at epoch 013 with test loss 0.2188908
Epoch: 014 | Train Loss: 0.2274614 | Grad norm: 2.628874 | Time: 4s584ms
Epoch: 014 | Test Loss: 0.2093433 | Time: 629ms
==> Save the model at epoch 014 with test loss 0.2093433
Epoch: 015 | Train Loss: 0.2324247 | Grad norm: 3.066744 | Time: 4s692ms
Epoch: 015 | Test Loss: 0.2091985 | Time: 648ms
==> Save the model at epoch 015 with test loss 0.2091985
Epoch: 016 | Train Loss: 0.2253535 | Grad norm: 3.140617 | Time: 4s475ms
Epoch: 016 | Test Loss: 0.2033294 | Time: 634ms
==> Save the model at epoch 016 with test loss 0.2033294
Epoch: 017 | Train Loss: 0.2153452 | Grad norm: 2.600991 | Time: 4s532ms
Epoch: 017 | Test Loss: 0.2094009 | Time: 706ms
Epoch: 018 | Train Loss: 0.2136171 | Grad norm: 2.251222 | Time: 4s617ms
Epoch: 018 | Test Loss: 0.2061235 | Time: 633ms
Epoch: 019 | Train Loss: 0.2061007 | Grad norm: 1.657361 | Time: 4s548ms
Epoch: 019 | Test Loss: 0.1908046 | Time: 626ms
==> Save the model at epoch 019 with test loss 0.1908046
Epoch: 020 | Train Loss: 0.2052213 | Grad norm: 1.938355 | Time: 4s692ms
Epoch: 020 | Test Loss: 0.1982242 | Time: 630ms
Epoch: 021 | Train Loss: 0.2016673 | Grad norm: 1.632749 | Time: 4s698ms
Epoch: 021 | Test Loss: 0.2001702 | Time: 663ms
Epoch: 022 | Train Loss: 0.2068185 | Grad norm: 2.410618 | Time: 4s737ms
Epoch: 022 | Test Loss: 0.1942494 | Time: 642ms
Epoch: 023 | Train Loss: 0.2010111 | Grad norm: 1.696453 | Time: 4s846ms
Epoch: 023 | Test Loss: 0.1884884 | Time: 622ms
==> Save the model at epoch 023 with test loss 0.1884884
Epoch: 024 | Train Loss: 0.1988144 | Grad norm: 1.489043 | Time: 4s682ms
Epoch: 024 | Test Loss: 0.1968140 | Time: 707ms
Epoch: 025 | Train Loss: 0.1997649 | Grad norm: 1.745216 | Time: 4s737ms
Epoch: 025 | Test Loss: 0.2192224 | Time: 645ms
Epoch: 026 | Train Loss: 0.1962486 | Grad norm: 1.218583 | Time: 5s52ms
Epoch: 026 | Test Loss: 0.1860824 | Time: 634ms
==> Save the model at epoch 026 with test loss 0.1860824
Epoch: 027 | Train Loss: 0.1954809 | Grad norm: 1.229724 | Time: 4s828ms
Epoch: 027 | Test Loss: 0.1913945 | Time: 636ms
Epoch: 028 | Train Loss: 0.1945669 | Grad norm: 1.116910 | Time: 4s712ms
Epoch: 028 | Test Loss: 0.1950590 | Time: 643ms
Epoch: 029 | Train Loss: 0.1927230 | Grad norm: 0.853110 | Time: 4s709ms
Epoch: 029 | Test Loss: 0.1862854 | Time: 627ms
Epoch: 030 | Train Loss: 0.1937688 | Grad norm: 1.190623 | Time: 4s849ms
Epoch: 030 | Test Loss: 0.1850232 | Time: 632ms
==> Save the model at epoch 030 with test loss 0.1850232
Epoch: 031 | Train Loss: 0.1914579 | Grad norm: 0.764122 | Time: 4s778ms
Epoch: 031 | Test Loss: 0.1847339 | Time: 707ms
==> Save the model at epoch 031 with test loss 0.1847339
Epoch: 032 | Train Loss: 0.1914849 | Grad norm: 0.818990 | Time: 4s559ms
Epoch: 032 | Test Loss: 0.1858422 | Time: 628ms
Epoch: 033 | Train Loss: 0.1914920 | Grad norm: 0.831247 | Time: 4s774ms
Epoch: 033 | Test Loss: 0.1856995 | Time: 621ms
Epoch: 034 | Train Loss: 0.1916096 | Grad norm: 0.812487 | Time: 4s515ms
Epoch: 034 | Test Loss: 0.1840528 | Time: 643ms
==> Save the model at epoch 034 with test loss 0.1840528
Epoch: 035 | Train Loss: 0.1915017 | Grad norm: 0.710039 | Time: 4s710ms
Epoch: 035 | Test Loss: 0.1838542 | Time: 629ms
==> Save the model at epoch 035 with test loss 0.1838542
Epoch: 036 | Train Loss: 0.1911923 | Grad norm: 0.643470 | Time: 4s662ms
Epoch: 036 | Test Loss: 0.1862033 | Time: 626ms
Epoch: 037 | Train Loss: 0.1906000 | Grad norm: 0.632074 | Time: 4s651ms
Epoch: 037 | Test Loss: 0.1838262 | Time: 628ms
==> Save the model at epoch 037 with test loss 0.1838262
Epoch: 038 | Train Loss: 0.1899496 | Grad norm: 0.556834 | Time: 4s436ms
Epoch: 038 | Test Loss: 0.1839014 | Time: 694ms
Epoch: 039 | Train Loss: 0.1900494 | Grad norm: 0.515947 | Time: 4s495ms
Epoch: 039 | Test Loss: 0.1837685 | Time: 627ms
==> Save the model at epoch 039 with test loss 0.1837685
Epoch: 040 | Train Loss: 0.1897380 | Grad norm: 0.495623 | Time: 4s606ms
Epoch: 040 | Test Loss: 0.1836865 | Time: 651ms
==> Save the model at epoch 040 with test loss 0.1836865
Total time: 3m31s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5123476 0.5241365]
==> Output transform to be applied to the neural network (trained):
[1.908  1.9349]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
