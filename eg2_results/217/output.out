==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 0.5980851 | L2 loss: 0.5979786 | Lip loss: 0.0001065 | Grad norm: 1.389109 | Time: 27s840ms
Epoch: 001 | Test Loss: 0.0040646 | Time: 744ms
==> Save the model at epoch 001 with test loss 0.0040646
Epoch: 002 | Loss: 0.0045254 | L2 loss: 0.0044164 | Lip loss: 0.0001090 | Grad norm: 0.595948 | Time: 27s786ms
Epoch: 002 | Test Loss: 0.0046789 | Time: 793ms
Epoch: 003 | Loss: 0.0043393 | L2 loss: 0.0042301 | Lip loss: 0.0001093 | Grad norm: 0.594873 | Time: 28s479ms
Epoch: 003 | Test Loss: 0.0038094 | Time: 790ms
==> Save the model at epoch 003 with test loss 0.0038094
Epoch: 004 | Loss: 0.0044451 | L2 loss: 0.0043365 | Lip loss: 0.0001086 | Grad norm: 0.564393 | Time: 27s825ms
Epoch: 004 | Test Loss: 0.0043628 | Time: 788ms
Epoch: 005 | Loss: 0.0037729 | L2 loss: 0.0036640 | Lip loss: 0.0001089 | Grad norm: 0.426106 | Time: 27s451ms
Epoch: 005 | Test Loss: 0.0030504 | Time: 717ms
==> Save the model at epoch 005 with test loss 0.0030504
Epoch: 006 | Loss: 0.0029418 | L2 loss: 0.0028327 | Lip loss: 0.0001090 | Grad norm: 0.124622 | Time: 28s530ms
Epoch: 006 | Test Loss: 0.0028938 | Time: 727ms
==> Save the model at epoch 006 with test loss 0.0028938
Epoch: 007 | Loss: 0.0029276 | L2 loss: 0.0028190 | Lip loss: 0.0001086 | Grad norm: 0.118567 | Time: 28s79ms
Epoch: 007 | Test Loss: 0.0028412 | Time: 785ms
==> Save the model at epoch 007 with test loss 0.0028412
Epoch: 008 | Loss: 0.0029319 | L2 loss: 0.0028236 | Lip loss: 0.0001082 | Grad norm: 0.125345 | Time: 27s765ms
Epoch: 008 | Test Loss: 0.0028369 | Time: 800ms
==> Save the model at epoch 008 with test loss 0.0028369
Epoch: 009 | Loss: 0.0029356 | L2 loss: 0.0028266 | Lip loss: 0.0001090 | Grad norm: 0.138322 | Time: 28s146ms
Epoch: 009 | Test Loss: 0.0028179 | Time: 812ms
==> Save the model at epoch 009 with test loss 0.0028179
Epoch: 010 | Loss: 0.0029282 | L2 loss: 0.0028197 | Lip loss: 0.0001085 | Grad norm: 0.132671 | Time: 28s726ms
Epoch: 010 | Test Loss: 0.0028658 | Time: 784ms
Epoch: 011 | Loss: 0.0028691 | L2 loss: 0.0027603 | Lip loss: 0.0001088 | Grad norm: 0.088104 | Time: 27s904ms
Epoch: 011 | Test Loss: 0.0027885 | Time: 801ms
==> Save the model at epoch 011 with test loss 0.0027885
Epoch: 012 | Loss: 0.0028664 | L2 loss: 0.0027574 | Lip loss: 0.0001090 | Grad norm: 0.088134 | Time: 28s424ms
Epoch: 012 | Test Loss: 0.0027913 | Time: 718ms
Epoch: 013 | Loss: 0.0028677 | L2 loss: 0.0027583 | Lip loss: 0.0001095 | Grad norm: 0.091719 | Time: 29s403ms
Epoch: 013 | Test Loss: 0.0027871 | Time: 795ms
==> Save the model at epoch 013 with test loss 0.0027871
Epoch: 014 | Loss: 0.0028668 | L2 loss: 0.0027581 | Lip loss: 0.0001087 | Grad norm: 0.091167 | Time: 28s208ms
Epoch: 014 | Test Loss: 0.0027929 | Time: 902ms
Epoch: 015 | Loss: 0.0028707 | L2 loss: 0.0027617 | Lip loss: 0.0001090 | Grad norm: 0.098087 | Time: 28s395ms
Epoch: 015 | Test Loss: 0.0027914 | Time: 709ms
Epoch: 016 | Loss: 0.0028553 | L2 loss: 0.0027472 | Lip loss: 0.0001082 | Grad norm: 0.081736 | Time: 28s910ms
Epoch: 016 | Test Loss: 0.0027797 | Time: 788ms
==> Save the model at epoch 016 with test loss 0.0027797
Epoch: 017 | Loss: 0.0028552 | L2 loss: 0.0027464 | Lip loss: 0.0001087 | Grad norm: 0.083249 | Time: 28s835ms
Epoch: 017 | Test Loss: 0.0027845 | Time: 800ms
Epoch: 018 | Loss: 0.0028563 | L2 loss: 0.0027469 | Lip loss: 0.0001094 | Grad norm: 0.082135 | Time: 28s191ms
Epoch: 018 | Test Loss: 0.0027792 | Time: 788ms
==> Save the model at epoch 018 with test loss 0.0027792
Epoch: 019 | Loss: 0.0028545 | L2 loss: 0.0027461 | Lip loss: 0.0001084 | Grad norm: 0.082183 | Time: 27s694ms
Epoch: 019 | Test Loss: 0.0027786 | Time: 802ms
==> Save the model at epoch 019 with test loss 0.0027786
Epoch: 020 | Loss: 0.0028552 | L2 loss: 0.0027465 | Lip loss: 0.0001087 | Grad norm: 0.085513 | Time: 27s859ms
Epoch: 020 | Test Loss: 0.0027798 | Time: 788ms
Epoch: 021 | Loss: 0.0028539 | L2 loss: 0.0027450 | Lip loss: 0.0001088 | Grad norm: 0.079660 | Time: 28s251ms
Epoch: 021 | Test Loss: 0.0027781 | Time: 730ms
==> Save the model at epoch 021 with test loss 0.0027781
Epoch: 022 | Loss: 0.0028533 | L2 loss: 0.0027443 | Lip loss: 0.0001090 | Grad norm: 0.082512 | Time: 28s353ms
Epoch: 022 | Test Loss: 0.0027782 | Time: 787ms
Epoch: 023 | Loss: 0.0028526 | L2 loss: 0.0027440 | Lip loss: 0.0001085 | Grad norm: 0.080115 | Time: 28s68ms
Epoch: 023 | Test Loss: 0.0027782 | Time: 801ms
Epoch: 024 | Loss: 0.0028533 | L2 loss: 0.0027445 | Lip loss: 0.0001088 | Grad norm: 0.079083 | Time: 27s549ms
Epoch: 024 | Test Loss: 0.0027782 | Time: 718ms
Epoch: 025 | Loss: 0.0028540 | L2 loss: 0.0027448 | Lip loss: 0.0001092 | Grad norm: 0.080840 | Time: 27s986ms
Epoch: 025 | Test Loss: 0.0027786 | Time: 711ms
Epoch: 026 | Loss: 0.0028529 | L2 loss: 0.0027444 | Lip loss: 0.0001084 | Grad norm: 0.082879 | Time: 27s842ms
Epoch: 026 | Test Loss: 0.0027784 | Time: 795ms
Epoch: 027 | Loss: 0.0028534 | L2 loss: 0.0027444 | Lip loss: 0.0001090 | Grad norm: 0.079288 | Time: 28s21ms
Epoch: 027 | Test Loss: 0.0027782 | Time: 794ms
Epoch: 028 | Loss: 0.0028523 | L2 loss: 0.0027439 | Lip loss: 0.0001084 | Grad norm: 0.080764 | Time: 27s963ms
Epoch: 028 | Test Loss: 0.0027782 | Time: 799ms
Epoch: 029 | Loss: 0.0028538 | L2 loss: 0.0027448 | Lip loss: 0.0001090 | Grad norm: 0.080436 | Time: 27s873ms
Epoch: 029 | Test Loss: 0.0027781 | Time: 778ms
Epoch: 030 | Loss: 0.0028530 | L2 loss: 0.0027446 | Lip loss: 0.0001084 | Grad norm: 0.077715 | Time: 28s113ms
Epoch: 030 | Test Loss: 0.0027781 | Time: 726ms
Epoch: 031 | Loss: 0.0028538 | L2 loss: 0.0027445 | Lip loss: 0.0001093 | Grad norm: 0.078554 | Time: 28s324ms
Epoch: 031 | Test Loss: 0.0027781 | Time: 781ms
Epoch: 032 | Loss: 0.0028538 | L2 loss: 0.0027449 | Lip loss: 0.0001088 | Grad norm: 0.081095 | Time: 28s431ms
Epoch: 032 | Test Loss: 0.0027781 | Time: 783ms
Epoch: 033 | Loss: 0.0028531 | L2 loss: 0.0027443 | Lip loss: 0.0001088 | Grad norm: 0.080343 | Time: 28s137ms
Epoch: 033 | Test Loss: 0.0027781 | Time: 756ms
Epoch: 034 | Loss: 0.0028540 | L2 loss: 0.0027450 | Lip loss: 0.0001090 | Grad norm: 0.079872 | Time: 27s350ms
Epoch: 034 | Test Loss: 0.0027781 | Time: 739ms
Epoch: 035 | Loss: 0.0028530 | L2 loss: 0.0027449 | Lip loss: 0.0001081 | Grad norm: 0.082142 | Time: 28s120ms
Epoch: 035 | Test Loss: 0.0027781 | Time: 789ms
Epoch: 036 | Loss: 0.0028544 | L2 loss: 0.0027454 | Lip loss: 0.0001090 | Grad norm: 0.078004 | Time: 28s537ms
Epoch: 036 | Test Loss: 0.0027781 | Time: 804ms
Epoch: 037 | Loss: 0.0028529 | L2 loss: 0.0027443 | Lip loss: 0.0001086 | Grad norm: 0.077144 | Time: 28s322ms
Epoch: 037 | Test Loss: 0.0027781 | Time: 782ms
Epoch: 038 | Loss: 0.0028529 | L2 loss: 0.0027443 | Lip loss: 0.0001085 | Grad norm: 0.079753 | Time: 28s501ms
Epoch: 038 | Test Loss: 0.0027781 | Time: 794ms
Epoch: 039 | Loss: 0.0028530 | L2 loss: 0.0027443 | Lip loss: 0.0001087 | Grad norm: 0.077752 | Time: 27s980ms
Epoch: 039 | Test Loss: 0.0027781 | Time: 715ms
Epoch: 040 | Loss: 0.0028539 | L2 loss: 0.0027452 | Lip loss: 0.0001087 | Grad norm: 0.077816 | Time: 27s899ms
Epoch: 040 | Test Loss: 0.0027781 | Time: 786ms
Total time: 19m17s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
