==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Lipschitz constant: 2.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5123476 0.5241365]
==> Ouput transform to be applied to the neural network:
[1.908  1.9349]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 7.3935007 | Grad norm: 3.158935 | Time: 9s482ms
Epoch: 001 | Test Loss: 7.3730142 | Time: 623ms
==> Save the model at epoch 001 with test loss 7.3730142
Epoch: 002 | Train Loss: 1.2194093 | Grad norm: 3.023370 | Time: 9s154ms
Epoch: 002 | Test Loss: 0.0904244 | Time: 696ms
==> Save the model at epoch 002 with test loss 0.0904244
Epoch: 003 | Train Loss: 0.0705326 | Grad norm: 1.508077 | Time: 9s295ms
Epoch: 003 | Test Loss: 0.0450964 | Time: 655ms
==> Save the model at epoch 003 with test loss 0.0450964
Epoch: 004 | Train Loss: 0.0434980 | Grad norm: 2.085144 | Time: 9s452ms
Epoch: 004 | Test Loss: 0.0353657 | Time: 629ms
==> Save the model at epoch 004 with test loss 0.0353657
Epoch: 005 | Train Loss: 0.0335028 | Grad norm: 2.222624 | Time: 9s435ms
Epoch: 005 | Test Loss: 0.0224096 | Time: 633ms
==> Save the model at epoch 005 with test loss 0.0224096
Epoch: 006 | Train Loss: 0.0435537 | Grad norm: 4.670158 | Time: 9s865ms
Epoch: 006 | Test Loss: 0.0282505 | Time: 639ms
Epoch: 007 | Train Loss: 0.0448626 | Grad norm: 4.853504 | Time: 9s904ms
Epoch: 007 | Test Loss: 0.0301855 | Time: 624ms
Epoch: 008 | Train Loss: 0.0450137 | Grad norm: 4.925059 | Time: 9s299ms
Epoch: 008 | Test Loss: 0.0664545 | Time: 627ms
Epoch: 009 | Train Loss: 0.0421128 | Grad norm: 4.211629 | Time: 9s75ms
Epoch: 009 | Test Loss: 0.0545124 | Time: 693ms
Epoch: 010 | Train Loss: 0.0414768 | Grad norm: 4.521360 | Time: 9s746ms
Epoch: 010 | Test Loss: 0.0142050 | Time: 633ms
==> Save the model at epoch 010 with test loss 0.0142050
Epoch: 011 | Train Loss: 0.0201893 | Grad norm: 2.436895 | Time: 9s293ms
Epoch: 011 | Test Loss: 0.0189907 | Time: 690ms
Epoch: 012 | Train Loss: 0.0166337 | Grad norm: 2.207430 | Time: 9s400ms
Epoch: 012 | Test Loss: 0.0135936 | Time: 627ms
==> Save the model at epoch 012 with test loss 0.0135936
Epoch: 013 | Train Loss: 0.0153011 | Grad norm: 2.005767 | Time: 9s329ms
Epoch: 013 | Test Loss: 0.0116407 | Time: 629ms
==> Save the model at epoch 013 with test loss 0.0116407
Epoch: 014 | Train Loss: 0.0126646 | Grad norm: 1.968632 | Time: 9s964ms
Epoch: 014 | Test Loss: 0.0101189 | Time: 623ms
==> Save the model at epoch 014 with test loss 0.0101189
Epoch: 015 | Train Loss: 0.0122985 | Grad norm: 1.513911 | Time: 9s436ms
Epoch: 015 | Test Loss: 0.0087953 | Time: 620ms
==> Save the model at epoch 015 with test loss 0.0087953
Epoch: 016 | Train Loss: 0.0131090 | Grad norm: 1.832419 | Time: 9s344ms
Epoch: 016 | Test Loss: 0.0081748 | Time: 618ms
==> Save the model at epoch 016 with test loss 0.0081748
Epoch: 017 | Train Loss: 0.0099903 | Grad norm: 1.559216 | Time: 9s667ms
Epoch: 017 | Test Loss: 0.0111667 | Time: 621ms
Epoch: 018 | Train Loss: 0.0134091 | Grad norm: 2.211688 | Time: 9s775ms
Epoch: 018 | Test Loss: 0.0127303 | Time: 710ms
Epoch: 019 | Train Loss: 0.0150266 | Grad norm: 2.496800 | Time: 9s338ms
Epoch: 019 | Test Loss: 0.0238607 | Time: 626ms
Epoch: 020 | Train Loss: 0.0091607 | Grad norm: 1.467528 | Time: 9s496ms
Epoch: 020 | Test Loss: 0.0056268 | Time: 624ms
==> Save the model at epoch 020 with test loss 0.0056268
Epoch: 021 | Train Loss: 0.0054484 | Grad norm: 0.632399 | Time: 9s422ms
Epoch: 021 | Test Loss: 0.0042871 | Time: 628ms
==> Save the model at epoch 021 with test loss 0.0042871
Epoch: 022 | Train Loss: 0.0051166 | Grad norm: 0.613012 | Time: 9s402ms
Epoch: 022 | Test Loss: 0.0046661 | Time: 636ms
Epoch: 023 | Train Loss: 0.0049241 | Grad norm: 0.645763 | Time: 9s161ms
Epoch: 023 | Test Loss: 0.0056519 | Time: 626ms
Epoch: 024 | Train Loss: 0.0048565 | Grad norm: 0.611804 | Time: 9s422ms
Epoch: 024 | Test Loss: 0.0032628 | Time: 629ms
==> Save the model at epoch 024 with test loss 0.0032628
Epoch: 025 | Train Loss: 0.0042778 | Grad norm: 0.505579 | Time: 8s971ms
Epoch: 025 | Test Loss: 0.0036064 | Time: 695ms
Epoch: 026 | Train Loss: 0.0040264 | Grad norm: 0.435795 | Time: 9s540ms
Epoch: 026 | Test Loss: 0.0033527 | Time: 630ms
Epoch: 027 | Train Loss: 0.0041456 | Grad norm: 0.492922 | Time: 9s535ms
Epoch: 027 | Test Loss: 0.0036533 | Time: 624ms
Epoch: 028 | Train Loss: 0.0037732 | Grad norm: 0.415919 | Time: 9s260ms
Epoch: 028 | Test Loss: 0.0034751 | Time: 635ms
Epoch: 029 | Train Loss: 0.0037700 | Grad norm: 0.467210 | Time: 9s648ms
Epoch: 029 | Test Loss: 0.0032639 | Time: 626ms
Epoch: 030 | Train Loss: 0.0035564 | Grad norm: 0.379249 | Time: 9s442ms
Epoch: 030 | Test Loss: 0.0033736 | Time: 651ms
Epoch: 031 | Train Loss: 0.0035235 | Grad norm: 0.396431 | Time: 9s398ms
Epoch: 031 | Test Loss: 0.0029328 | Time: 625ms
==> Save the model at epoch 031 with test loss 0.0029328
Epoch: 032 | Train Loss: 0.0033945 | Grad norm: 0.365890 | Time: 9s264ms
Epoch: 032 | Test Loss: 0.0029218 | Time: 692ms
==> Save the model at epoch 032 with test loss 0.0029218
Epoch: 033 | Train Loss: 0.0032345 | Grad norm: 0.286697 | Time: 9s473ms
Epoch: 033 | Test Loss: 0.0029782 | Time: 632ms
Epoch: 034 | Train Loss: 0.0031802 | Grad norm: 0.284260 | Time: 9s401ms
Epoch: 034 | Test Loss: 0.0029768 | Time: 626ms
Epoch: 035 | Train Loss: 0.0030987 | Grad norm: 0.237994 | Time: 9s385ms
Epoch: 035 | Test Loss: 0.0026964 | Time: 631ms
==> Save the model at epoch 035 with test loss 0.0026964
Epoch: 036 | Train Loss: 0.0030160 | Grad norm: 0.216908 | Time: 9s419ms
Epoch: 036 | Test Loss: 0.0027096 | Time: 636ms
Epoch: 037 | Train Loss: 0.0029887 | Grad norm: 0.199530 | Time: 9s548ms
Epoch: 037 | Test Loss: 0.0027106 | Time: 624ms
Epoch: 038 | Train Loss: 0.0029570 | Grad norm: 0.184931 | Time: 9s15ms
Epoch: 038 | Test Loss: 0.0028379 | Time: 628ms
Epoch: 039 | Train Loss: 0.0029331 | Grad norm: 0.176447 | Time: 9s606ms
Epoch: 039 | Test Loss: 0.0026137 | Time: 693ms
==> Save the model at epoch 039 with test loss 0.0026137
Epoch: 040 | Train Loss: 0.0029066 | Grad norm: 0.156093 | Time: 9s521ms
Epoch: 040 | Test Loss: 0.0026001 | Time: 656ms
==> Save the model at epoch 040 with test loss 0.0026001
Total time: 6m43s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5123476 0.5241365]
==> Output transform to be applied to the neural network (trained):
[1.908  1.9349]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
