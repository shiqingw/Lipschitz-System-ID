==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 6.9002153 | L2 loss: 6.8979291 | Lip loss: 0.0022862 | Grad norm: 1.909618 | Time: 17s532ms
Epoch: 001 | Test Loss: 4.5880681 | Time: 440ms
==> Save the model at epoch 001 with test loss 4.5880681
Epoch: 002 | Loss: 1.3978579 | L2 loss: 1.3859186 | Lip loss: 0.0119393 | Grad norm: 3.293508 | Time: 17s254ms
Epoch: 002 | Test Loss: 0.2677917 | Time: 439ms
==> Save the model at epoch 002 with test loss 0.2677917
Epoch: 003 | Loss: 0.2404415 | L2 loss: 0.2264080 | Lip loss: 0.0140335 | Grad norm: 2.417706 | Time: 17s560ms
Epoch: 003 | Test Loss: 0.1895650 | Time: 547ms
==> Save the model at epoch 003 with test loss 0.1895650
Epoch: 004 | Loss: 0.1849702 | L2 loss: 0.1705184 | Lip loss: 0.0144519 | Grad norm: 2.726210 | Time: 17s399ms
Epoch: 004 | Test Loss: 0.1351840 | Time: 467ms
==> Save the model at epoch 004 with test loss 0.1351840
Epoch: 005 | Loss: 0.1345510 | L2 loss: 0.1198395 | Lip loss: 0.0147114 | Grad norm: 2.376530 | Time: 17s215ms
Epoch: 005 | Test Loss: 0.0953308 | Time: 441ms
==> Save the model at epoch 005 with test loss 0.0953308
Epoch: 006 | Loss: 0.1122847 | L2 loss: 0.0974077 | Lip loss: 0.0148771 | Grad norm: 1.640414 | Time: 17s138ms
Epoch: 006 | Test Loss: 0.0927284 | Time: 444ms
==> Save the model at epoch 006 with test loss 0.0927284
Epoch: 007 | Loss: 0.1091916 | L2 loss: 0.0942318 | Lip loss: 0.0149598 | Grad norm: 1.685823 | Time: 17s284ms
Epoch: 007 | Test Loss: 0.0888142 | Time: 450ms
==> Save the model at epoch 007 with test loss 0.0888142
Epoch: 008 | Loss: 0.1065952 | L2 loss: 0.0913623 | Lip loss: 0.0152329 | Grad norm: 1.714296 | Time: 17s259ms
Epoch: 008 | Test Loss: 0.0858425 | Time: 447ms
==> Save the model at epoch 008 with test loss 0.0858425
Epoch: 009 | Loss: 0.1034574 | L2 loss: 0.0883367 | Lip loss: 0.0151207 | Grad norm: 1.667716 | Time: 17s3ms
Epoch: 009 | Test Loss: 0.0829089 | Time: 446ms
==> Save the model at epoch 009 with test loss 0.0829089
Epoch: 010 | Loss: 0.1006805 | L2 loss: 0.0855443 | Lip loss: 0.0151362 | Grad norm: 1.728854 | Time: 17s443ms
Epoch: 010 | Test Loss: 0.0804781 | Time: 514ms
==> Save the model at epoch 010 with test loss 0.0804781
Epoch: 011 | Loss: 0.0991719 | L2 loss: 0.0839219 | Lip loss: 0.0152500 | Grad norm: 1.526527 | Time: 17s121ms
Epoch: 011 | Test Loss: 0.0800880 | Time: 443ms
==> Save the model at epoch 011 with test loss 0.0800880
Epoch: 012 | Loss: 0.0983666 | L2 loss: 0.0833961 | Lip loss: 0.0149706 | Grad norm: 1.484059 | Time: 17s173ms
Epoch: 012 | Test Loss: 0.0797316 | Time: 449ms
==> Save the model at epoch 012 with test loss 0.0797316
Epoch: 013 | Loss: 0.0979215 | L2 loss: 0.0830815 | Lip loss: 0.0148401 | Grad norm: 1.509445 | Time: 16s940ms
Epoch: 013 | Test Loss: 0.0794915 | Time: 436ms
==> Save the model at epoch 013 with test loss 0.0794915
Epoch: 014 | Loss: 0.0979652 | L2 loss: 0.0829924 | Lip loss: 0.0149728 | Grad norm: 1.514901 | Time: 17s400ms
Epoch: 014 | Test Loss: 0.0791602 | Time: 452ms
==> Save the model at epoch 014 with test loss 0.0791602
Epoch: 015 | Loss: 0.0982095 | L2 loss: 0.0829013 | Lip loss: 0.0153081 | Grad norm: 1.491126 | Time: 17s714ms
Epoch: 015 | Test Loss: 0.0789028 | Time: 446ms
==> Save the model at epoch 015 with test loss 0.0789028
Epoch: 016 | Loss: 0.0977465 | L2 loss: 0.0828789 | Lip loss: 0.0148676 | Grad norm: 1.466744 | Time: 17s145ms
Epoch: 016 | Test Loss: 0.0788748 | Time: 439ms
==> Save the model at epoch 016 with test loss 0.0788748
Epoch: 017 | Loss: 0.0984323 | L2 loss: 0.0833757 | Lip loss: 0.0150565 | Grad norm: 1.506027 | Time: 16s890ms
Epoch: 017 | Test Loss: 0.0788380 | Time: 527ms
==> Save the model at epoch 017 with test loss 0.0788380
Epoch: 018 | Loss: 0.0973148 | L2 loss: 0.0823388 | Lip loss: 0.0149760 | Grad norm: 1.457590 | Time: 17s429ms
Epoch: 018 | Test Loss: 0.0788198 | Time: 435ms
==> Save the model at epoch 018 with test loss 0.0788198
Epoch: 019 | Loss: 0.0974645 | L2 loss: 0.0824138 | Lip loss: 0.0150508 | Grad norm: 1.518061 | Time: 16s826ms
Epoch: 019 | Test Loss: 0.0788073 | Time: 444ms
==> Save the model at epoch 019 with test loss 0.0788073
Epoch: 020 | Loss: 0.0974568 | L2 loss: 0.0822437 | Lip loss: 0.0152131 | Grad norm: 1.496752 | Time: 16s746ms
Epoch: 020 | Test Loss: 0.0787648 | Time: 448ms
==> Save the model at epoch 020 with test loss 0.0787648
Epoch: 021 | Loss: 0.0975881 | L2 loss: 0.0822562 | Lip loss: 0.0153320 | Grad norm: 1.484223 | Time: 17s63ms
Epoch: 021 | Test Loss: 0.0787619 | Time: 447ms
==> Save the model at epoch 021 with test loss 0.0787619
Epoch: 022 | Loss: 0.0975534 | L2 loss: 0.0822313 | Lip loss: 0.0153221 | Grad norm: 1.376157 | Time: 17s40ms
Epoch: 022 | Test Loss: 0.0787602 | Time: 471ms
==> Save the model at epoch 022 with test loss 0.0787602
Epoch: 023 | Loss: 0.0973456 | L2 loss: 0.0823554 | Lip loss: 0.0149901 | Grad norm: 1.498021 | Time: 17s234ms
Epoch: 023 | Test Loss: 0.0787573 | Time: 434ms
==> Save the model at epoch 023 with test loss 0.0787573
Epoch: 024 | Loss: 0.0971888 | L2 loss: 0.0822357 | Lip loss: 0.0149531 | Grad norm: 1.524743 | Time: 16s956ms
Epoch: 024 | Test Loss: 0.0787529 | Time: 551ms
==> Save the model at epoch 024 with test loss 0.0787529
Epoch: 025 | Loss: 0.0973448 | L2 loss: 0.0822123 | Lip loss: 0.0151325 | Grad norm: 1.403784 | Time: 17s843ms
Epoch: 025 | Test Loss: 0.0787518 | Time: 462ms
==> Save the model at epoch 025 with test loss 0.0787518
Epoch: 026 | Loss: 0.0973002 | L2 loss: 0.0822744 | Lip loss: 0.0150258 | Grad norm: 1.493644 | Time: 17s504ms
Epoch: 026 | Test Loss: 0.0787518 | Time: 443ms
==> Save the model at epoch 026 with test loss 0.0787518
Epoch: 027 | Loss: 0.0973925 | L2 loss: 0.0822932 | Lip loss: 0.0150993 | Grad norm: 1.419934 | Time: 17s53ms
Epoch: 027 | Test Loss: 0.0787517 | Time: 437ms
==> Save the model at epoch 027 with test loss 0.0787517
Epoch: 028 | Loss: 0.0981633 | L2 loss: 0.0830362 | Lip loss: 0.0151271 | Grad norm: 1.450224 | Time: 17s339ms
Epoch: 028 | Test Loss: 0.0787517 | Time: 438ms
==> Save the model at epoch 028 with test loss 0.0787517
Epoch: 029 | Loss: 0.0978570 | L2 loss: 0.0825065 | Lip loss: 0.0153505 | Grad norm: 1.475860 | Time: 16s912ms
Epoch: 029 | Test Loss: 0.0787517 | Time: 447ms
==> Save the model at epoch 029 with test loss 0.0787517
Epoch: 030 | Loss: 0.0971845 | L2 loss: 0.0822835 | Lip loss: 0.0149011 | Grad norm: 1.441069 | Time: 17s28ms
Epoch: 030 | Test Loss: 0.0787516 | Time: 438ms
==> Save the model at epoch 030 with test loss 0.0787516
Epoch: 031 | Loss: 0.0983968 | L2 loss: 0.0833867 | Lip loss: 0.0150101 | Grad norm: 1.521762 | Time: 16s943ms
Epoch: 031 | Test Loss: 0.0787516 | Time: 438ms
==> Save the model at epoch 031 with test loss 0.0787516
Epoch: 032 | Loss: 0.0976567 | L2 loss: 0.0822531 | Lip loss: 0.0154036 | Grad norm: 1.416452 | Time: 16s718ms
Epoch: 032 | Test Loss: 0.0787516 | Time: 450ms
Epoch: 033 | Loss: 0.0975832 | L2 loss: 0.0825733 | Lip loss: 0.0150099 | Grad norm: 1.459577 | Time: 17s264ms
Epoch: 033 | Test Loss: 0.0787516 | Time: 437ms
==> Save the model at epoch 033 with test loss 0.0787516
Epoch: 034 | Loss: 0.0970592 | L2 loss: 0.0822740 | Lip loss: 0.0147852 | Grad norm: 1.516787 | Time: 16s960ms
Epoch: 034 | Test Loss: 0.0787516 | Time: 444ms
Epoch: 035 | Loss: 0.0978152 | L2 loss: 0.0824019 | Lip loss: 0.0154133 | Grad norm: 1.423732 | Time: 17s22ms
Epoch: 035 | Test Loss: 0.0787516 | Time: 444ms
==> Save the model at epoch 035 with test loss 0.0787516
Epoch: 036 | Loss: 0.0974340 | L2 loss: 0.0822587 | Lip loss: 0.0151752 | Grad norm: 1.495301 | Time: 17s544ms
Epoch: 036 | Test Loss: 0.0787516 | Time: 543ms
Epoch: 037 | Loss: 0.0973930 | L2 loss: 0.0822947 | Lip loss: 0.0150983 | Grad norm: 1.485458 | Time: 16s865ms
Epoch: 037 | Test Loss: 0.0787516 | Time: 440ms
==> Save the model at epoch 037 with test loss 0.0787516
Epoch: 038 | Loss: 0.0973306 | L2 loss: 0.0822189 | Lip loss: 0.0151117 | Grad norm: 1.444408 | Time: 17s320ms
Epoch: 038 | Test Loss: 0.0787516 | Time: 447ms
Epoch: 039 | Loss: 0.0972804 | L2 loss: 0.0822256 | Lip loss: 0.0150549 | Grad norm: 1.492537 | Time: 16s926ms
Epoch: 039 | Test Loss: 0.0787516 | Time: 460ms
Epoch: 040 | Loss: 0.0972953 | L2 loss: 0.0822287 | Lip loss: 0.0150666 | Grad norm: 1.476667 | Time: 17s59ms
Epoch: 040 | Test Loss: 0.0787516 | Time: 461ms
Total time: 11m45s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
