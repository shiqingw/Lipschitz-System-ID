==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 0.7575785 | L2 loss: 0.7440624 | Lip loss: 0.0135161 | Grad norm: 4.622674 | Time: 34s719ms
Epoch: 001 | Test Loss: 0.0656252 | Time: 434ms
==> Save the model at epoch 001 with test loss 0.0656252
Epoch: 002 | Loss: 0.0602600 | L2 loss: 0.0456187 | Lip loss: 0.0146413 | Grad norm: 4.556160 | Time: 34s199ms
Epoch: 002 | Test Loss: 0.0290487 | Time: 434ms
==> Save the model at epoch 002 with test loss 0.0290487
Epoch: 003 | Loss: 0.0384513 | L2 loss: 0.0238775 | Lip loss: 0.0145739 | Grad norm: 3.578545 | Time: 34s420ms
Epoch: 003 | Test Loss: 0.0127889 | Time: 448ms
==> Save the model at epoch 003 with test loss 0.0127889
Epoch: 004 | Loss: 0.0379018 | L2 loss: 0.0233363 | Lip loss: 0.0145655 | Grad norm: 3.239510 | Time: 34s967ms
Epoch: 004 | Test Loss: 0.0229924 | Time: 430ms
Epoch: 005 | Loss: 0.0348573 | L2 loss: 0.0201986 | Lip loss: 0.0146587 | Grad norm: 2.677666 | Time: 34s636ms
Epoch: 005 | Test Loss: 0.0222482 | Time: 438ms
Epoch: 006 | Loss: 0.0226693 | L2 loss: 0.0081481 | Lip loss: 0.0145212 | Grad norm: 0.538225 | Time: 34s243ms
Epoch: 006 | Test Loss: 0.0079791 | Time: 499ms
==> Save the model at epoch 006 with test loss 0.0079791
Epoch: 007 | Loss: 0.0222253 | L2 loss: 0.0077406 | Lip loss: 0.0144847 | Grad norm: 0.510390 | Time: 34s329ms
Epoch: 007 | Test Loss: 0.0075188 | Time: 509ms
==> Save the model at epoch 007 with test loss 0.0075188
Epoch: 008 | Loss: 0.0220133 | L2 loss: 0.0076292 | Lip loss: 0.0143841 | Grad norm: 0.550839 | Time: 35s284ms
Epoch: 008 | Test Loss: 0.0072126 | Time: 429ms
==> Save the model at epoch 008 with test loss 0.0072126
Epoch: 009 | Loss: 0.0223774 | L2 loss: 0.0075897 | Lip loss: 0.0147877 | Grad norm: 0.525407 | Time: 34s689ms
Epoch: 009 | Test Loss: 0.0071448 | Time: 429ms
==> Save the model at epoch 009 with test loss 0.0071448
Epoch: 010 | Loss: 0.0220621 | L2 loss: 0.0075006 | Lip loss: 0.0145615 | Grad norm: 0.491781 | Time: 34s283ms
Epoch: 010 | Test Loss: 0.0074835 | Time: 445ms
Epoch: 011 | Loss: 0.0214743 | L2 loss: 0.0071062 | Lip loss: 0.0143681 | Grad norm: 0.317856 | Time: 34s331ms
Epoch: 011 | Test Loss: 0.0069518 | Time: 440ms
==> Save the model at epoch 011 with test loss 0.0069518
Epoch: 012 | Loss: 0.0215702 | L2 loss: 0.0070691 | Lip loss: 0.0145011 | Grad norm: 0.304635 | Time: 34s592ms
Epoch: 012 | Test Loss: 0.0069976 | Time: 438ms
Epoch: 013 | Loss: 0.0213840 | L2 loss: 0.0070583 | Lip loss: 0.0143257 | Grad norm: 0.300926 | Time: 34s860ms
Epoch: 013 | Test Loss: 0.0070980 | Time: 436ms
Epoch: 014 | Loss: 0.0216069 | L2 loss: 0.0070622 | Lip loss: 0.0145447 | Grad norm: 0.298768 | Time: 34s600ms
Epoch: 014 | Test Loss: 0.0069362 | Time: 429ms
==> Save the model at epoch 014 with test loss 0.0069362
Epoch: 015 | Loss: 0.0213395 | L2 loss: 0.0070659 | Lip loss: 0.0142737 | Grad norm: 0.310996 | Time: 34s841ms
Epoch: 015 | Test Loss: 0.0069287 | Time: 507ms
==> Save the model at epoch 015 with test loss 0.0069287
Epoch: 016 | Loss: 0.0214628 | L2 loss: 0.0070131 | Lip loss: 0.0144497 | Grad norm: 0.273951 | Time: 34s895ms
Epoch: 016 | Test Loss: 0.0069034 | Time: 519ms
==> Save the model at epoch 016 with test loss 0.0069034
Epoch: 017 | Loss: 0.0217295 | L2 loss: 0.0070049 | Lip loss: 0.0147247 | Grad norm: 0.267578 | Time: 35s314ms
Epoch: 017 | Test Loss: 0.0069032 | Time: 443ms
==> Save the model at epoch 017 with test loss 0.0069032
Epoch: 018 | Loss: 0.0217081 | L2 loss: 0.0070086 | Lip loss: 0.0146995 | Grad norm: 0.276010 | Time: 34s662ms
Epoch: 018 | Test Loss: 0.0069026 | Time: 446ms
==> Save the model at epoch 018 with test loss 0.0069026
Epoch: 019 | Loss: 0.0213678 | L2 loss: 0.0070058 | Lip loss: 0.0143620 | Grad norm: 0.272933 | Time: 34s552ms
Epoch: 019 | Test Loss: 0.0069009 | Time: 447ms
==> Save the model at epoch 019 with test loss 0.0069009
Epoch: 020 | Loss: 0.0214942 | L2 loss: 0.0070014 | Lip loss: 0.0144928 | Grad norm: 0.265747 | Time: 34s544ms
Epoch: 020 | Test Loss: 0.0069068 | Time: 441ms
Epoch: 021 | Loss: 0.0213849 | L2 loss: 0.0069985 | Lip loss: 0.0143864 | Grad norm: 0.268779 | Time: 35s5ms
Epoch: 021 | Test Loss: 0.0068993 | Time: 443ms
==> Save the model at epoch 021 with test loss 0.0068993
Epoch: 022 | Loss: 0.0217173 | L2 loss: 0.0069964 | Lip loss: 0.0147210 | Grad norm: 0.268228 | Time: 34s535ms
Epoch: 022 | Test Loss: 0.0069008 | Time: 434ms
Epoch: 023 | Loss: 0.0216519 | L2 loss: 0.0069986 | Lip loss: 0.0146534 | Grad norm: 0.262472 | Time: 35s221ms
Epoch: 023 | Test Loss: 0.0068994 | Time: 431ms
Epoch: 024 | Loss: 0.0216845 | L2 loss: 0.0069976 | Lip loss: 0.0146869 | Grad norm: 0.264344 | Time: 35s299ms
Epoch: 024 | Test Loss: 0.0069006 | Time: 505ms
Epoch: 025 | Loss: 0.0214042 | L2 loss: 0.0069974 | Lip loss: 0.0144068 | Grad norm: 0.268770 | Time: 35s68ms
Epoch: 025 | Test Loss: 0.0068999 | Time: 502ms
Epoch: 026 | Loss: 0.0214208 | L2 loss: 0.0069970 | Lip loss: 0.0144239 | Grad norm: 0.265138 | Time: 34s779ms
Epoch: 026 | Test Loss: 0.0068997 | Time: 452ms
Epoch: 027 | Loss: 0.0217209 | L2 loss: 0.0069970 | Lip loss: 0.0147238 | Grad norm: 0.268521 | Time: 34s702ms
Epoch: 027 | Test Loss: 0.0068995 | Time: 449ms
Epoch: 028 | Loss: 0.0216251 | L2 loss: 0.0069970 | Lip loss: 0.0146280 | Grad norm: 0.260512 | Time: 35s266ms
Epoch: 028 | Test Loss: 0.0068995 | Time: 431ms
Epoch: 029 | Loss: 0.0213244 | L2 loss: 0.0069969 | Lip loss: 0.0143275 | Grad norm: 0.264671 | Time: 35s377ms
Epoch: 029 | Test Loss: 0.0068994 | Time: 437ms
Epoch: 030 | Loss: 0.0219476 | L2 loss: 0.0069971 | Lip loss: 0.0149506 | Grad norm: 0.271298 | Time: 34s685ms
Epoch: 030 | Test Loss: 0.0068993 | Time: 427ms
Epoch: 031 | Loss: 0.0215015 | L2 loss: 0.0069969 | Lip loss: 0.0145046 | Grad norm: 0.263906 | Time: 34s812ms
Epoch: 031 | Test Loss: 0.0068993 | Time: 429ms
Epoch: 032 | Loss: 0.0214141 | L2 loss: 0.0069969 | Lip loss: 0.0144172 | Grad norm: 0.263853 | Time: 34s226ms
Epoch: 032 | Test Loss: 0.0068993 | Time: 512ms
Epoch: 033 | Loss: 0.0215759 | L2 loss: 0.0069969 | Lip loss: 0.0145790 | Grad norm: 0.265988 | Time: 34s876ms
Epoch: 033 | Test Loss: 0.0068993 | Time: 428ms
Epoch: 034 | Loss: 0.0215868 | L2 loss: 0.0069969 | Lip loss: 0.0145899 | Grad norm: 0.267159 | Time: 35s266ms
Epoch: 034 | Test Loss: 0.0068993 | Time: 507ms
==> Save the model at epoch 034 with test loss 0.0068993
Epoch: 035 | Loss: 0.0217618 | L2 loss: 0.0069969 | Lip loss: 0.0147649 | Grad norm: 0.277810 | Time: 35s218ms
Epoch: 035 | Test Loss: 0.0068993 | Time: 506ms
==> Save the model at epoch 035 with test loss 0.0068993
Epoch: 036 | Loss: 0.0211805 | L2 loss: 0.0069969 | Lip loss: 0.0141836 | Grad norm: 0.260461 | Time: 34s977ms
Epoch: 036 | Test Loss: 0.0068993 | Time: 441ms
==> Save the model at epoch 036 with test loss 0.0068993
Epoch: 037 | Loss: 0.0213399 | L2 loss: 0.0069969 | Lip loss: 0.0143430 | Grad norm: 0.261848 | Time: 35s67ms
Epoch: 037 | Test Loss: 0.0068993 | Time: 429ms
Epoch: 038 | Loss: 0.0216460 | L2 loss: 0.0069969 | Lip loss: 0.0146491 | Grad norm: 0.262107 | Time: 34s603ms
Epoch: 038 | Test Loss: 0.0068993 | Time: 439ms
Epoch: 039 | Loss: 0.0215624 | L2 loss: 0.0069969 | Lip loss: 0.0145655 | Grad norm: 0.257154 | Time: 34s826ms
Epoch: 039 | Test Loss: 0.0068993 | Time: 432ms
Epoch: 040 | Loss: 0.0213531 | L2 loss: 0.0069969 | Lip loss: 0.0143562 | Grad norm: 0.267705 | Time: 35s238ms
Epoch: 040 | Test Loss: 0.0068993 | Time: 431ms
Total time: 23m30s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
