==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 1.3947416 | L2 loss: 1.3947403 | Lip loss: 0.0000013 | Grad norm: 4.416656 | Time: 17s507ms
Epoch: 001 | Test Loss: 0.1393863 | Time: 452ms
==> Save the model at epoch 001 with test loss 0.1393863
Epoch: 002 | Loss: 0.1158672 | L2 loss: 0.1158657 | Lip loss: 0.0000015 | Grad norm: 5.419476 | Time: 17s245ms
Epoch: 002 | Test Loss: 0.0987971 | Time: 448ms
==> Save the model at epoch 002 with test loss 0.0987971
Epoch: 003 | Loss: 0.0572720 | L2 loss: 0.0572705 | Lip loss: 0.0000015 | Grad norm: 3.795292 | Time: 17s38ms
Epoch: 003 | Test Loss: 0.0375109 | Time: 511ms
==> Save the model at epoch 003 with test loss 0.0375109
Epoch: 004 | Loss: 0.0538946 | L2 loss: 0.0538931 | Lip loss: 0.0000014 | Grad norm: 6.436379 | Time: 17s209ms
Epoch: 004 | Test Loss: 0.0663736 | Time: 449ms
Epoch: 005 | Loss: 0.0375129 | L2 loss: 0.0375114 | Lip loss: 0.0000015 | Grad norm: 4.716592 | Time: 17s453ms
Epoch: 005 | Test Loss: 0.0315771 | Time: 448ms
==> Save the model at epoch 005 with test loss 0.0315771
Epoch: 006 | Loss: 0.0117025 | L2 loss: 0.0117011 | Lip loss: 0.0000015 | Grad norm: 0.826232 | Time: 17s201ms
Epoch: 006 | Test Loss: 0.0102532 | Time: 450ms
==> Save the model at epoch 006 with test loss 0.0102532
Epoch: 007 | Loss: 0.0102880 | L2 loss: 0.0102865 | Lip loss: 0.0000015 | Grad norm: 0.774943 | Time: 17s54ms
Epoch: 007 | Test Loss: 0.0095336 | Time: 447ms
==> Save the model at epoch 007 with test loss 0.0095336
Epoch: 008 | Loss: 0.0097278 | L2 loss: 0.0097263 | Lip loss: 0.0000015 | Grad norm: 0.753517 | Time: 17s409ms
Epoch: 008 | Test Loss: 0.0090921 | Time: 439ms
==> Save the model at epoch 008 with test loss 0.0090921
Epoch: 009 | Loss: 0.0092926 | L2 loss: 0.0092911 | Lip loss: 0.0000015 | Grad norm: 0.696542 | Time: 17s30ms
Epoch: 009 | Test Loss: 0.0091153 | Time: 453ms
Epoch: 010 | Loss: 0.0089603 | L2 loss: 0.0089588 | Lip loss: 0.0000015 | Grad norm: 0.636013 | Time: 17s135ms
Epoch: 010 | Test Loss: 0.0086216 | Time: 508ms
==> Save the model at epoch 010 with test loss 0.0086216
Epoch: 011 | Loss: 0.0084479 | L2 loss: 0.0084465 | Lip loss: 0.0000015 | Grad norm: 0.416078 | Time: 17s294ms
Epoch: 011 | Test Loss: 0.0083757 | Time: 451ms
==> Save the model at epoch 011 with test loss 0.0083757
Epoch: 012 | Loss: 0.0083948 | L2 loss: 0.0083934 | Lip loss: 0.0000015 | Grad norm: 0.385677 | Time: 17s560ms
Epoch: 012 | Test Loss: 0.0083171 | Time: 441ms
==> Save the model at epoch 012 with test loss 0.0083171
Epoch: 013 | Loss: 0.0083718 | L2 loss: 0.0083704 | Lip loss: 0.0000015 | Grad norm: 0.391731 | Time: 17s271ms
Epoch: 013 | Test Loss: 0.0083026 | Time: 450ms
==> Save the model at epoch 013 with test loss 0.0083026
Epoch: 014 | Loss: 0.0083791 | L2 loss: 0.0083776 | Lip loss: 0.0000015 | Grad norm: 0.397205 | Time: 17s874ms
Epoch: 014 | Test Loss: 0.0082609 | Time: 453ms
==> Save the model at epoch 014 with test loss 0.0082609
Epoch: 015 | Loss: 0.0083812 | L2 loss: 0.0083798 | Lip loss: 0.0000015 | Grad norm: 0.402547 | Time: 17s919ms
Epoch: 015 | Test Loss: 0.0082478 | Time: 440ms
==> Save the model at epoch 015 with test loss 0.0082478
Epoch: 016 | Loss: 0.0082672 | L2 loss: 0.0082657 | Lip loss: 0.0000015 | Grad norm: 0.346293 | Time: 17s760ms
Epoch: 016 | Test Loss: 0.0082319 | Time: 433ms
==> Save the model at epoch 016 with test loss 0.0082319
Epoch: 017 | Loss: 0.0082700 | L2 loss: 0.0082685 | Lip loss: 0.0000014 | Grad norm: 0.362475 | Time: 17s226ms
Epoch: 017 | Test Loss: 0.0082247 | Time: 522ms
==> Save the model at epoch 017 with test loss 0.0082247
Epoch: 018 | Loss: 0.0082637 | L2 loss: 0.0082622 | Lip loss: 0.0000015 | Grad norm: 0.348573 | Time: 17s285ms
Epoch: 018 | Test Loss: 0.0082263 | Time: 450ms
Epoch: 019 | Loss: 0.0082651 | L2 loss: 0.0082636 | Lip loss: 0.0000015 | Grad norm: 0.342805 | Time: 17s170ms
Epoch: 019 | Test Loss: 0.0082231 | Time: 438ms
==> Save the model at epoch 019 with test loss 0.0082231
Epoch: 020 | Loss: 0.0082545 | L2 loss: 0.0082530 | Lip loss: 0.0000015 | Grad norm: 0.359407 | Time: 17s76ms
Epoch: 020 | Test Loss: 0.0082263 | Time: 455ms
Epoch: 021 | Loss: 0.0082447 | L2 loss: 0.0082432 | Lip loss: 0.0000015 | Grad norm: 0.342403 | Time: 17s865ms
Epoch: 021 | Test Loss: 0.0082241 | Time: 463ms
Epoch: 022 | Loss: 0.0082479 | L2 loss: 0.0082464 | Lip loss: 0.0000015 | Grad norm: 0.347351 | Time: 17s573ms
Epoch: 022 | Test Loss: 0.0082241 | Time: 446ms
Epoch: 023 | Loss: 0.0082458 | L2 loss: 0.0082443 | Lip loss: 0.0000015 | Grad norm: 0.340985 | Time: 17s768ms
Epoch: 023 | Test Loss: 0.0082219 | Time: 444ms
==> Save the model at epoch 023 with test loss 0.0082219
Epoch: 024 | Loss: 0.0082533 | L2 loss: 0.0082519 | Lip loss: 0.0000014 | Grad norm: 0.329213 | Time: 17s456ms
Epoch: 024 | Test Loss: 0.0082210 | Time: 508ms
==> Save the model at epoch 024 with test loss 0.0082210
Epoch: 025 | Loss: 0.0082566 | L2 loss: 0.0082552 | Lip loss: 0.0000014 | Grad norm: 0.340466 | Time: 17s644ms
Epoch: 025 | Test Loss: 0.0082205 | Time: 447ms
==> Save the model at epoch 025 with test loss 0.0082205
Epoch: 026 | Loss: 0.0082442 | L2 loss: 0.0082427 | Lip loss: 0.0000015 | Grad norm: 0.355938 | Time: 17s818ms
Epoch: 026 | Test Loss: 0.0082206 | Time: 453ms
Epoch: 027 | Loss: 0.0082527 | L2 loss: 0.0082512 | Lip loss: 0.0000015 | Grad norm: 0.348092 | Time: 17s368ms
Epoch: 027 | Test Loss: 0.0082207 | Time: 448ms
Epoch: 028 | Loss: 0.0082448 | L2 loss: 0.0082433 | Lip loss: 0.0000015 | Grad norm: 0.346554 | Time: 17s714ms
Epoch: 028 | Test Loss: 0.0082208 | Time: 437ms
Epoch: 029 | Loss: 0.0082419 | L2 loss: 0.0082405 | Lip loss: 0.0000014 | Grad norm: 0.346793 | Time: 17s35ms
Epoch: 029 | Test Loss: 0.0082208 | Time: 446ms
Epoch: 030 | Loss: 0.0082427 | L2 loss: 0.0082412 | Lip loss: 0.0000015 | Grad norm: 0.339114 | Time: 17s582ms
Epoch: 030 | Test Loss: 0.0082210 | Time: 450ms
Epoch: 031 | Loss: 0.0082482 | L2 loss: 0.0082467 | Lip loss: 0.0000015 | Grad norm: 0.348293 | Time: 17s83ms
Epoch: 031 | Test Loss: 0.0082210 | Time: 447ms
Epoch: 032 | Loss: 0.0082551 | L2 loss: 0.0082536 | Lip loss: 0.0000014 | Grad norm: 0.346557 | Time: 17s478ms
Epoch: 032 | Test Loss: 0.0082210 | Time: 452ms
Epoch: 033 | Loss: 0.0082441 | L2 loss: 0.0082426 | Lip loss: 0.0000014 | Grad norm: 0.331722 | Time: 17s434ms
Epoch: 033 | Test Loss: 0.0082210 | Time: 460ms
Epoch: 034 | Loss: 0.0082491 | L2 loss: 0.0082476 | Lip loss: 0.0000015 | Grad norm: 0.342011 | Time: 17s219ms
Epoch: 034 | Test Loss: 0.0082210 | Time: 440ms
Epoch: 035 | Loss: 0.0082534 | L2 loss: 0.0082519 | Lip loss: 0.0000015 | Grad norm: 0.360413 | Time: 17s213ms
Epoch: 035 | Test Loss: 0.0082210 | Time: 431ms
Epoch: 036 | Loss: 0.0082433 | L2 loss: 0.0082418 | Lip loss: 0.0000015 | Grad norm: 0.338131 | Time: 17s366ms
Epoch: 036 | Test Loss: 0.0082210 | Time: 530ms
Epoch: 037 | Loss: 0.0082456 | L2 loss: 0.0082441 | Lip loss: 0.0000015 | Grad norm: 0.339031 | Time: 18s99ms
Epoch: 037 | Test Loss: 0.0082210 | Time: 447ms
Epoch: 038 | Loss: 0.0082529 | L2 loss: 0.0082514 | Lip loss: 0.0000015 | Grad norm: 0.337310 | Time: 17s644ms
Epoch: 038 | Test Loss: 0.0082210 | Time: 438ms
Epoch: 039 | Loss: 0.0082475 | L2 loss: 0.0082461 | Lip loss: 0.0000014 | Grad norm: 0.353195 | Time: 17s86ms
Epoch: 039 | Test Loss: 0.0082210 | Time: 447ms
Epoch: 040 | Loss: 0.0082503 | L2 loss: 0.0082488 | Lip loss: 0.0000015 | Grad norm: 0.324036 | Time: 17s331ms
Epoch: 040 | Test Loss: 0.0082210 | Time: 449ms
Total time: 11m54s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
