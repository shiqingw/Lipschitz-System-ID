==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Lipschitz constant: 16.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (M): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.6496010 | Grad norm: 96.072248 | Time: 5s566ms
Epoch: 001 | Test Loss: 30.1808543 | Time: 817ms
==> Save the model at epoch 001 with test loss 30.1808543
Epoch: 002 | Train Loss: 3.8200378 | Grad norm: 32.826767 | Time: 5s552ms
Epoch: 002 | Test Loss: 0.0163602 | Time: 834ms
==> Save the model at epoch 002 with test loss 0.0163602
Epoch: 003 | Train Loss: 0.0349660 | Grad norm: 14.169107 | Time: 5s121ms
Epoch: 003 | Test Loss: 0.0568649 | Time: 811ms
Epoch: 004 | Train Loss: 0.0414114 | Grad norm: 12.982262 | Time: 5s631ms
Epoch: 004 | Test Loss: 0.0267532 | Time: 815ms
Epoch: 005 | Train Loss: 0.0562170 | Grad norm: 16.779896 | Time: 5s453ms
Epoch: 005 | Test Loss: 0.0590754 | Time: 795ms
Epoch: 006 | Train Loss: 0.0697417 | Grad norm: 18.717329 | Time: 5s491ms
Epoch: 006 | Test Loss: 0.0624500 | Time: 814ms
Epoch: 007 | Train Loss: 0.1027010 | Grad norm: 20.061069 | Time: 4s260ms
Epoch: 007 | Test Loss: 0.0636473 | Time: 845ms
Epoch: 008 | Train Loss: 0.1082020 | Grad norm: 20.910700 | Time: 4s225ms
Epoch: 008 | Test Loss: 0.1306761 | Time: 808ms
Epoch: 009 | Train Loss: 0.1295237 | Grad norm: 21.048587 | Time: 4s234ms
Epoch: 009 | Test Loss: 0.0907353 | Time: 781ms
Epoch: 010 | Train Loss: 0.1052856 | Grad norm: 20.480177 | Time: 4s213ms
Epoch: 010 | Test Loss: 0.1238404 | Time: 786ms
Epoch: 011 | Train Loss: 0.0905026 | Grad norm: 18.366506 | Time: 4s359ms
Epoch: 011 | Test Loss: 0.1222124 | Time: 839ms
Epoch: 012 | Train Loss: 0.1049864 | Grad norm: 17.430645 | Time: 4s213ms
Epoch: 012 | Test Loss: 0.1422487 | Time: 780ms
Epoch: 013 | Train Loss: 0.0836611 | Grad norm: 14.484164 | Time: 4s428ms
Epoch: 013 | Test Loss: 0.0416762 | Time: 784ms
Epoch: 014 | Train Loss: 0.0698518 | Grad norm: 15.252555 | Time: 5s296ms
Epoch: 014 | Test Loss: 0.0518879 | Time: 818ms
Epoch: 015 | Train Loss: 0.0630618 | Grad norm: 14.148124 | Time: 5s265ms
Epoch: 015 | Test Loss: 0.0669593 | Time: 789ms
Epoch: 016 | Train Loss: 0.0554625 | Grad norm: 12.132684 | Time: 4s733ms
Epoch: 016 | Test Loss: 0.0981523 | Time: 779ms
Epoch: 017 | Train Loss: 0.0505756 | Grad norm: 12.138670 | Time: 4s477ms
Epoch: 017 | Test Loss: 0.0335037 | Time: 778ms
Epoch: 018 | Train Loss: 0.0501847 | Grad norm: 9.854849 | Time: 4s509ms
Epoch: 018 | Test Loss: 0.0590505 | Time: 783ms
Epoch: 019 | Train Loss: 0.0494928 | Grad norm: 11.376620 | Time: 4s347ms
Epoch: 019 | Test Loss: 0.0369603 | Time: 816ms
Epoch: 020 | Train Loss: 0.0370320 | Grad norm: 10.069628 | Time: 4s278ms
Epoch: 020 | Test Loss: 0.0465089 | Time: 819ms
Epoch: 021 | Train Loss: 0.0347567 | Grad norm: 9.073633 | Time: 4s259ms
Epoch: 021 | Test Loss: 0.0346674 | Time: 817ms
Epoch: 022 | Train Loss: 0.0273618 | Grad norm: 7.924695 | Time: 4s296ms
Epoch: 022 | Test Loss: 0.0307745 | Time: 847ms
Epoch: 023 | Train Loss: 0.0254352 | Grad norm: 6.309310 | Time: 4s262ms
Epoch: 023 | Test Loss: 0.0144182 | Time: 803ms
==> Save the model at epoch 023 with test loss 0.0144182
Epoch: 024 | Train Loss: 0.0298046 | Grad norm: 6.475477 | Time: 5s199ms
Epoch: 024 | Test Loss: 0.0338925 | Time: 797ms
Epoch: 025 | Train Loss: 0.0206301 | Grad norm: 5.332217 | Time: 5s81ms
Epoch: 025 | Test Loss: 0.0229116 | Time: 782ms
Epoch: 026 | Train Loss: 0.0180001 | Grad norm: 5.221902 | Time: 5s91ms
Epoch: 026 | Test Loss: 0.0194676 | Time: 818ms
Epoch: 027 | Train Loss: 0.0153513 | Grad norm: 5.329069 | Time: 5s105ms
Epoch: 027 | Test Loss: 0.0112263 | Time: 834ms
==> Save the model at epoch 027 with test loss 0.0112263
Epoch: 028 | Train Loss: 0.0132669 | Grad norm: 4.520840 | Time: 4s588ms
Epoch: 028 | Test Loss: 0.0116298 | Time: 783ms
Epoch: 029 | Train Loss: 0.0139526 | Grad norm: 4.593847 | Time: 4s413ms
Epoch: 029 | Test Loss: 0.0141493 | Time: 782ms
Epoch: 030 | Train Loss: 0.0127898 | Grad norm: 3.772712 | Time: 4s498ms
Epoch: 030 | Test Loss: 0.0109904 | Time: 798ms
==> Save the model at epoch 030 with test loss 0.0109904
Epoch: 031 | Train Loss: 0.0102943 | Grad norm: 2.710654 | Time: 4s345ms
Epoch: 031 | Test Loss: 0.0107265 | Time: 819ms
==> Save the model at epoch 031 with test loss 0.0107265
Epoch: 032 | Train Loss: 0.0101437 | Grad norm: 2.372598 | Time: 4s798ms
Epoch: 032 | Test Loss: 0.0095546 | Time: 813ms
==> Save the model at epoch 032 with test loss 0.0095546
Epoch: 033 | Train Loss: 0.0092652 | Grad norm: 1.898728 | Time: 5s364ms
Epoch: 033 | Test Loss: 0.0086138 | Time: 882ms
==> Save the model at epoch 033 with test loss 0.0086138
Epoch: 034 | Train Loss: 0.0083723 | Grad norm: 1.603180 | Time: 5s389ms
Epoch: 034 | Test Loss: 0.0094340 | Time: 799ms
Epoch: 035 | Train Loss: 0.0079691 | Grad norm: 1.324750 | Time: 5s452ms
Epoch: 035 | Test Loss: 0.0079845 | Time: 791ms
==> Save the model at epoch 035 with test loss 0.0079845
Epoch: 036 | Train Loss: 0.0077531 | Grad norm: 1.274545 | Time: 5s78ms
Epoch: 036 | Test Loss: 0.0077831 | Time: 799ms
==> Save the model at epoch 036 with test loss 0.0077831
Epoch: 037 | Train Loss: 0.0074853 | Grad norm: 1.028999 | Time: 5s500ms
Epoch: 037 | Test Loss: 0.0075233 | Time: 875ms
==> Save the model at epoch 037 with test loss 0.0075233
Epoch: 038 | Train Loss: 0.0072846 | Grad norm: 0.961919 | Time: 5s246ms
Epoch: 038 | Test Loss: 0.0073598 | Time: 884ms
==> Save the model at epoch 038 with test loss 0.0073598
Epoch: 039 | Train Loss: 0.0071066 | Grad norm: 0.716029 | Time: 4s386ms
Epoch: 039 | Test Loss: 0.0072352 | Time: 815ms
==> Save the model at epoch 039 with test loss 0.0072352
Epoch: 040 | Train Loss: 0.0069853 | Grad norm: 0.601653 | Time: 4s402ms
Epoch: 040 | Test Loss: 0.0071213 | Time: 821ms
==> Save the model at epoch 040 with test loss 0.0071213
Total time: 3m44s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
