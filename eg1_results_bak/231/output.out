==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (M): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 5.5728370 | L2 loss: 5.5726417 | Lip loss: 0.0001954 | Grad norm: 16.624552 | Time: 25s664ms
Epoch: 001 | Test Loss: 0.4410888 | Time: 576ms
==> Save the model at epoch 001 with test loss 0.4410888
Epoch: 002 | Loss: 0.0906541 | L2 loss: 0.0904432 | Lip loss: 0.0002109 | Grad norm: 11.390553 | Time: 24s172ms
Epoch: 002 | Test Loss: 0.0202921 | Time: 580ms
==> Save the model at epoch 002 with test loss 0.0202921
Epoch: 003 | Loss: 0.0234029 | L2 loss: 0.0231964 | Lip loss: 0.0002065 | Grad norm: 4.767837 | Time: 25s721ms
Epoch: 003 | Test Loss: 0.0226902 | Time: 576ms
Epoch: 004 | Loss: 0.0336447 | L2 loss: 0.0334390 | Lip loss: 0.0002057 | Grad norm: 5.572740 | Time: 25s379ms
Epoch: 004 | Test Loss: 0.0209945 | Time: 570ms
Epoch: 005 | Loss: 0.0189026 | L2 loss: 0.0186975 | Lip loss: 0.0002051 | Grad norm: 3.358417 | Time: 23s431ms
Epoch: 005 | Test Loss: 0.0140590 | Time: 564ms
==> Save the model at epoch 005 with test loss 0.0140590
Epoch: 006 | Loss: 0.0082592 | L2 loss: 0.0080556 | Lip loss: 0.0002036 | Grad norm: 0.624428 | Time: 24s245ms
Epoch: 006 | Test Loss: 0.0079865 | Time: 600ms
==> Save the model at epoch 006 with test loss 0.0079865
Epoch: 007 | Loss: 0.0079614 | L2 loss: 0.0077581 | Lip loss: 0.0002032 | Grad norm: 0.523737 | Time: 25s439ms
Epoch: 007 | Test Loss: 0.0081565 | Time: 576ms
Epoch: 008 | Loss: 0.0079796 | L2 loss: 0.0077761 | Lip loss: 0.0002035 | Grad norm: 0.589364 | Time: 23s574ms
Epoch: 008 | Test Loss: 0.0076743 | Time: 628ms
==> Save the model at epoch 008 with test loss 0.0076743
Epoch: 009 | Loss: 0.0079535 | L2 loss: 0.0077505 | Lip loss: 0.0002030 | Grad norm: 0.607950 | Time: 23s399ms
Epoch: 009 | Test Loss: 0.0076652 | Time: 567ms
==> Save the model at epoch 009 with test loss 0.0076652
Epoch: 010 | Loss: 0.0079033 | L2 loss: 0.0077002 | Lip loss: 0.0002031 | Grad norm: 0.589693 | Time: 23s456ms
Epoch: 010 | Test Loss: 0.0083499 | Time: 564ms
Epoch: 011 | Loss: 0.0076137 | L2 loss: 0.0074107 | Lip loss: 0.0002029 | Grad norm: 0.346374 | Time: 23s400ms
Epoch: 011 | Test Loss: 0.0073007 | Time: 569ms
==> Save the model at epoch 011 with test loss 0.0073007
Epoch: 012 | Loss: 0.0075742 | L2 loss: 0.0073712 | Lip loss: 0.0002030 | Grad norm: 0.318508 | Time: 23s446ms
Epoch: 012 | Test Loss: 0.0073146 | Time: 567ms
Epoch: 013 | Loss: 0.0075939 | L2 loss: 0.0073909 | Lip loss: 0.0002030 | Grad norm: 0.329396 | Time: 23s432ms
Epoch: 013 | Test Loss: 0.0073063 | Time: 567ms
Epoch: 014 | Loss: 0.0075782 | L2 loss: 0.0073750 | Lip loss: 0.0002032 | Grad norm: 0.342424 | Time: 23s708ms
Epoch: 014 | Test Loss: 0.0072954 | Time: 573ms
==> Save the model at epoch 014 with test loss 0.0072954
Epoch: 015 | Loss: 0.0075727 | L2 loss: 0.0073695 | Lip loss: 0.0002033 | Grad norm: 0.333146 | Time: 23s291ms
Epoch: 015 | Test Loss: 0.0073565 | Time: 575ms
Epoch: 016 | Loss: 0.0075394 | L2 loss: 0.0073363 | Lip loss: 0.0002031 | Grad norm: 0.296586 | Time: 23s416ms
Epoch: 016 | Test Loss: 0.0072714 | Time: 564ms
==> Save the model at epoch 016 with test loss 0.0072714
Epoch: 017 | Loss: 0.0075416 | L2 loss: 0.0073385 | Lip loss: 0.0002031 | Grad norm: 0.289853 | Time: 23s272ms
Epoch: 017 | Test Loss: 0.0072693 | Time: 563ms
==> Save the model at epoch 017 with test loss 0.0072693
Epoch: 018 | Loss: 0.0075325 | L2 loss: 0.0073297 | Lip loss: 0.0002029 | Grad norm: 0.287274 | Time: 23s280ms
Epoch: 018 | Test Loss: 0.0072736 | Time: 567ms
Epoch: 019 | Loss: 0.0075400 | L2 loss: 0.0073371 | Lip loss: 0.0002030 | Grad norm: 0.280684 | Time: 23s217ms
Epoch: 019 | Test Loss: 0.0072736 | Time: 563ms
Epoch: 020 | Loss: 0.0075341 | L2 loss: 0.0073312 | Lip loss: 0.0002029 | Grad norm: 0.290308 | Time: 24s465ms
Epoch: 020 | Test Loss: 0.0072705 | Time: 566ms
Epoch: 021 | Loss: 0.0075296 | L2 loss: 0.0073266 | Lip loss: 0.0002029 | Grad norm: 0.281724 | Time: 23s218ms
Epoch: 021 | Test Loss: 0.0072687 | Time: 628ms
==> Save the model at epoch 021 with test loss 0.0072687
Epoch: 022 | Loss: 0.0075408 | L2 loss: 0.0073375 | Lip loss: 0.0002033 | Grad norm: 0.276741 | Time: 25s331ms
Epoch: 022 | Test Loss: 0.0072684 | Time: 576ms
==> Save the model at epoch 022 with test loss 0.0072684
Epoch: 023 | Loss: 0.0075267 | L2 loss: 0.0073236 | Lip loss: 0.0002031 | Grad norm: 0.285094 | Time: 25s528ms
Epoch: 023 | Test Loss: 0.0072681 | Time: 597ms
==> Save the model at epoch 023 with test loss 0.0072681
Epoch: 024 | Loss: 0.0075386 | L2 loss: 0.0073358 | Lip loss: 0.0002028 | Grad norm: 0.281453 | Time: 24s845ms
Epoch: 024 | Test Loss: 0.0072681 | Time: 589ms
==> Save the model at epoch 024 with test loss 0.0072681
Epoch: 025 | Loss: 0.0075412 | L2 loss: 0.0073381 | Lip loss: 0.0002031 | Grad norm: 0.283240 | Time: 23s419ms
Epoch: 025 | Test Loss: 0.0072684 | Time: 576ms
Epoch: 026 | Loss: 0.0075264 | L2 loss: 0.0073236 | Lip loss: 0.0002028 | Grad norm: 0.267626 | Time: 26s808ms
Epoch: 026 | Test Loss: 0.0072684 | Time: 573ms
Epoch: 027 | Loss: 0.0075346 | L2 loss: 0.0073316 | Lip loss: 0.0002029 | Grad norm: 0.283989 | Time: 25s56ms
Epoch: 027 | Test Loss: 0.0072683 | Time: 571ms
Epoch: 028 | Loss: 0.0075278 | L2 loss: 0.0073251 | Lip loss: 0.0002027 | Grad norm: 0.275860 | Time: 25s38ms
Epoch: 028 | Test Loss: 0.0072683 | Time: 567ms
Epoch: 029 | Loss: 0.0075300 | L2 loss: 0.0073267 | Lip loss: 0.0002033 | Grad norm: 0.275623 | Time: 23s431ms
Epoch: 029 | Test Loss: 0.0072683 | Time: 619ms
Epoch: 030 | Loss: 0.0075287 | L2 loss: 0.0073258 | Lip loss: 0.0002029 | Grad norm: 0.281654 | Time: 23s416ms
Epoch: 030 | Test Loss: 0.0072682 | Time: 566ms
Epoch: 031 | Loss: 0.0075258 | L2 loss: 0.0073229 | Lip loss: 0.0002029 | Grad norm: 0.281011 | Time: 23s458ms
Epoch: 031 | Test Loss: 0.0072682 | Time: 568ms
Epoch: 032 | Loss: 0.0075293 | L2 loss: 0.0073263 | Lip loss: 0.0002031 | Grad norm: 0.277420 | Time: 23s409ms
Epoch: 032 | Test Loss: 0.0072682 | Time: 562ms
Epoch: 033 | Loss: 0.0075291 | L2 loss: 0.0073261 | Lip loss: 0.0002030 | Grad norm: 0.276037 | Time: 23s466ms
Epoch: 033 | Test Loss: 0.0072682 | Time: 568ms
Epoch: 034 | Loss: 0.0075320 | L2 loss: 0.0073290 | Lip loss: 0.0002030 | Grad norm: 0.289024 | Time: 23s398ms
Epoch: 034 | Test Loss: 0.0072682 | Time: 628ms
Epoch: 035 | Loss: 0.0075271 | L2 loss: 0.0073243 | Lip loss: 0.0002028 | Grad norm: 0.282056 | Time: 25s603ms
Epoch: 035 | Test Loss: 0.0072682 | Time: 572ms
Epoch: 036 | Loss: 0.0075269 | L2 loss: 0.0073241 | Lip loss: 0.0002028 | Grad norm: 0.281914 | Time: 24s385ms
Epoch: 036 | Test Loss: 0.0072682 | Time: 579ms
Epoch: 037 | Loss: 0.0075277 | L2 loss: 0.0073247 | Lip loss: 0.0002030 | Grad norm: 0.278481 | Time: 24s265ms
Epoch: 037 | Test Loss: 0.0072682 | Time: 568ms
Epoch: 038 | Loss: 0.0075341 | L2 loss: 0.0073313 | Lip loss: 0.0002029 | Grad norm: 0.268359 | Time: 23s439ms
Epoch: 038 | Test Loss: 0.0072682 | Time: 559ms
Epoch: 039 | Loss: 0.0075322 | L2 loss: 0.0073291 | Lip loss: 0.0002030 | Grad norm: 0.277612 | Time: 23s372ms
Epoch: 039 | Test Loss: 0.0072682 | Time: 625ms
Epoch: 040 | Loss: 0.0075279 | L2 loss: 0.0073252 | Lip loss: 0.0002027 | Grad norm: 0.272446 | Time: 23s378ms
Epoch: 040 | Test Loss: 0.0072682 | Time: 561ms
Total time: 16m26s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
