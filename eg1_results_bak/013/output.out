==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Lipschitz constant: 0.50
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (M): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.8675379 | Grad norm: 4.263474 | Time: 20s963ms
Epoch: 001 | Test Loss: 31.2312768 | Time: 869ms
==> Save the model at epoch 001 with test loss 31.2312768
Epoch: 002 | Train Loss: 16.0039237 | Grad norm: 9.746897 | Time: 19s211ms
Epoch: 002 | Test Loss: 13.0004804 | Time: 1s39ms
==> Save the model at epoch 002 with test loss 13.0004804
Epoch: 003 | Train Loss: 10.9297753 | Grad norm: 6.814988 | Time: 21s573ms
Epoch: 003 | Test Loss: 9.5784232 | Time: 774ms
==> Save the model at epoch 003 with test loss 9.5784232
Epoch: 004 | Train Loss: 8.5235784 | Grad norm: 3.876984 | Time: 20s119ms
Epoch: 004 | Test Loss: 8.1020597 | Time: 803ms
==> Save the model at epoch 004 with test loss 8.1020597
Epoch: 005 | Train Loss: 7.9416624 | Grad norm: 9.166452 | Time: 20s607ms
Epoch: 005 | Test Loss: 7.9591552 | Time: 797ms
==> Save the model at epoch 005 with test loss 7.9591552
Epoch: 006 | Train Loss: 7.8361601 | Grad norm: 8.298566 | Time: 17s141ms
Epoch: 006 | Test Loss: 7.8936587 | Time: 792ms
==> Save the model at epoch 006 with test loss 7.8936587
Epoch: 007 | Train Loss: 7.7924901 | Grad norm: 7.499707 | Time: 16s353ms
Epoch: 007 | Test Loss: 7.8597262 | Time: 760ms
==> Save the model at epoch 007 with test loss 7.8597262
Epoch: 008 | Train Loss: 7.7508275 | Grad norm: 4.169581 | Time: 16s538ms
Epoch: 008 | Test Loss: 7.8302766 | Time: 759ms
==> Save the model at epoch 008 with test loss 7.8302766
Epoch: 009 | Train Loss: 7.7461430 | Grad norm: 4.190522 | Time: 19s23ms
Epoch: 009 | Test Loss: 7.8350497 | Time: 789ms
Epoch: 010 | Train Loss: 7.7499738 | Grad norm: 3.595939 | Time: 17s61ms
Epoch: 010 | Test Loss: 7.8358242 | Time: 760ms
Epoch: 011 | Train Loss: 7.7415785 | Grad norm: 3.296086 | Time: 19s577ms
Epoch: 011 | Test Loss: 7.8334499 | Time: 794ms
Epoch: 012 | Train Loss: 7.7411468 | Grad norm: 3.007981 | Time: 20s94ms
Epoch: 012 | Test Loss: 7.8335415 | Time: 762ms
Epoch: 013 | Train Loss: 7.7453322 | Grad norm: 2.614829 | Time: 17s160ms
Epoch: 013 | Test Loss: 7.8382679 | Time: 794ms
Epoch: 014 | Train Loss: 7.7392103 | Grad norm: 2.537083 | Time: 18s703ms
Epoch: 014 | Test Loss: 7.8277419 | Time: 771ms
==> Save the model at epoch 014 with test loss 7.8277419
Epoch: 015 | Train Loss: 7.7380137 | Grad norm: 2.295807 | Time: 21s321ms
Epoch: 015 | Test Loss: 7.8293023 | Time: 798ms
Epoch: 016 | Train Loss: 7.7387915 | Grad norm: 2.021717 | Time: 20s676ms
Epoch: 016 | Test Loss: 7.8417967 | Time: 789ms
Epoch: 017 | Train Loss: 7.7375429 | Grad norm: 1.922984 | Time: 21s390ms
Epoch: 017 | Test Loss: 7.8287183 | Time: 791ms
Epoch: 018 | Train Loss: 7.7368250 | Grad norm: 1.735770 | Time: 20s163ms
Epoch: 018 | Test Loss: 7.8347884 | Time: 797ms
Epoch: 019 | Train Loss: 7.7392171 | Grad norm: 1.400416 | Time: 19s678ms
Epoch: 019 | Test Loss: 7.8281096 | Time: 798ms
Epoch: 020 | Train Loss: 7.7364113 | Grad norm: 1.604585 | Time: 20s927ms
Epoch: 020 | Test Loss: 7.8270876 | Time: 770ms
==> Save the model at epoch 020 with test loss 7.8270876
Epoch: 021 | Train Loss: 7.7358228 | Grad norm: 1.309705 | Time: 16s989ms
Epoch: 021 | Test Loss: 7.8274365 | Time: 761ms
Epoch: 022 | Train Loss: 7.7335926 | Grad norm: 0.508011 | Time: 17s140ms
Epoch: 022 | Test Loss: 7.8273535 | Time: 836ms
Epoch: 023 | Train Loss: 7.7351514 | Grad norm: 0.979543 | Time: 18s832ms
Epoch: 023 | Test Loss: 7.8254773 | Time: 785ms
==> Save the model at epoch 023 with test loss 7.8254773
Epoch: 024 | Train Loss: 7.7336734 | Grad norm: 0.569915 | Time: 21s65ms
Epoch: 024 | Test Loss: 7.8250707 | Time: 793ms
==> Save the model at epoch 024 with test loss 7.8250707
Epoch: 025 | Train Loss: 7.7334304 | Grad norm: 0.416501 | Time: 20s465ms
Epoch: 025 | Test Loss: 7.8252502 | Time: 794ms
Epoch: 026 | Train Loss: 7.7335243 | Grad norm: 0.472955 | Time: 20s110ms
Epoch: 026 | Test Loss: 7.8251281 | Time: 790ms
Epoch: 027 | Train Loss: 7.7339865 | Grad norm: 0.469249 | Time: 20s903ms
Epoch: 027 | Test Loss: 7.8258164 | Time: 793ms
Epoch: 028 | Train Loss: 7.7333447 | Grad norm: 0.370879 | Time: 17s618ms
Epoch: 028 | Test Loss: 7.8259201 | Time: 760ms
Epoch: 029 | Train Loss: 7.7333309 | Grad norm: 0.357034 | Time: 18s968ms
Epoch: 029 | Test Loss: 7.8250562 | Time: 768ms
==> Save the model at epoch 029 with test loss 7.8250562
Epoch: 030 | Train Loss: 7.7332551 | Grad norm: 0.284927 | Time: 20s921ms
Epoch: 030 | Test Loss: 7.8249992 | Time: 796ms
==> Save the model at epoch 030 with test loss 7.8249992
Epoch: 031 | Train Loss: 7.7333003 | Grad norm: 0.323525 | Time: 19s170ms
Epoch: 031 | Test Loss: 7.8250485 | Time: 794ms
Epoch: 032 | Train Loss: 7.7332124 | Grad norm: 0.231481 | Time: 19s781ms
Epoch: 032 | Test Loss: 7.8250382 | Time: 768ms
Epoch: 033 | Train Loss: 7.7332612 | Grad norm: 0.281115 | Time: 20s521ms
Epoch: 033 | Test Loss: 7.8249983 | Time: 764ms
==> Save the model at epoch 033 with test loss 7.8249983
Epoch: 034 | Train Loss: 7.7331954 | Grad norm: 0.202815 | Time: 20s364ms
Epoch: 034 | Test Loss: 7.8250384 | Time: 827ms
Epoch: 035 | Train Loss: 7.7331668 | Grad norm: 0.165053 | Time: 19s885ms
Epoch: 035 | Test Loss: 7.8249871 | Time: 801ms
==> Save the model at epoch 035 with test loss 7.8249871
Epoch: 036 | Train Loss: 7.7331689 | Grad norm: 0.169820 | Time: 20s485ms
Epoch: 036 | Test Loss: 7.8251384 | Time: 793ms
Epoch: 037 | Train Loss: 7.7331465 | Grad norm: 0.127680 | Time: 20s630ms
Epoch: 037 | Test Loss: 7.8250819 | Time: 822ms
Epoch: 038 | Train Loss: 7.7331465 | Grad norm: 0.126489 | Time: 17s362ms
Epoch: 038 | Test Loss: 7.8250622 | Time: 792ms
Epoch: 039 | Train Loss: 7.7331337 | Grad norm: 0.095228 | Time: 17s257ms
Epoch: 039 | Test Loss: 7.8249907 | Time: 789ms
Epoch: 040 | Train Loss: 7.7331293 | Grad norm: 0.070669 | Time: 19s49ms
Epoch: 040 | Test Loss: 7.8249959 | Time: 869ms
Total time: 13m27s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
