==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Lipschitz constant: 0.50
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.48014596 0.48395684]
==> Ouput transform to be applied to the neural network:
[2.0663 2.0805]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 8.5823131 | Grad norm: 1.241573 | Time: 13s954ms
Epoch: 001 | Test Loss: 8.6044786 | Time: 933ms
==> Save the model at epoch 001 with test loss 8.6044786
Epoch: 002 | Train Loss: 4.6623531 | Grad norm: 2.797496 | Time: 13s477ms
Epoch: 002 | Test Loss: 3.7363541 | Time: 944ms
==> Save the model at epoch 002 with test loss 3.7363541
Epoch: 003 | Train Loss: 3.1689950 | Grad norm: 2.091642 | Time: 13s620ms
Epoch: 003 | Test Loss: 2.7233335 | Time: 1s26ms
==> Save the model at epoch 003 with test loss 2.7233335
Epoch: 004 | Train Loss: 2.4381723 | Grad norm: 1.479133 | Time: 13s618ms
Epoch: 004 | Test Loss: 2.2974627 | Time: 930ms
==> Save the model at epoch 004 with test loss 2.2974627
Epoch: 005 | Train Loss: 2.2483103 | Grad norm: 1.486069 | Time: 13s452ms
Epoch: 005 | Test Loss: 2.2177662 | Time: 930ms
==> Save the model at epoch 005 with test loss 2.2177662
Epoch: 006 | Train Loss: 2.2150635 | Grad norm: 2.721329 | Time: 13s955ms
Epoch: 006 | Test Loss: 2.2012296 | Time: 1s3ms
==> Save the model at epoch 006 with test loss 2.2012296
Epoch: 007 | Train Loss: 2.1915387 | Grad norm: 1.472259 | Time: 13s627ms
Epoch: 007 | Test Loss: 2.1845758 | Time: 928ms
==> Save the model at epoch 007 with test loss 2.1845758
Epoch: 008 | Train Loss: 2.1826744 | Grad norm: 1.180008 | Time: 13s659ms
Epoch: 008 | Test Loss: 2.1949324 | Time: 930ms
Epoch: 009 | Train Loss: 2.1816660 | Grad norm: 1.324940 | Time: 13s568ms
Epoch: 009 | Test Loss: 2.1817965 | Time: 1s1ms
==> Save the model at epoch 009 with test loss 2.1817965
Epoch: 010 | Train Loss: 2.1811451 | Grad norm: 1.051876 | Time: 13s653ms
Epoch: 010 | Test Loss: 2.1817894 | Time: 929ms
==> Save the model at epoch 010 with test loss 2.1817894
Epoch: 011 | Train Loss: 2.1791659 | Grad norm: 0.854744 | Time: 13s666ms
Epoch: 011 | Test Loss: 2.1809186 | Time: 932ms
==> Save the model at epoch 011 with test loss 2.1809186
Epoch: 012 | Train Loss: 2.1801824 | Grad norm: 0.837592 | Time: 13s864ms
Epoch: 012 | Test Loss: 2.1850053 | Time: 969ms
Epoch: 013 | Train Loss: 2.1777231 | Grad norm: 0.587683 | Time: 13s882ms
Epoch: 013 | Test Loss: 2.1820274 | Time: 930ms
Epoch: 014 | Train Loss: 2.1788868 | Grad norm: 0.748492 | Time: 13s793ms
Epoch: 014 | Test Loss: 2.1812071 | Time: 1s9ms
Epoch: 015 | Train Loss: 2.1780990 | Grad norm: 0.674753 | Time: 13s842ms
Epoch: 015 | Test Loss: 2.1816827 | Time: 933ms
Epoch: 016 | Train Loss: 2.1771143 | Grad norm: 0.576538 | Time: 14s123ms
Epoch: 016 | Test Loss: 2.1808786 | Time: 930ms
==> Save the model at epoch 016 with test loss 2.1808786
Epoch: 017 | Train Loss: 2.1762681 | Grad norm: 0.500910 | Time: 13s864ms
Epoch: 017 | Test Loss: 2.1819813 | Time: 998ms
Epoch: 018 | Train Loss: 2.1766040 | Grad norm: 0.465924 | Time: 14s449ms
Epoch: 018 | Test Loss: 2.1810970 | Time: 936ms
Epoch: 019 | Train Loss: 2.1764680 | Grad norm: 0.441473 | Time: 13s945ms
Epoch: 019 | Test Loss: 2.1817805 | Time: 927ms
Epoch: 020 | Train Loss: 2.1765185 | Grad norm: 0.428244 | Time: 13s571ms
Epoch: 020 | Test Loss: 2.1811023 | Time: 1s2ms
Epoch: 021 | Train Loss: 2.1770344 | Grad norm: 0.392686 | Time: 14s39ms
Epoch: 021 | Test Loss: 2.1810379 | Time: 952ms
Epoch: 022 | Train Loss: 2.1766956 | Grad norm: 0.316286 | Time: 14s559ms
Epoch: 022 | Test Loss: 2.1822336 | Time: 926ms
Epoch: 023 | Train Loss: 2.1761763 | Grad norm: 0.400140 | Time: 14s88ms
Epoch: 023 | Test Loss: 2.1811747 | Time: 929ms
Epoch: 024 | Train Loss: 2.1762738 | Grad norm: 0.341213 | Time: 14s196ms
Epoch: 024 | Test Loss: 2.1842161 | Time: 927ms
Epoch: 025 | Train Loss: 2.1758663 | Grad norm: 0.318408 | Time: 14s272ms
Epoch: 025 | Test Loss: 2.1808682 | Time: 927ms
==> Save the model at epoch 025 with test loss 2.1808682
Epoch: 026 | Train Loss: 2.1763355 | Grad norm: 0.353940 | Time: 14s344ms
Epoch: 026 | Test Loss: 2.1808784 | Time: 950ms
Epoch: 027 | Train Loss: 2.1760598 | Grad norm: 0.254124 | Time: 14s198ms
Epoch: 027 | Test Loss: 2.1812580 | Time: 929ms
Epoch: 028 | Train Loss: 2.1759426 | Grad norm: 0.271035 | Time: 13s990ms
Epoch: 028 | Test Loss: 2.1811996 | Time: 1s
Epoch: 029 | Train Loss: 2.1760163 | Grad norm: 0.304434 | Time: 13s990ms
Epoch: 029 | Test Loss: 2.1812275 | Time: 945ms
Epoch: 030 | Train Loss: 2.1760709 | Grad norm: 0.236462 | Time: 13s653ms
Epoch: 030 | Test Loss: 2.1810277 | Time: 920ms
Epoch: 031 | Train Loss: 2.1758262 | Grad norm: 0.204487 | Time: 13s870ms
Epoch: 031 | Test Loss: 2.1811167 | Time: 1s3ms
Epoch: 032 | Train Loss: 2.1757740 | Grad norm: 0.211656 | Time: 14s173ms
Epoch: 032 | Test Loss: 2.1808748 | Time: 927ms
Epoch: 033 | Train Loss: 2.1759492 | Grad norm: 0.214600 | Time: 14s45ms
Epoch: 033 | Test Loss: 2.1810196 | Time: 939ms
Epoch: 034 | Train Loss: 2.1756187 | Grad norm: 0.168570 | Time: 13s881ms
Epoch: 034 | Test Loss: 2.1810651 | Time: 926ms
Epoch: 035 | Train Loss: 2.1760960 | Grad norm: 0.180969 | Time: 13s688ms
Epoch: 035 | Test Loss: 2.1810609 | Time: 935ms
Epoch: 036 | Train Loss: 2.1756401 | Grad norm: 0.169370 | Time: 13s860ms
Epoch: 036 | Test Loss: 2.1808842 | Time: 926ms
Epoch: 037 | Train Loss: 2.1756801 | Grad norm: 0.147726 | Time: 13s901ms
Epoch: 037 | Test Loss: 2.1808686 | Time: 935ms
Epoch: 038 | Train Loss: 2.1758591 | Grad norm: 0.134206 | Time: 13s842ms
Epoch: 038 | Test Loss: 2.1808756 | Time: 932ms
Epoch: 039 | Train Loss: 2.1759937 | Grad norm: 0.106266 | Time: 14s230ms
Epoch: 039 | Test Loss: 2.1809438 | Time: 999ms
Epoch: 040 | Train Loss: 2.1753448 | Grad norm: 0.110788 | Time: 14s490ms
Epoch: 040 | Test Loss: 2.1808622 | Time: 939ms
==> Save the model at epoch 040 with test loss 2.1808622
Total time: 9m54s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.48014596 0.48395684]
==> Output transform to be applied to the neural network (trained):
[2.0663 2.0805]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
