==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0. 0. 0.]
==> Input transform to be applied to the neural network:
[1. 1. 1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    10
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   320
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,420
Trainable params: 25,410
Non-trainable params: 10
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.11
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 0.0580524 | L2 loss: 0.0446399 | Lip loss: 0.0134126 | Grad norm: 0.134850 | Time: 8s957ms
Epoch: 001 | Test Loss: 0.0125854 | Time: 518ms
==> Save the model at epoch 001 with test loss 0.0125854
Epoch: 002 | Loss: 0.0286274 | L2 loss: 0.0118000 | Lip loss: 0.0168274 | Grad norm: 0.117699 | Time: 8s410ms
Epoch: 002 | Test Loss: 0.0122629 | Time: 560ms
==> Save the model at epoch 002 with test loss 0.0122629
Epoch: 003 | Loss: 0.0283286 | L2 loss: 0.0118110 | Lip loss: 0.0165176 | Grad norm: 0.116119 | Time: 8s527ms
Epoch: 003 | Test Loss: 0.0122456 | Time: 516ms
==> Save the model at epoch 003 with test loss 0.0122456
Epoch: 004 | Loss: 0.0287067 | L2 loss: 0.0120169 | Lip loss: 0.0166898 | Grad norm: 0.120402 | Time: 8s219ms
Epoch: 004 | Test Loss: 0.0127996 | Time: 487ms
Epoch: 005 | Loss: 0.0284812 | L2 loss: 0.0117244 | Lip loss: 0.0167568 | Grad norm: 0.102944 | Time: 8s533ms
Epoch: 005 | Test Loss: 0.0131058 | Time: 488ms
Epoch: 006 | Loss: 0.0281315 | L2 loss: 0.0112000 | Lip loss: 0.0169315 | Grad norm: 0.075056 | Time: 8s360ms
Epoch: 006 | Test Loss: 0.0118712 | Time: 493ms
==> Save the model at epoch 006 with test loss 0.0118712
Epoch: 007 | Loss: 0.0277464 | L2 loss: 0.0109788 | Lip loss: 0.0167676 | Grad norm: 0.074069 | Time: 8s558ms
Epoch: 007 | Test Loss: 0.0115387 | Time: 494ms
==> Save the model at epoch 007 with test loss 0.0115387
Epoch: 008 | Loss: 0.0277329 | L2 loss: 0.0109495 | Lip loss: 0.0167834 | Grad norm: 0.064617 | Time: 8s956ms
Epoch: 008 | Test Loss: 0.0117813 | Time: 584ms
Epoch: 009 | Loss: 0.0277063 | L2 loss: 0.0109744 | Lip loss: 0.0167318 | Grad norm: 0.070691 | Time: 8s442ms
Epoch: 009 | Test Loss: 0.0118796 | Time: 515ms
Epoch: 010 | Loss: 0.0276586 | L2 loss: 0.0109953 | Lip loss: 0.0166632 | Grad norm: 0.070018 | Time: 8s657ms
Epoch: 010 | Test Loss: 0.0116658 | Time: 511ms
Epoch: 011 | Loss: 0.0273938 | L2 loss: 0.0108968 | Lip loss: 0.0164970 | Grad norm: 0.061482 | Time: 8s370ms
Epoch: 011 | Test Loss: 0.0116351 | Time: 508ms
Epoch: 012 | Loss: 0.0275996 | L2 loss: 0.0108901 | Lip loss: 0.0167094 | Grad norm: 0.061570 | Time: 8s612ms
Epoch: 012 | Test Loss: 0.0116606 | Time: 504ms
Epoch: 013 | Loss: 0.0275430 | L2 loss: 0.0108741 | Lip loss: 0.0166689 | Grad norm: 0.064095 | Time: 8s377ms
Epoch: 013 | Test Loss: 0.0116334 | Time: 566ms
Epoch: 014 | Loss: 0.0277080 | L2 loss: 0.0108639 | Lip loss: 0.0168441 | Grad norm: 0.064048 | Time: 8s528ms
Epoch: 014 | Test Loss: 0.0116348 | Time: 509ms
Epoch: 015 | Loss: 0.0274884 | L2 loss: 0.0108372 | Lip loss: 0.0166512 | Grad norm: 0.063012 | Time: 8s580ms
Epoch: 015 | Test Loss: 0.0116254 | Time: 494ms
Epoch: 016 | Loss: 0.0274801 | L2 loss: 0.0108423 | Lip loss: 0.0166378 | Grad norm: 0.064793 | Time: 8s550ms
Epoch: 016 | Test Loss: 0.0116272 | Time: 575ms
Epoch: 017 | Loss: 0.0275119 | L2 loss: 0.0108300 | Lip loss: 0.0166819 | Grad norm: 0.066674 | Time: 8s479ms
Epoch: 017 | Test Loss: 0.0116178 | Time: 507ms
Epoch: 018 | Loss: 0.0275945 | L2 loss: 0.0108381 | Lip loss: 0.0167564 | Grad norm: 0.062521 | Time: 8s551ms
Epoch: 018 | Test Loss: 0.0116188 | Time: 496ms
Epoch: 019 | Loss: 0.0273548 | L2 loss: 0.0108270 | Lip loss: 0.0165277 | Grad norm: 0.065517 | Time: 8s676ms
Epoch: 019 | Test Loss: 0.0116131 | Time: 511ms
Epoch: 020 | Loss: 0.0276709 | L2 loss: 0.0108539 | Lip loss: 0.0168170 | Grad norm: 0.062974 | Time: 8s422ms
Epoch: 020 | Test Loss: 0.0116295 | Time: 498ms
Epoch: 021 | Loss: 0.0275749 | L2 loss: 0.0108487 | Lip loss: 0.0167261 | Grad norm: 0.062926 | Time: 8s459ms
Epoch: 021 | Test Loss: 0.0116298 | Time: 508ms
Epoch: 022 | Loss: 0.0274363 | L2 loss: 0.0108443 | Lip loss: 0.0165921 | Grad norm: 0.063262 | Time: 8s679ms
Epoch: 022 | Test Loss: 0.0116306 | Time: 498ms
Epoch: 023 | Loss: 0.0275641 | L2 loss: 0.0108443 | Lip loss: 0.0167198 | Grad norm: 0.063823 | Time: 8s579ms
Epoch: 023 | Test Loss: 0.0116307 | Time: 510ms
Epoch: 024 | Loss: 0.0275601 | L2 loss: 0.0108427 | Lip loss: 0.0167174 | Grad norm: 0.061131 | Time: 8s437ms
Epoch: 024 | Test Loss: 0.0116308 | Time: 574ms
Epoch: 025 | Loss: 0.0274539 | L2 loss: 0.0108466 | Lip loss: 0.0166073 | Grad norm: 0.064120 | Time: 8s503ms
Epoch: 025 | Test Loss: 0.0116308 | Time: 561ms
Epoch: 026 | Loss: 0.0274178 | L2 loss: 0.0108448 | Lip loss: 0.0165730 | Grad norm: 0.063047 | Time: 8s713ms
Epoch: 026 | Test Loss: 0.0116307 | Time: 517ms
Epoch: 027 | Loss: 0.0274480 | L2 loss: 0.0108536 | Lip loss: 0.0165944 | Grad norm: 0.062861 | Time: 8s800ms
Epoch: 027 | Test Loss: 0.0116307 | Time: 571ms
Epoch: 028 | Loss: 0.0275980 | L2 loss: 0.0108428 | Lip loss: 0.0167552 | Grad norm: 0.063565 | Time: 8s525ms
Epoch: 028 | Test Loss: 0.0116307 | Time: 492ms
Epoch: 029 | Loss: 0.0275710 | L2 loss: 0.0108397 | Lip loss: 0.0167313 | Grad norm: 0.065073 | Time: 8s545ms
Epoch: 029 | Test Loss: 0.0116308 | Time: 504ms
Epoch: 030 | Loss: 0.0276118 | L2 loss: 0.0108414 | Lip loss: 0.0167704 | Grad norm: 0.060049 | Time: 8s832ms
Epoch: 030 | Test Loss: 0.0116309 | Time: 493ms
Epoch: 031 | Loss: 0.0275219 | L2 loss: 0.0108437 | Lip loss: 0.0166783 | Grad norm: 0.061176 | Time: 8s735ms
Epoch: 031 | Test Loss: 0.0116309 | Time: 489ms
Epoch: 032 | Loss: 0.0274026 | L2 loss: 0.0108490 | Lip loss: 0.0165536 | Grad norm: 0.058825 | Time: 8s519ms
Epoch: 032 | Test Loss: 0.0116309 | Time: 490ms
Epoch: 033 | Loss: 0.0277447 | L2 loss: 0.0108550 | Lip loss: 0.0168897 | Grad norm: 0.064248 | Time: 8s321ms
Epoch: 033 | Test Loss: 0.0116309 | Time: 504ms
Epoch: 034 | Loss: 0.0275876 | L2 loss: 0.0108420 | Lip loss: 0.0167457 | Grad norm: 0.067230 | Time: 8s649ms
Epoch: 034 | Test Loss: 0.0116309 | Time: 497ms
Epoch: 035 | Loss: 0.0275124 | L2 loss: 0.0108486 | Lip loss: 0.0166639 | Grad norm: 0.066908 | Time: 8s537ms
Epoch: 035 | Test Loss: 0.0116309 | Time: 580ms
Epoch: 036 | Loss: 0.0274813 | L2 loss: 0.0108480 | Lip loss: 0.0166333 | Grad norm: 0.064216 | Time: 8s392ms
Epoch: 036 | Test Loss: 0.0116309 | Time: 517ms
Epoch: 037 | Loss: 0.0274534 | L2 loss: 0.0108532 | Lip loss: 0.0166002 | Grad norm: 0.061781 | Time: 8s563ms
Epoch: 037 | Test Loss: 0.0116309 | Time: 489ms
Epoch: 038 | Loss: 0.0275953 | L2 loss: 0.0108436 | Lip loss: 0.0167517 | Grad norm: 0.063852 | Time: 8s184ms
Epoch: 038 | Test Loss: 0.0116309 | Time: 578ms
Epoch: 039 | Loss: 0.0276588 | L2 loss: 0.0108753 | Lip loss: 0.0167835 | Grad norm: 0.062627 | Time: 8s434ms
Epoch: 039 | Test Loss: 0.0116309 | Time: 498ms
Epoch: 040 | Loss: 0.0276934 | L2 loss: 0.0108432 | Lip loss: 0.0168502 | Grad norm: 0.061937 | Time: 8s234ms
Epoch: 040 | Test Loss: 0.0116309 | Time: 498ms
Total time: 6m2s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0. 0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1. 1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
