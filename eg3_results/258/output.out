==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0. 0. 0.]
==> Input transform to be applied to the neural network:
[1. 1. 1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    10
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   320
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,420
Trainable params: 25,410
Non-trainable params: 10
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.11
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 0.0053917 | L2 loss: 0.0048102 | Lip loss: 0.0005814 | Grad norm: 0.024411 | Time: 34s885ms
Epoch: 001 | Test Loss: 0.0031791 | Time: 512ms
==> Save the model at epoch 001 with test loss 0.0031791
Epoch: 002 | Loss: 0.0043947 | L2 loss: 0.0032745 | Lip loss: 0.0011202 | Grad norm: 0.020567 | Time: 33s793ms
Epoch: 002 | Test Loss: 0.0030790 | Time: 495ms
==> Save the model at epoch 002 with test loss 0.0030790
Epoch: 003 | Loss: 0.0044309 | L2 loss: 0.0032534 | Lip loss: 0.0011776 | Grad norm: 0.017324 | Time: 34s16ms
Epoch: 003 | Test Loss: 0.0031375 | Time: 505ms
Epoch: 004 | Loss: 0.0044179 | L2 loss: 0.0032649 | Lip loss: 0.0011531 | Grad norm: 0.018776 | Time: 34s11ms
Epoch: 004 | Test Loss: 0.0029483 | Time: 497ms
==> Save the model at epoch 004 with test loss 0.0029483
Epoch: 005 | Loss: 0.0044388 | L2 loss: 0.0032530 | Lip loss: 0.0011857 | Grad norm: 0.018650 | Time: 33s725ms
Epoch: 005 | Test Loss: 0.0031061 | Time: 497ms
Epoch: 006 | Loss: 0.0042709 | L2 loss: 0.0030556 | Lip loss: 0.0012153 | Grad norm: 0.012377 | Time: 33s753ms
Epoch: 006 | Test Loss: 0.0029821 | Time: 576ms
Epoch: 007 | Loss: 0.0042183 | L2 loss: 0.0030144 | Lip loss: 0.0012040 | Grad norm: 0.012724 | Time: 34s188ms
Epoch: 007 | Test Loss: 0.0030043 | Time: 574ms
Epoch: 008 | Loss: 0.0042929 | L2 loss: 0.0030454 | Lip loss: 0.0012475 | Grad norm: 0.013232 | Time: 34s82ms
Epoch: 008 | Test Loss: 0.0030726 | Time: 510ms
Epoch: 009 | Loss: 0.0043421 | L2 loss: 0.0030731 | Lip loss: 0.0012690 | Grad norm: 0.012587 | Time: 34s166ms
Epoch: 009 | Test Loss: 0.0030421 | Time: 499ms
Epoch: 010 | Loss: 0.0043004 | L2 loss: 0.0030543 | Lip loss: 0.0012461 | Grad norm: 0.012633 | Time: 33s924ms
Epoch: 010 | Test Loss: 0.0030711 | Time: 505ms
Epoch: 011 | Loss: 0.0042660 | L2 loss: 0.0030767 | Lip loss: 0.0011892 | Grad norm: 0.012197 | Time: 34s129ms
Epoch: 011 | Test Loss: 0.0030398 | Time: 497ms
Epoch: 012 | Loss: 0.0042539 | L2 loss: 0.0030463 | Lip loss: 0.0012076 | Grad norm: 0.012086 | Time: 34s407ms
Epoch: 012 | Test Loss: 0.0030212 | Time: 509ms
Epoch: 013 | Loss: 0.0042229 | L2 loss: 0.0030333 | Lip loss: 0.0011895 | Grad norm: 0.011676 | Time: 33s487ms
Epoch: 013 | Test Loss: 0.0030024 | Time: 496ms
Epoch: 014 | Loss: 0.0043457 | L2 loss: 0.0030294 | Lip loss: 0.0013163 | Grad norm: 0.012718 | Time: 33s206ms
Epoch: 014 | Test Loss: 0.0030270 | Time: 504ms
Epoch: 015 | Loss: 0.0042481 | L2 loss: 0.0030457 | Lip loss: 0.0012024 | Grad norm: 0.012107 | Time: 32s675ms
Epoch: 015 | Test Loss: 0.0030148 | Time: 583ms
Epoch: 016 | Loss: 0.0042828 | L2 loss: 0.0030363 | Lip loss: 0.0012465 | Grad norm: 0.012073 | Time: 33s77ms
Epoch: 016 | Test Loss: 0.0030151 | Time: 568ms
Epoch: 017 | Loss: 0.0043037 | L2 loss: 0.0030370 | Lip loss: 0.0012667 | Grad norm: 0.012469 | Time: 34s8ms
Epoch: 017 | Test Loss: 0.0030157 | Time: 509ms
Epoch: 018 | Loss: 0.0042252 | L2 loss: 0.0030361 | Lip loss: 0.0011891 | Grad norm: 0.011765 | Time: 33s839ms
Epoch: 018 | Test Loss: 0.0030140 | Time: 501ms
Epoch: 019 | Loss: 0.0042829 | L2 loss: 0.0030350 | Lip loss: 0.0012479 | Grad norm: 0.011979 | Time: 34s32ms
Epoch: 019 | Test Loss: 0.0030148 | Time: 495ms
Epoch: 020 | Loss: 0.0042656 | L2 loss: 0.0030361 | Lip loss: 0.0012295 | Grad norm: 0.011958 | Time: 33s655ms
Epoch: 020 | Test Loss: 0.0030137 | Time: 501ms
Epoch: 021 | Loss: 0.0043104 | L2 loss: 0.0030351 | Lip loss: 0.0012752 | Grad norm: 0.012244 | Time: 33s902ms
Epoch: 021 | Test Loss: 0.0030138 | Time: 636ms
Epoch: 022 | Loss: 0.0042735 | L2 loss: 0.0030353 | Lip loss: 0.0012382 | Grad norm: 0.012045 | Time: 34s375ms
Epoch: 022 | Test Loss: 0.0030137 | Time: 498ms
Epoch: 023 | Loss: 0.0042438 | L2 loss: 0.0030351 | Lip loss: 0.0012087 | Grad norm: 0.011923 | Time: 34s369ms
Epoch: 023 | Test Loss: 0.0030137 | Time: 510ms
Epoch: 024 | Loss: 0.0042577 | L2 loss: 0.0030350 | Lip loss: 0.0012226 | Grad norm: 0.011789 | Time: 34s269ms
Epoch: 024 | Test Loss: 0.0030136 | Time: 578ms
Epoch: 025 | Loss: 0.0042558 | L2 loss: 0.0030350 | Lip loss: 0.0012208 | Grad norm: 0.012393 | Time: 34s433ms
Epoch: 025 | Test Loss: 0.0030134 | Time: 563ms
Epoch: 026 | Loss: 0.0042557 | L2 loss: 0.0030349 | Lip loss: 0.0012208 | Grad norm: 0.011941 | Time: 34s505ms
Epoch: 026 | Test Loss: 0.0030134 | Time: 505ms
Epoch: 027 | Loss: 0.0042455 | L2 loss: 0.0030349 | Lip loss: 0.0012107 | Grad norm: 0.011958 | Time: 34s256ms
Epoch: 027 | Test Loss: 0.0030134 | Time: 504ms
Epoch: 028 | Loss: 0.0042862 | L2 loss: 0.0030349 | Lip loss: 0.0012513 | Grad norm: 0.011727 | Time: 34s527ms
Epoch: 028 | Test Loss: 0.0030134 | Time: 499ms
Epoch: 029 | Loss: 0.0042451 | L2 loss: 0.0030349 | Lip loss: 0.0012103 | Grad norm: 0.011925 | Time: 33s577ms
Epoch: 029 | Test Loss: 0.0030134 | Time: 501ms
Epoch: 030 | Loss: 0.0042595 | L2 loss: 0.0030349 | Lip loss: 0.0012246 | Grad norm: 0.012156 | Time: 34s187ms
Epoch: 030 | Test Loss: 0.0030134 | Time: 496ms
Epoch: 031 | Loss: 0.0042450 | L2 loss: 0.0030349 | Lip loss: 0.0012101 | Grad norm: 0.012117 | Time: 34s668ms
Epoch: 031 | Test Loss: 0.0030134 | Time: 503ms
Epoch: 032 | Loss: 0.0042306 | L2 loss: 0.0030349 | Lip loss: 0.0011957 | Grad norm: 0.011923 | Time: 34s743ms
Epoch: 032 | Test Loss: 0.0030134 | Time: 570ms
Epoch: 033 | Loss: 0.0042665 | L2 loss: 0.0030349 | Lip loss: 0.0012316 | Grad norm: 0.012087 | Time: 34s793ms
Epoch: 033 | Test Loss: 0.0030134 | Time: 503ms
Epoch: 034 | Loss: 0.0042407 | L2 loss: 0.0030349 | Lip loss: 0.0012058 | Grad norm: 0.012466 | Time: 33s874ms
Epoch: 034 | Test Loss: 0.0030134 | Time: 564ms
Epoch: 035 | Loss: 0.0042542 | L2 loss: 0.0030349 | Lip loss: 0.0012194 | Grad norm: 0.011631 | Time: 34s219ms
Epoch: 035 | Test Loss: 0.0030134 | Time: 568ms
Epoch: 036 | Loss: 0.0041825 | L2 loss: 0.0030349 | Lip loss: 0.0011477 | Grad norm: 0.011506 | Time: 34s229ms
Epoch: 036 | Test Loss: 0.0030134 | Time: 499ms
Epoch: 037 | Loss: 0.0042174 | L2 loss: 0.0030349 | Lip loss: 0.0011825 | Grad norm: 0.011288 | Time: 33s988ms
Epoch: 037 | Test Loss: 0.0030134 | Time: 501ms
Epoch: 038 | Loss: 0.0042617 | L2 loss: 0.0030349 | Lip loss: 0.0012268 | Grad norm: 0.011903 | Time: 33s989ms
Epoch: 038 | Test Loss: 0.0030134 | Time: 500ms
Epoch: 039 | Loss: 0.0043568 | L2 loss: 0.0030349 | Lip loss: 0.0013219 | Grad norm: 0.012820 | Time: 34s97ms
Epoch: 039 | Test Loss: 0.0030134 | Time: 499ms
Epoch: 040 | Loss: 0.0042920 | L2 loss: 0.0030349 | Lip loss: 0.0012571 | Grad norm: 0.011805 | Time: 33s979ms
Epoch: 040 | Test Loss: 0.0030134 | Time: 520ms
Total time: 23m2s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0. 0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1. 1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
