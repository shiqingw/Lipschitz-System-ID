==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Lipschitz constant: 2.00
==> Input bias to be applied to the neural network:
[0. 0. 0. 0.]
==> Input transform to be applied to the neural network:
[0.7072636  0.72212595 0.6760411  0.67349136]
==> Ouput transform to be applied to the neural network:
[0.2287 0.2871]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    10
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,481
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,552
Trainable params: 54,542
Non-trainable params: 10
Total mult-adds (Units.MEGABYTES): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.23
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 0.1357828 | Grad norm: 0.064730 | Time: 9s15ms
Epoch: 001 | Test Loss: 0.1348561 | Time: 614ms
==> Save the model at epoch 001 with test loss 0.1348561
Epoch: 002 | Train Loss: 0.0157220 | Grad norm: 0.073508 | Time: 8s823ms
Epoch: 002 | Test Loss: 0.0101079 | Time: 687ms
==> Save the model at epoch 002 with test loss 0.0101079
Epoch: 003 | Train Loss: 0.0113327 | Grad norm: 0.069679 | Time: 8s401ms
Epoch: 003 | Test Loss: 0.0107065 | Time: 614ms
Epoch: 004 | Train Loss: 0.0109545 | Grad norm: 0.059643 | Time: 8s481ms
Epoch: 004 | Test Loss: 0.0110332 | Time: 629ms
Epoch: 005 | Train Loss: 0.0110767 | Grad norm: 0.058443 | Time: 8s641ms
Epoch: 005 | Test Loss: 0.0106231 | Time: 614ms
Epoch: 006 | Train Loss: 0.0110447 | Grad norm: 0.053988 | Time: 8s536ms
Epoch: 006 | Test Loss: 0.0101126 | Time: 627ms
Epoch: 007 | Train Loss: 0.0107397 | Grad norm: 0.038636 | Time: 8s442ms
Epoch: 007 | Test Loss: 0.0100345 | Time: 613ms
==> Save the model at epoch 007 with test loss 0.0100345
Epoch: 008 | Train Loss: 0.0106067 | Grad norm: 0.032915 | Time: 8s467ms
Epoch: 008 | Test Loss: 0.0102741 | Time: 628ms
Epoch: 009 | Train Loss: 0.0106079 | Grad norm: 0.029599 | Time: 8s416ms
Epoch: 009 | Test Loss: 0.0098208 | Time: 631ms
==> Save the model at epoch 009 with test loss 0.0098208
Epoch: 010 | Train Loss: 0.0104895 | Grad norm: 0.026579 | Time: 8s264ms
Epoch: 010 | Test Loss: 0.0098589 | Time: 691ms
Epoch: 011 | Train Loss: 0.0103821 | Grad norm: 0.023964 | Time: 8s184ms
Epoch: 011 | Test Loss: 0.0098276 | Time: 630ms
Epoch: 012 | Train Loss: 0.0103393 | Grad norm: 0.023292 | Time: 8s345ms
Epoch: 012 | Test Loss: 0.0101320 | Time: 624ms
Epoch: 013 | Train Loss: 0.0101801 | Grad norm: 0.018216 | Time: 8s363ms
Epoch: 013 | Test Loss: 0.0099807 | Time: 635ms
Epoch: 014 | Train Loss: 0.0101685 | Grad norm: 0.019510 | Time: 8s282ms
Epoch: 014 | Test Loss: 0.0098565 | Time: 615ms
Epoch: 015 | Train Loss: 0.0102064 | Grad norm: 0.022524 | Time: 8s201ms
Epoch: 015 | Test Loss: 0.0095928 | Time: 692ms
==> Save the model at epoch 015 with test loss 0.0095928
Epoch: 016 | Train Loss: 0.0100926 | Grad norm: 0.017365 | Time: 8s585ms
Epoch: 016 | Test Loss: 0.0096334 | Time: 616ms
Epoch: 017 | Train Loss: 0.0100786 | Grad norm: 0.017437 | Time: 8s406ms
Epoch: 017 | Test Loss: 0.0097850 | Time: 615ms
Epoch: 018 | Train Loss: 0.0100258 | Grad norm: 0.017674 | Time: 8s467ms
Epoch: 018 | Test Loss: 0.0097146 | Time: 744ms
Epoch: 019 | Train Loss: 0.0099954 | Grad norm: 0.015783 | Time: 8s292ms
Epoch: 019 | Test Loss: 0.0096706 | Time: 626ms
Epoch: 020 | Train Loss: 0.0099307 | Grad norm: 0.013010 | Time: 8s476ms
Epoch: 020 | Test Loss: 0.0095679 | Time: 624ms
==> Save the model at epoch 020 with test loss 0.0095679
Epoch: 021 | Train Loss: 0.0099201 | Grad norm: 0.014389 | Time: 8s683ms
Epoch: 021 | Test Loss: 0.0095458 | Time: 634ms
==> Save the model at epoch 021 with test loss 0.0095458
Epoch: 022 | Train Loss: 0.0098617 | Grad norm: 0.013026 | Time: 8s349ms
Epoch: 022 | Test Loss: 0.0099755 | Time: 628ms
Epoch: 023 | Train Loss: 0.0098502 | Grad norm: 0.013446 | Time: 8s176ms
Epoch: 023 | Test Loss: 0.0099399 | Time: 684ms
Epoch: 024 | Train Loss: 0.0098179 | Grad norm: 0.013406 | Time: 8s197ms
Epoch: 024 | Test Loss: 0.0095377 | Time: 628ms
==> Save the model at epoch 024 with test loss 0.0095377
Epoch: 025 | Train Loss: 0.0097783 | Grad norm: 0.011795 | Time: 8s328ms
Epoch: 025 | Test Loss: 0.0095329 | Time: 616ms
==> Save the model at epoch 025 with test loss 0.0095329
Epoch: 026 | Train Loss: 0.0097533 | Grad norm: 0.011755 | Time: 8s383ms
Epoch: 026 | Test Loss: 0.0096417 | Time: 625ms
Epoch: 027 | Train Loss: 0.0097500 | Grad norm: 0.012926 | Time: 8s310ms
Epoch: 027 | Test Loss: 0.0094014 | Time: 614ms
==> Save the model at epoch 027 with test loss 0.0094014
Epoch: 028 | Train Loss: 0.0097424 | Grad norm: 0.012832 | Time: 8s60ms
Epoch: 028 | Test Loss: 0.0094853 | Time: 617ms
Epoch: 029 | Train Loss: 0.0097049 | Grad norm: 0.012073 | Time: 8s509ms
Epoch: 029 | Test Loss: 0.0094496 | Time: 609ms
Epoch: 030 | Train Loss: 0.0096799 | Grad norm: 0.011005 | Time: 8s128ms
Epoch: 030 | Test Loss: 0.0094084 | Time: 639ms
Epoch: 031 | Train Loss: 0.0096376 | Grad norm: 0.011352 | Time: 8s513ms
Epoch: 031 | Test Loss: 0.0093191 | Time: 625ms
==> Save the model at epoch 031 with test loss 0.0093191
Epoch: 032 | Train Loss: 0.0095959 | Grad norm: 0.010108 | Time: 8s89ms
Epoch: 032 | Test Loss: 0.0092884 | Time: 626ms
==> Save the model at epoch 032 with test loss 0.0092884
Epoch: 033 | Train Loss: 0.0095789 | Grad norm: 0.009609 | Time: 8s472ms
Epoch: 033 | Test Loss: 0.0093312 | Time: 641ms
Epoch: 034 | Train Loss: 0.0095389 | Grad norm: 0.009703 | Time: 8s580ms
Epoch: 034 | Test Loss: 0.0092478 | Time: 621ms
==> Save the model at epoch 034 with test loss 0.0092478
Epoch: 035 | Train Loss: 0.0095192 | Grad norm: 0.009133 | Time: 8s851ms
Epoch: 035 | Test Loss: 0.0092790 | Time: 622ms
Epoch: 036 | Train Loss: 0.0095001 | Grad norm: 0.009079 | Time: 8s642ms
Epoch: 036 | Test Loss: 0.0092223 | Time: 688ms
==> Save the model at epoch 036 with test loss 0.0092223
Epoch: 037 | Train Loss: 0.0094700 | Grad norm: 0.008682 | Time: 8s141ms
Epoch: 037 | Test Loss: 0.0092133 | Time: 612ms
==> Save the model at epoch 037 with test loss 0.0092133
Epoch: 038 | Train Loss: 0.0094606 | Grad norm: 0.008089 | Time: 8s390ms
Epoch: 038 | Test Loss: 0.0092001 | Time: 614ms
==> Save the model at epoch 038 with test loss 0.0092001
Epoch: 039 | Train Loss: 0.0094420 | Grad norm: 0.008126 | Time: 8s933ms
Epoch: 039 | Test Loss: 0.0091858 | Time: 621ms
==> Save the model at epoch 039 with test loss 0.0091858
Epoch: 040 | Train Loss: 0.0094442 | Grad norm: 0.008185 | Time: 8s576ms
Epoch: 040 | Test Loss: 0.0091835 | Time: 673ms
==> Save the model at epoch 040 with test loss 0.0091835
Total time: 6m2s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0. 0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.7072636  0.72212595 0.6760411  0.67349136]
==> Output transform to be applied to the neural network (trained):
[0.2287 0.2871]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
