==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0. 0. 0.]
==> Input transform to be applied to the neural network:
[1. 1. 1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    10
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   320
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,420
Trainable params: 25,410
Non-trainable params: 10
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.11
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 0.0202929 | L2 loss: 0.0185895 | Lip loss: 0.0017034 | Grad norm: 0.103600 | Time: 35s29ms
Epoch: 001 | Test Loss: 0.0107132 | Time: 519ms
==> Save the model at epoch 001 with test loss 0.0107132
Epoch: 002 | Loss: 0.0119206 | L2 loss: 0.0101263 | Lip loss: 0.0017943 | Grad norm: 0.066096 | Time: 34s157ms
Epoch: 002 | Test Loss: 0.0107948 | Time: 519ms
Epoch: 003 | Loss: 0.0117874 | L2 loss: 0.0099916 | Lip loss: 0.0017958 | Grad norm: 0.060591 | Time: 34s631ms
Epoch: 003 | Test Loss: 0.0106514 | Time: 499ms
==> Save the model at epoch 003 with test loss 0.0106514
Epoch: 004 | Loss: 0.0116880 | L2 loss: 0.0099005 | Lip loss: 0.0017876 | Grad norm: 0.056584 | Time: 34s178ms
Epoch: 004 | Test Loss: 0.0107075 | Time: 507ms
Epoch: 005 | Loss: 0.0116205 | L2 loss: 0.0098250 | Lip loss: 0.0017954 | Grad norm: 0.054347 | Time: 34s155ms
Epoch: 005 | Test Loss: 0.0104406 | Time: 517ms
==> Save the model at epoch 005 with test loss 0.0104406
Epoch: 006 | Loss: 0.0112620 | L2 loss: 0.0094660 | Lip loss: 0.0017961 | Grad norm: 0.030039 | Time: 34s395ms
Epoch: 006 | Test Loss: 0.0101595 | Time: 578ms
==> Save the model at epoch 006 with test loss 0.0101595
Epoch: 007 | Loss: 0.0112258 | L2 loss: 0.0094257 | Lip loss: 0.0018001 | Grad norm: 0.027929 | Time: 33s832ms
Epoch: 007 | Test Loss: 0.0101058 | Time: 598ms
==> Save the model at epoch 007 with test loss 0.0101058
Epoch: 008 | Loss: 0.0112047 | L2 loss: 0.0094129 | Lip loss: 0.0017918 | Grad norm: 0.030063 | Time: 34s308ms
Epoch: 008 | Test Loss: 0.0101159 | Time: 504ms
Epoch: 009 | Loss: 0.0111868 | L2 loss: 0.0093904 | Lip loss: 0.0017965 | Grad norm: 0.030743 | Time: 34s211ms
Epoch: 009 | Test Loss: 0.0100854 | Time: 500ms
==> Save the model at epoch 009 with test loss 0.0100854
Epoch: 010 | Loss: 0.0111685 | L2 loss: 0.0093688 | Lip loss: 0.0017996 | Grad norm: 0.031237 | Time: 34s401ms
Epoch: 010 | Test Loss: 0.0100754 | Time: 508ms
==> Save the model at epoch 010 with test loss 0.0100754
Epoch: 011 | Loss: 0.0111253 | L2 loss: 0.0093154 | Lip loss: 0.0018099 | Grad norm: 0.027133 | Time: 34s604ms
Epoch: 011 | Test Loss: 0.0100177 | Time: 499ms
==> Save the model at epoch 011 with test loss 0.0100177
Epoch: 012 | Loss: 0.0110994 | L2 loss: 0.0093067 | Lip loss: 0.0017927 | Grad norm: 0.026840 | Time: 34s657ms
Epoch: 012 | Test Loss: 0.0100153 | Time: 501ms
==> Save the model at epoch 012 with test loss 0.0100153
Epoch: 013 | Loss: 0.0111162 | L2 loss: 0.0093039 | Lip loss: 0.0018123 | Grad norm: 0.026942 | Time: 34s285ms
Epoch: 013 | Test Loss: 0.0100069 | Time: 517ms
==> Save the model at epoch 013 with test loss 0.0100069
Epoch: 014 | Loss: 0.0110949 | L2 loss: 0.0093003 | Lip loss: 0.0017946 | Grad norm: 0.027654 | Time: 34s460ms
Epoch: 014 | Test Loss: 0.0100056 | Time: 504ms
==> Save the model at epoch 014 with test loss 0.0100056
Epoch: 015 | Loss: 0.0110899 | L2 loss: 0.0092962 | Lip loss: 0.0017937 | Grad norm: 0.027118 | Time: 34s437ms
Epoch: 015 | Test Loss: 0.0099981 | Time: 583ms
==> Save the model at epoch 015 with test loss 0.0099981
Epoch: 016 | Loss: 0.0110850 | L2 loss: 0.0092870 | Lip loss: 0.0017980 | Grad norm: 0.026493 | Time: 34s188ms
Epoch: 016 | Test Loss: 0.0099997 | Time: 573ms
Epoch: 017 | Loss: 0.0110959 | L2 loss: 0.0092880 | Lip loss: 0.0018079 | Grad norm: 0.026942 | Time: 33s684ms
Epoch: 017 | Test Loss: 0.0100001 | Time: 503ms
Epoch: 018 | Loss: 0.0110884 | L2 loss: 0.0092876 | Lip loss: 0.0018008 | Grad norm: 0.026862 | Time: 34s276ms
Epoch: 018 | Test Loss: 0.0099999 | Time: 502ms
Epoch: 019 | Loss: 0.0110930 | L2 loss: 0.0092873 | Lip loss: 0.0018057 | Grad norm: 0.027279 | Time: 34s630ms
Epoch: 019 | Test Loss: 0.0099984 | Time: 498ms
Epoch: 020 | Loss: 0.0110930 | L2 loss: 0.0092867 | Lip loss: 0.0018063 | Grad norm: 0.027026 | Time: 33s964ms
Epoch: 020 | Test Loss: 0.0099989 | Time: 508ms
Epoch: 021 | Loss: 0.0110950 | L2 loss: 0.0092862 | Lip loss: 0.0018088 | Grad norm: 0.027884 | Time: 34s388ms
Epoch: 021 | Test Loss: 0.0099986 | Time: 508ms
Epoch: 022 | Loss: 0.0110907 | L2 loss: 0.0092863 | Lip loss: 0.0018044 | Grad norm: 0.026500 | Time: 34s491ms
Epoch: 022 | Test Loss: 0.0099984 | Time: 508ms
Epoch: 023 | Loss: 0.0110909 | L2 loss: 0.0092860 | Lip loss: 0.0018049 | Grad norm: 0.027060 | Time: 34s516ms
Epoch: 023 | Test Loss: 0.0099983 | Time: 514ms
Epoch: 024 | Loss: 0.0110906 | L2 loss: 0.0092860 | Lip loss: 0.0018047 | Grad norm: 0.026759 | Time: 34s387ms
Epoch: 024 | Test Loss: 0.0099983 | Time: 576ms
Epoch: 025 | Loss: 0.0110926 | L2 loss: 0.0092859 | Lip loss: 0.0018067 | Grad norm: 0.026589 | Time: 33s842ms
Epoch: 025 | Test Loss: 0.0099981 | Time: 572ms
==> Save the model at epoch 025 with test loss 0.0099981
Epoch: 026 | Loss: 0.0110902 | L2 loss: 0.0092858 | Lip loss: 0.0018044 | Grad norm: 0.026503 | Time: 33s713ms
Epoch: 026 | Test Loss: 0.0099981 | Time: 515ms
==> Save the model at epoch 026 with test loss 0.0099981
Epoch: 027 | Loss: 0.0110988 | L2 loss: 0.0092858 | Lip loss: 0.0018130 | Grad norm: 0.026580 | Time: 34s86ms
Epoch: 027 | Test Loss: 0.0099981 | Time: 512ms
Epoch: 028 | Loss: 0.0110813 | L2 loss: 0.0092858 | Lip loss: 0.0017955 | Grad norm: 0.026758 | Time: 34s136ms
Epoch: 028 | Test Loss: 0.0099981 | Time: 501ms
==> Save the model at epoch 028 with test loss 0.0099981
Epoch: 029 | Loss: 0.0110817 | L2 loss: 0.0092858 | Lip loss: 0.0017959 | Grad norm: 0.026995 | Time: 34s254ms
Epoch: 029 | Test Loss: 0.0099981 | Time: 514ms
==> Save the model at epoch 029 with test loss 0.0099981
Epoch: 030 | Loss: 0.0110947 | L2 loss: 0.0092858 | Lip loss: 0.0018090 | Grad norm: 0.026364 | Time: 33s229ms
Epoch: 030 | Test Loss: 0.0099981 | Time: 503ms
==> Save the model at epoch 030 with test loss 0.0099981
Epoch: 031 | Loss: 0.0110918 | L2 loss: 0.0092858 | Lip loss: 0.0018060 | Grad norm: 0.026597 | Time: 34s565ms
Epoch: 031 | Test Loss: 0.0099981 | Time: 508ms
==> Save the model at epoch 031 with test loss 0.0099981
Epoch: 032 | Loss: 0.0110926 | L2 loss: 0.0092858 | Lip loss: 0.0018068 | Grad norm: 0.026878 | Time: 33s979ms
Epoch: 032 | Test Loss: 0.0099981 | Time: 569ms
Epoch: 033 | Loss: 0.0110965 | L2 loss: 0.0092858 | Lip loss: 0.0018108 | Grad norm: 0.027525 | Time: 33s406ms
Epoch: 033 | Test Loss: 0.0099981 | Time: 506ms
==> Save the model at epoch 033 with test loss 0.0099981
Epoch: 034 | Loss: 0.0110685 | L2 loss: 0.0092858 | Lip loss: 0.0017827 | Grad norm: 0.026638 | Time: 34s739ms
Epoch: 034 | Test Loss: 0.0099981 | Time: 575ms
Epoch: 035 | Loss: 0.0110973 | L2 loss: 0.0092858 | Lip loss: 0.0018115 | Grad norm: 0.027085 | Time: 34s230ms
Epoch: 035 | Test Loss: 0.0099981 | Time: 575ms
Epoch: 036 | Loss: 0.0111013 | L2 loss: 0.0092858 | Lip loss: 0.0018155 | Grad norm: 0.025591 | Time: 34s42ms
Epoch: 036 | Test Loss: 0.0099981 | Time: 500ms
Epoch: 037 | Loss: 0.0110766 | L2 loss: 0.0092858 | Lip loss: 0.0017908 | Grad norm: 0.026313 | Time: 34s235ms
Epoch: 037 | Test Loss: 0.0099981 | Time: 506ms
Epoch: 038 | Loss: 0.0110971 | L2 loss: 0.0092858 | Lip loss: 0.0018113 | Grad norm: 0.026695 | Time: 33s820ms
Epoch: 038 | Test Loss: 0.0099981 | Time: 503ms
Epoch: 039 | Loss: 0.0110844 | L2 loss: 0.0092858 | Lip loss: 0.0017987 | Grad norm: 0.027121 | Time: 34s512ms
Epoch: 039 | Test Loss: 0.0099981 | Time: 501ms
Epoch: 040 | Loss: 0.0110874 | L2 loss: 0.0092858 | Lip loss: 0.0018016 | Grad norm: 0.026413 | Time: 33s936ms
Epoch: 040 | Test Loss: 0.0099981 | Time: 545ms
==> Save the model at epoch 040 with test loss 0.0099981
Total time: 23m10s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0. 0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1. 1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
