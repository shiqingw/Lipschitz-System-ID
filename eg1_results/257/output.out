==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 9.4842299 | L2 loss: 9.4703590 | Lip loss: 0.0138708 | Grad norm: 5.249508 | Time: 40s306ms
Epoch: 001 | Test Loss: 0.0068741 | Time: 520ms
==> Save the model at epoch 001 with test loss 0.0068741
Epoch: 002 | Loss: 0.0260330 | L2 loss: 0.0051840 | Lip loss: 0.0208489 | Grad norm: 1.419417 | Time: 40s960ms
Epoch: 002 | Test Loss: 0.0040484 | Time: 549ms
==> Save the model at epoch 002 with test loss 0.0040484
Epoch: 003 | Loss: 0.0246743 | L2 loss: 0.0041083 | Lip loss: 0.0205660 | Grad norm: 1.285013 | Time: 41s644ms
Epoch: 003 | Test Loss: 0.0040799 | Time: 531ms
Epoch: 004 | Loss: 0.0241035 | L2 loss: 0.0036443 | Lip loss: 0.0204593 | Grad norm: 1.072007 | Time: 41s164ms
Epoch: 004 | Test Loss: 0.0035775 | Time: 522ms
==> Save the model at epoch 004 with test loss 0.0035775
Epoch: 005 | Loss: 0.0239357 | L2 loss: 0.0035396 | Lip loss: 0.0203960 | Grad norm: 1.132020 | Time: 41s104ms
Epoch: 005 | Test Loss: 0.0034720 | Time: 535ms
==> Save the model at epoch 005 with test loss 0.0034720
Epoch: 006 | Loss: 0.0234256 | L2 loss: 0.0030637 | Lip loss: 0.0203619 | Grad norm: 0.343355 | Time: 41s167ms
Epoch: 006 | Test Loss: 0.0030681 | Time: 602ms
==> Save the model at epoch 006 with test loss 0.0030681
Epoch: 007 | Loss: 0.0234024 | L2 loss: 0.0030491 | Lip loss: 0.0203533 | Grad norm: 0.331214 | Time: 41s298ms
Epoch: 007 | Test Loss: 0.0030558 | Time: 532ms
==> Save the model at epoch 007 with test loss 0.0030558
Epoch: 008 | Loss: 0.0233976 | L2 loss: 0.0030442 | Lip loss: 0.0203533 | Grad norm: 0.356440 | Time: 40s504ms
Epoch: 008 | Test Loss: 0.0030797 | Time: 524ms
Epoch: 009 | Loss: 0.0234083 | L2 loss: 0.0030431 | Lip loss: 0.0203651 | Grad norm: 0.396858 | Time: 41s150ms
Epoch: 009 | Test Loss: 0.0030229 | Time: 530ms
==> Save the model at epoch 009 with test loss 0.0030229
Epoch: 010 | Loss: 0.0233732 | L2 loss: 0.0030332 | Lip loss: 0.0203400 | Grad norm: 0.394254 | Time: 40s968ms
Epoch: 010 | Test Loss: 0.0029996 | Time: 514ms
==> Save the model at epoch 010 with test loss 0.0029996
Epoch: 011 | Loss: 0.0233404 | L2 loss: 0.0029904 | Lip loss: 0.0203500 | Grad norm: 0.261404 | Time: 41s321ms
Epoch: 011 | Test Loss: 0.0030035 | Time: 523ms
Epoch: 012 | Loss: 0.0233358 | L2 loss: 0.0029888 | Lip loss: 0.0203470 | Grad norm: 0.253758 | Time: 41s510ms
Epoch: 012 | Test Loss: 0.0029975 | Time: 549ms
==> Save the model at epoch 012 with test loss 0.0029975
Epoch: 013 | Loss: 0.0233164 | L2 loss: 0.0029882 | Lip loss: 0.0203281 | Grad norm: 0.255463 | Time: 40s324ms
Epoch: 013 | Test Loss: 0.0029971 | Time: 605ms
==> Save the model at epoch 013 with test loss 0.0029971
Epoch: 014 | Loss: 0.0233439 | L2 loss: 0.0029862 | Lip loss: 0.0203577 | Grad norm: 0.251695 | Time: 41s722ms
Epoch: 014 | Test Loss: 0.0029949 | Time: 524ms
==> Save the model at epoch 014 with test loss 0.0029949
Epoch: 015 | Loss: 0.0233259 | L2 loss: 0.0029854 | Lip loss: 0.0203405 | Grad norm: 0.251640 | Time: 41s73ms
Epoch: 015 | Test Loss: 0.0030063 | Time: 526ms
Epoch: 016 | Loss: 0.0233373 | L2 loss: 0.0029805 | Lip loss: 0.0203569 | Grad norm: 0.240686 | Time: 41s674ms
Epoch: 016 | Test Loss: 0.0029927 | Time: 740ms
==> Save the model at epoch 016 with test loss 0.0029927
Epoch: 017 | Loss: 0.0233383 | L2 loss: 0.0029805 | Lip loss: 0.0203578 | Grad norm: 0.235597 | Time: 41s32ms
Epoch: 017 | Test Loss: 0.0029909 | Time: 535ms
==> Save the model at epoch 017 with test loss 0.0029909
Epoch: 018 | Loss: 0.0233196 | L2 loss: 0.0029794 | Lip loss: 0.0203402 | Grad norm: 0.243018 | Time: 41s560ms
Epoch: 018 | Test Loss: 0.0029921 | Time: 524ms
Epoch: 019 | Loss: 0.0233303 | L2 loss: 0.0029798 | Lip loss: 0.0203505 | Grad norm: 0.242336 | Time: 41s549ms
Epoch: 019 | Test Loss: 0.0029906 | Time: 531ms
==> Save the model at epoch 019 with test loss 0.0029906
Epoch: 020 | Loss: 0.0233210 | L2 loss: 0.0029797 | Lip loss: 0.0203413 | Grad norm: 0.238086 | Time: 41s786ms
Epoch: 020 | Test Loss: 0.0029908 | Time: 520ms
Epoch: 021 | Loss: 0.0233327 | L2 loss: 0.0029792 | Lip loss: 0.0203535 | Grad norm: 0.239030 | Time: 40s608ms
Epoch: 021 | Test Loss: 0.0029907 | Time: 525ms
Epoch: 022 | Loss: 0.0233241 | L2 loss: 0.0029787 | Lip loss: 0.0203454 | Grad norm: 0.249591 | Time: 41s663ms
Epoch: 022 | Test Loss: 0.0029907 | Time: 526ms
Epoch: 023 | Loss: 0.0233401 | L2 loss: 0.0029791 | Lip loss: 0.0203610 | Grad norm: 0.235485 | Time: 40s918ms
Epoch: 023 | Test Loss: 0.0029907 | Time: 604ms
Epoch: 024 | Loss: 0.0233394 | L2 loss: 0.0029789 | Lip loss: 0.0203605 | Grad norm: 0.235951 | Time: 42s2ms
Epoch: 024 | Test Loss: 0.0029907 | Time: 529ms
Epoch: 025 | Loss: 0.0233254 | L2 loss: 0.0029789 | Lip loss: 0.0203465 | Grad norm: 0.235657 | Time: 41s776ms
Epoch: 025 | Test Loss: 0.0029907 | Time: 524ms
Epoch: 026 | Loss: 0.0233073 | L2 loss: 0.0029791 | Lip loss: 0.0203282 | Grad norm: 0.238470 | Time: 41s286ms
Epoch: 026 | Test Loss: 0.0029907 | Time: 514ms
Epoch: 027 | Loss: 0.0233104 | L2 loss: 0.0029791 | Lip loss: 0.0203312 | Grad norm: 0.235307 | Time: 40s239ms
Epoch: 027 | Test Loss: 0.0029907 | Time: 666ms
Epoch: 028 | Loss: 0.0233149 | L2 loss: 0.0029786 | Lip loss: 0.0203363 | Grad norm: 0.240107 | Time: 41s285ms
Epoch: 028 | Test Loss: 0.0029908 | Time: 523ms
Epoch: 029 | Loss: 0.0233105 | L2 loss: 0.0029789 | Lip loss: 0.0203316 | Grad norm: 0.235506 | Time: 40s512ms
Epoch: 029 | Test Loss: 0.0029908 | Time: 517ms
Epoch: 030 | Loss: 0.0233505 | L2 loss: 0.0029789 | Lip loss: 0.0203716 | Grad norm: 0.230981 | Time: 41s228ms
Epoch: 030 | Test Loss: 0.0029908 | Time: 528ms
Epoch: 031 | Loss: 0.0233179 | L2 loss: 0.0029789 | Lip loss: 0.0203390 | Grad norm: 0.240274 | Time: 40s375ms
Epoch: 031 | Test Loss: 0.0029908 | Time: 534ms
Epoch: 032 | Loss: 0.0233298 | L2 loss: 0.0029789 | Lip loss: 0.0203509 | Grad norm: 0.235186 | Time: 41s266ms
Epoch: 032 | Test Loss: 0.0029908 | Time: 539ms
Epoch: 033 | Loss: 0.0233326 | L2 loss: 0.0029789 | Lip loss: 0.0203536 | Grad norm: 0.238441 | Time: 40s361ms
Epoch: 033 | Test Loss: 0.0029908 | Time: 521ms
Epoch: 034 | Loss: 0.0233194 | L2 loss: 0.0029789 | Lip loss: 0.0203405 | Grad norm: 0.235847 | Time: 42s97ms
Epoch: 034 | Test Loss: 0.0029908 | Time: 554ms
Epoch: 035 | Loss: 0.0233329 | L2 loss: 0.0029788 | Lip loss: 0.0203541 | Grad norm: 0.235553 | Time: 41s772ms
Epoch: 035 | Test Loss: 0.0029908 | Time: 526ms
Epoch: 036 | Loss: 0.0233260 | L2 loss: 0.0029792 | Lip loss: 0.0203468 | Grad norm: 0.241758 | Time: 40s808ms
Epoch: 036 | Test Loss: 0.0029908 | Time: 528ms
Epoch: 037 | Loss: 0.0233049 | L2 loss: 0.0029790 | Lip loss: 0.0203259 | Grad norm: 0.242471 | Time: 40s784ms
Epoch: 037 | Test Loss: 0.0029908 | Time: 598ms
Epoch: 038 | Loss: 0.0233311 | L2 loss: 0.0029788 | Lip loss: 0.0203523 | Grad norm: 0.240664 | Time: 41s717ms
Epoch: 038 | Test Loss: 0.0029908 | Time: 539ms
Epoch: 039 | Loss: 0.0233017 | L2 loss: 0.0029790 | Lip loss: 0.0203228 | Grad norm: 0.238082 | Time: 40s923ms
Epoch: 039 | Test Loss: 0.0029908 | Time: 524ms
Epoch: 040 | Loss: 0.0233179 | L2 loss: 0.0029791 | Lip loss: 0.0203388 | Grad norm: 0.239489 | Time: 41s3ms
Epoch: 040 | Test Loss: 0.0029908 | Time: 520ms
Total time: 27m48s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
