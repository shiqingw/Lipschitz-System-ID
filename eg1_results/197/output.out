==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 20.5020701 | L2 loss: 20.4995810 | Lip loss: 0.0024891 | Grad norm: 1.350070 | Time: 10s57ms
Epoch: 001 | Test Loss: 19.6786507 | Time: 521ms
==> Save the model at epoch 001 with test loss 19.6786507
Epoch: 002 | Loss: 14.8159598 | L2 loss: 14.7232695 | Lip loss: 0.0926903 | Grad norm: 9.638233 | Time: 10s329ms
Epoch: 002 | Test Loss: 7.1931634 | Time: 532ms
==> Save the model at epoch 002 with test loss 7.1931634
Epoch: 003 | Loss: 2.1363623 | L2 loss: 1.9019258 | Lip loss: 0.2344365 | Grad norm: 7.625680 | Time: 9s799ms
Epoch: 003 | Test Loss: 0.0370803 | Time: 518ms
==> Save the model at epoch 003 with test loss 0.0370803
Epoch: 004 | Loss: 0.2280207 | L2 loss: 0.0148684 | Lip loss: 0.2131523 | Grad norm: 2.328147 | Time: 10s53ms
Epoch: 004 | Test Loss: 0.0082752 | Time: 592ms
==> Save the model at epoch 004 with test loss 0.0082752
Epoch: 005 | Loss: 0.2160674 | L2 loss: 0.0083710 | Lip loss: 0.2076964 | Grad norm: 2.060169 | Time: 9s989ms
Epoch: 005 | Test Loss: 0.0079196 | Time: 526ms
==> Save the model at epoch 005 with test loss 0.0079196
Epoch: 006 | Loss: 0.2125008 | L2 loss: 0.0059544 | Lip loss: 0.2065464 | Grad norm: 1.322327 | Time: 10s357ms
Epoch: 006 | Test Loss: 0.0058753 | Time: 535ms
==> Save the model at epoch 006 with test loss 0.0058753
Epoch: 007 | Loss: 0.2115976 | L2 loss: 0.0058071 | Lip loss: 0.2057905 | Grad norm: 1.213800 | Time: 10s209ms
Epoch: 007 | Test Loss: 0.0056966 | Time: 522ms
==> Save the model at epoch 007 with test loss 0.0056966
Epoch: 008 | Loss: 0.2110969 | L2 loss: 0.0056539 | Lip loss: 0.2054430 | Grad norm: 1.197393 | Time: 10s79ms
Epoch: 008 | Test Loss: 0.0055822 | Time: 533ms
==> Save the model at epoch 008 with test loss 0.0055822
Epoch: 009 | Loss: 0.2111272 | L2 loss: 0.0055741 | Lip loss: 0.2055531 | Grad norm: 1.192147 | Time: 9s914ms
Epoch: 009 | Test Loss: 0.0054494 | Time: 514ms
==> Save the model at epoch 009 with test loss 0.0054494
Epoch: 010 | Loss: 0.2113428 | L2 loss: 0.0055143 | Lip loss: 0.2058285 | Grad norm: 1.159891 | Time: 10s11ms
Epoch: 010 | Test Loss: 0.0054507 | Time: 516ms
Epoch: 011 | Loss: 0.2106008 | L2 loss: 0.0053225 | Lip loss: 0.2052783 | Grad norm: 1.040844 | Time: 9s937ms
Epoch: 011 | Test Loss: 0.0052361 | Time: 591ms
==> Save the model at epoch 011 with test loss 0.0052361
Epoch: 012 | Loss: 0.2110686 | L2 loss: 0.0052993 | Lip loss: 0.2057693 | Grad norm: 0.966980 | Time: 10s15ms
Epoch: 012 | Test Loss: 0.0052355 | Time: 535ms
==> Save the model at epoch 012 with test loss 0.0052355
Epoch: 013 | Loss: 0.2100030 | L2 loss: 0.0052965 | Lip loss: 0.2047065 | Grad norm: 0.989642 | Time: 9s887ms
Epoch: 013 | Test Loss: 0.0052282 | Time: 518ms
==> Save the model at epoch 013 with test loss 0.0052282
Epoch: 014 | Loss: 0.2099648 | L2 loss: 0.0053088 | Lip loss: 0.2046559 | Grad norm: 1.025885 | Time: 9s811ms
Epoch: 014 | Test Loss: 0.0052073 | Time: 517ms
==> Save the model at epoch 014 with test loss 0.0052073
Epoch: 015 | Loss: 0.2110430 | L2 loss: 0.0052656 | Lip loss: 0.2057774 | Grad norm: 1.042808 | Time: 9s853ms
Epoch: 015 | Test Loss: 0.0052077 | Time: 515ms
Epoch: 016 | Loss: 0.2103230 | L2 loss: 0.0052503 | Lip loss: 0.2050727 | Grad norm: 0.974828 | Time: 9s905ms
Epoch: 016 | Test Loss: 0.0051950 | Time: 519ms
==> Save the model at epoch 016 with test loss 0.0051950
Epoch: 017 | Loss: 0.2105471 | L2 loss: 0.0052468 | Lip loss: 0.2053003 | Grad norm: 0.967859 | Time: 9s873ms
Epoch: 017 | Test Loss: 0.0051900 | Time: 524ms
==> Save the model at epoch 017 with test loss 0.0051900
Epoch: 018 | Loss: 0.2109129 | L2 loss: 0.0052451 | Lip loss: 0.2056678 | Grad norm: 1.006851 | Time: 10s96ms
Epoch: 018 | Test Loss: 0.0051896 | Time: 532ms
==> Save the model at epoch 018 with test loss 0.0051896
Epoch: 019 | Loss: 0.2105659 | L2 loss: 0.0052418 | Lip loss: 0.2053240 | Grad norm: 0.995678 | Time: 9s816ms
Epoch: 019 | Test Loss: 0.0051886 | Time: 532ms
==> Save the model at epoch 019 with test loss 0.0051886
Epoch: 020 | Loss: 0.2111557 | L2 loss: 0.0052434 | Lip loss: 0.2059123 | Grad norm: 0.968184 | Time: 9s876ms
Epoch: 020 | Test Loss: 0.0051911 | Time: 590ms
Epoch: 021 | Loss: 0.2103605 | L2 loss: 0.0052487 | Lip loss: 0.2051118 | Grad norm: 0.922048 | Time: 9s795ms
Epoch: 021 | Test Loss: 0.0051910 | Time: 532ms
Epoch: 022 | Loss: 0.2108589 | L2 loss: 0.0052481 | Lip loss: 0.2056108 | Grad norm: 1.000361 | Time: 9s833ms
Epoch: 022 | Test Loss: 0.0051910 | Time: 528ms
Epoch: 023 | Loss: 0.2107206 | L2 loss: 0.0052479 | Lip loss: 0.2054727 | Grad norm: 0.981094 | Time: 9s798ms
Epoch: 023 | Test Loss: 0.0051909 | Time: 517ms
Epoch: 024 | Loss: 0.2103903 | L2 loss: 0.0052489 | Lip loss: 0.2051415 | Grad norm: 0.995531 | Time: 10s36ms
Epoch: 024 | Test Loss: 0.0051908 | Time: 520ms
Epoch: 025 | Loss: 0.2101946 | L2 loss: 0.0052458 | Lip loss: 0.2049488 | Grad norm: 0.941342 | Time: 9s976ms
Epoch: 025 | Test Loss: 0.0051908 | Time: 518ms
Epoch: 026 | Loss: 0.2107957 | L2 loss: 0.0052460 | Lip loss: 0.2055497 | Grad norm: 0.929500 | Time: 9s723ms
Epoch: 026 | Test Loss: 0.0051908 | Time: 517ms
Epoch: 027 | Loss: 0.2107293 | L2 loss: 0.0052438 | Lip loss: 0.2054855 | Grad norm: 0.961476 | Time: 9s876ms
Epoch: 027 | Test Loss: 0.0051908 | Time: 518ms
Epoch: 028 | Loss: 0.2107872 | L2 loss: 0.0052504 | Lip loss: 0.2055368 | Grad norm: 1.023351 | Time: 9s728ms
Epoch: 028 | Test Loss: 0.0051908 | Time: 523ms
Epoch: 029 | Loss: 0.2107825 | L2 loss: 0.0052479 | Lip loss: 0.2055347 | Grad norm: 0.984885 | Time: 9s636ms
Epoch: 029 | Test Loss: 0.0051908 | Time: 591ms
Epoch: 030 | Loss: 0.2102443 | L2 loss: 0.0052515 | Lip loss: 0.2049928 | Grad norm: 0.960763 | Time: 9s940ms
Epoch: 030 | Test Loss: 0.0051908 | Time: 537ms
Epoch: 031 | Loss: 0.2103740 | L2 loss: 0.0052487 | Lip loss: 0.2051253 | Grad norm: 0.944280 | Time: 9s757ms
Epoch: 031 | Test Loss: 0.0051908 | Time: 519ms
Epoch: 032 | Loss: 0.2102264 | L2 loss: 0.0052433 | Lip loss: 0.2049831 | Grad norm: 0.976917 | Time: 9s951ms
Epoch: 032 | Test Loss: 0.0051908 | Time: 520ms
Epoch: 033 | Loss: 0.2101663 | L2 loss: 0.0052472 | Lip loss: 0.2049191 | Grad norm: 0.951721 | Time: 9s799ms
Epoch: 033 | Test Loss: 0.0051908 | Time: 519ms
Epoch: 034 | Loss: 0.2111285 | L2 loss: 0.0052452 | Lip loss: 0.2058833 | Grad norm: 1.008996 | Time: 9s871ms
Epoch: 034 | Test Loss: 0.0051908 | Time: 519ms
Epoch: 035 | Loss: 0.2099965 | L2 loss: 0.0052493 | Lip loss: 0.2047473 | Grad norm: 0.989011 | Time: 9s754ms
Epoch: 035 | Test Loss: 0.0051908 | Time: 526ms
Epoch: 036 | Loss: 0.2105005 | L2 loss: 0.0052455 | Lip loss: 0.2052550 | Grad norm: 1.023210 | Time: 9s773ms
Epoch: 036 | Test Loss: 0.0051908 | Time: 523ms
Epoch: 037 | Loss: 0.2105647 | L2 loss: 0.0052404 | Lip loss: 0.2053243 | Grad norm: 0.927659 | Time: 9s852ms
Epoch: 037 | Test Loss: 0.0051908 | Time: 517ms
Epoch: 038 | Loss: 0.2106977 | L2 loss: 0.0052453 | Lip loss: 0.2054524 | Grad norm: 0.965832 | Time: 9s846ms
Epoch: 038 | Test Loss: 0.0051908 | Time: 611ms
Epoch: 039 | Loss: 0.2115377 | L2 loss: 0.0052503 | Lip loss: 0.2062874 | Grad norm: 0.956399 | Time: 9s805ms
Epoch: 039 | Test Loss: 0.0051908 | Time: 525ms
Epoch: 040 | Loss: 0.2107958 | L2 loss: 0.0052479 | Lip loss: 0.2055480 | Grad norm: 1.100236 | Time: 9s973ms
Epoch: 040 | Test Loss: 0.0051908 | Time: 529ms
Total time: 6m57s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
