==> torch device:  cuda:1
==> Lipschitz constant: 2.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 128]                  16,897
│    └─SandwichFc: 2-2                   [1, 128]                  33,025
│    └─SandwichFc: 2-3                   [1, 128]                  33,025
│    └─SandwichFc: 2-4                   [1, 128]                  33,025
│    └─SandwichFc: 2-5                   [1, 128]                  33,025
│    └─SandwichFc: 2-6                   [1, 128]                  33,025
│    └─SandwichFc: 2-7                   [1, 128]                  33,025
│    └─SandwichLin: 2-8                  [1, 2]                    263
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 128]                  (recursive)
│    └─SandwichFc: 2-10                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-11                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-12                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-13                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-14                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-15                  [1, 128]                  (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 215,316
Trainable params: 215,310
Non-trainable params: 6
Total mult-adds (M): 0.43
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.50
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 31.0006986 | Grad norm: 15.339559 | Time: 11s952ms
Epoch: 001 | Test Loss: 30.9983877 | Time: 815ms
==> Save the model at epoch 001 with test loss 30.9983877
Epoch: 002 | Train Loss: 3.4545416 | Grad norm: 9.815959 | Time: 10s702ms
Epoch: 002 | Test Loss: 0.0236518 | Time: 800ms
==> Save the model at epoch 002 with test loss 0.0236518
Epoch: 003 | Train Loss: 0.0415586 | Grad norm: 8.579607 | Time: 8s977ms
Epoch: 003 | Test Loss: 0.0861872 | Time: 795ms
Epoch: 004 | Train Loss: 0.0901089 | Grad norm: 16.884113 | Time: 9s151ms
Epoch: 004 | Test Loss: 0.0776054 | Time: 796ms
Epoch: 005 | Train Loss: 0.1200167 | Grad norm: 19.231537 | Time: 9s287ms
Epoch: 005 | Test Loss: 0.0673913 | Time: 802ms
Epoch: 006 | Train Loss: 0.1068231 | Grad norm: 16.161356 | Time: 11s230ms
Epoch: 006 | Test Loss: 0.0347111 | Time: 798ms
Epoch: 007 | Train Loss: 0.0601165 | Grad norm: 12.726631 | Time: 10s640ms
Epoch: 007 | Test Loss: 0.0828462 | Time: 839ms
Epoch: 008 | Train Loss: 0.0682489 | Grad norm: 13.086700 | Time: 9s974ms
Epoch: 008 | Test Loss: 0.0504510 | Time: 803ms
Epoch: 009 | Train Loss: 0.0748295 | Grad norm: 13.481831 | Time: 8s964ms
Epoch: 009 | Test Loss: 0.0625071 | Time: 798ms
Epoch: 010 | Train Loss: 0.0761574 | Grad norm: 13.362693 | Time: 10s523ms
Epoch: 010 | Test Loss: 0.0612713 | Time: 804ms
Epoch: 011 | Train Loss: 0.0767004 | Grad norm: 13.002668 | Time: 11s528ms
Epoch: 011 | Test Loss: 0.0792490 | Time: 841ms
Epoch: 012 | Train Loss: 0.0640191 | Grad norm: 11.467737 | Time: 9s98ms
Epoch: 012 | Test Loss: 0.0377517 | Time: 808ms
Epoch: 013 | Train Loss: 0.0567586 | Grad norm: 10.250584 | Time: 9s745ms
Epoch: 013 | Test Loss: 0.0331824 | Time: 808ms
Epoch: 014 | Train Loss: 0.0497206 | Grad norm: 9.268254 | Time: 11s372ms
Epoch: 014 | Test Loss: 0.0560069 | Time: 802ms
Epoch: 015 | Train Loss: 0.0445014 | Grad norm: 8.383571 | Time: 10s456ms
Epoch: 015 | Test Loss: 0.0360591 | Time: 817ms
Epoch: 016 | Train Loss: 0.0403666 | Grad norm: 7.769135 | Time: 10s721ms
Epoch: 016 | Test Loss: 0.0318100 | Time: 798ms
Epoch: 017 | Train Loss: 0.0354286 | Grad norm: 7.182257 | Time: 9s814ms
Epoch: 017 | Test Loss: 0.0393574 | Time: 793ms
Epoch: 018 | Train Loss: 0.0331953 | Grad norm: 6.621136 | Time: 11s711ms
Epoch: 018 | Test Loss: 0.0366491 | Time: 796ms
Epoch: 019 | Train Loss: 0.0301595 | Grad norm: 6.185488 | Time: 11s114ms
Epoch: 019 | Test Loss: 0.0275216 | Time: 801ms
Epoch: 020 | Train Loss: 0.0303946 | Grad norm: 5.830110 | Time: 11s440ms
Epoch: 020 | Test Loss: 0.0286867 | Time: 810ms
Epoch: 021 | Train Loss: 0.0252342 | Grad norm: 5.415865 | Time: 11s75ms
Epoch: 021 | Test Loss: 0.0197094 | Time: 817ms
==> Save the model at epoch 021 with test loss 0.0197094
Epoch: 022 | Train Loss: 0.0233396 | Grad norm: 5.068782 | Time: 11s386ms
Epoch: 022 | Test Loss: 0.0238712 | Time: 864ms
Epoch: 023 | Train Loss: 0.0224279 | Grad norm: 4.837384 | Time: 10s878ms
Epoch: 023 | Test Loss: 0.0251705 | Time: 809ms
Epoch: 024 | Train Loss: 0.0208877 | Grad norm: 4.524863 | Time: 10s663ms
Epoch: 024 | Test Loss: 0.0270010 | Time: 799ms
Epoch: 025 | Train Loss: 0.0196029 | Grad norm: 4.224255 | Time: 9s43ms
Epoch: 025 | Test Loss: 0.0196429 | Time: 805ms
==> Save the model at epoch 025 with test loss 0.0196429
Epoch: 026 | Train Loss: 0.0176935 | Grad norm: 3.993918 | Time: 8s953ms
Epoch: 026 | Test Loss: 0.0166139 | Time: 803ms
==> Save the model at epoch 026 with test loss 0.0166139
Epoch: 027 | Train Loss: 0.0172601 | Grad norm: 3.722430 | Time: 9s10ms
Epoch: 027 | Test Loss: 0.0199837 | Time: 858ms
Epoch: 028 | Train Loss: 0.0168040 | Grad norm: 3.583925 | Time: 8s990ms
Epoch: 028 | Test Loss: 0.0148576 | Time: 800ms
==> Save the model at epoch 028 with test loss 0.0148576
Epoch: 029 | Train Loss: 0.0153129 | Grad norm: 3.343209 | Time: 8s924ms
Epoch: 029 | Test Loss: 0.0133244 | Time: 816ms
==> Save the model at epoch 029 with test loss 0.0133244
Epoch: 030 | Train Loss: 0.0145941 | Grad norm: 3.178572 | Time: 9s859ms
Epoch: 030 | Test Loss: 0.0149047 | Time: 805ms
Epoch: 031 | Train Loss: 0.0139893 | Grad norm: 2.979102 | Time: 8s943ms
Epoch: 031 | Test Loss: 0.0127814 | Time: 806ms
==> Save the model at epoch 031 with test loss 0.0127814
Epoch: 032 | Train Loss: 0.0129912 | Grad norm: 2.799313 | Time: 8s927ms
Epoch: 032 | Test Loss: 0.0128349 | Time: 863ms
Epoch: 033 | Train Loss: 0.0127202 | Grad norm: 2.646404 | Time: 8s939ms
Epoch: 033 | Test Loss: 0.0128578 | Time: 805ms
Epoch: 034 | Train Loss: 0.0127919 | Grad norm: 2.476533 | Time: 8s927ms
Epoch: 034 | Test Loss: 0.0115624 | Time: 802ms
==> Save the model at epoch 034 with test loss 0.0115624
Epoch: 035 | Train Loss: 0.0115866 | Grad norm: 2.325616 | Time: 8s976ms
Epoch: 035 | Test Loss: 0.0101323 | Time: 802ms
==> Save the model at epoch 035 with test loss 0.0101323
Epoch: 036 | Train Loss: 0.0116792 | Grad norm: 2.176509 | Time: 8s915ms
Epoch: 036 | Test Loss: 0.0108320 | Time: 802ms
Epoch: 037 | Train Loss: 0.0106344 | Grad norm: 2.043998 | Time: 9s42ms
Epoch: 037 | Test Loss: 0.0106096 | Time: 860ms
Epoch: 038 | Train Loss: 0.0147757 | Grad norm: 3.113077 | Time: 8s932ms
Epoch: 038 | Test Loss: 0.0143192 | Time: 799ms
Epoch: 039 | Train Loss: 0.0137563 | Grad norm: 3.098816 | Time: 10s67ms
Epoch: 039 | Test Loss: 0.0116657 | Time: 794ms
Epoch: 040 | Train Loss: 0.0126165 | Grad norm: 2.818590 | Time: 10s849ms
Epoch: 040 | Test Loss: 0.0140996 | Time: 864ms
Epoch: 041 | Train Loss: 0.0119510 | Grad norm: 2.558858 | Time: 12s119ms
Epoch: 041 | Test Loss: 0.0108799 | Time: 800ms
Epoch: 042 | Train Loss: 0.0111159 | Grad norm: 2.328394 | Time: 11s135ms
Epoch: 042 | Test Loss: 0.0095698 | Time: 891ms
==> Save the model at epoch 042 with test loss 0.0095698
Epoch: 043 | Train Loss: 0.0097974 | Grad norm: 1.639226 | Time: 10s786ms
Epoch: 043 | Test Loss: 0.0078701 | Time: 798ms
==> Save the model at epoch 043 with test loss 0.0078701
Epoch: 044 | Train Loss: 0.0085070 | Grad norm: 0.913126 | Time: 10s24ms
Epoch: 044 | Test Loss: 0.0130698 | Time: 800ms
Epoch: 045 | Train Loss: 0.0083528 | Grad norm: 0.818754 | Time: 9s33ms
Epoch: 045 | Test Loss: 0.0081726 | Time: 788ms
Epoch: 046 | Train Loss: 0.0079941 | Grad norm: 0.624374 | Time: 9s188ms
Epoch: 046 | Test Loss: 0.0076341 | Time: 797ms
==> Save the model at epoch 046 with test loss 0.0076341
Epoch: 047 | Train Loss: 0.0079262 | Grad norm: 0.722258 | Time: 8s985ms
Epoch: 047 | Test Loss: 0.0077855 | Time: 852ms
Epoch: 048 | Train Loss: 0.0077050 | Grad norm: 0.544107 | Time: 9s303ms
Epoch: 048 | Test Loss: 0.0085504 | Time: 797ms
Epoch: 049 | Train Loss: 0.0077225 | Grad norm: 0.597626 | Time: 8s983ms
Epoch: 049 | Test Loss: 0.0075915 | Time: 791ms
==> Save the model at epoch 049 with test loss 0.0075915
Epoch: 050 | Train Loss: 0.0076658 | Grad norm: 0.574017 | Time: 9s70ms
Epoch: 050 | Test Loss: 0.0078452 | Time: 789ms
Epoch: 051 | Train Loss: 0.0075093 | Grad norm: 0.512756 | Time: 9s308ms
Epoch: 051 | Test Loss: 0.0077101 | Time: 800ms
Epoch: 052 | Train Loss: 0.0074835 | Grad norm: 0.513947 | Time: 9s20ms
Epoch: 052 | Test Loss: 0.0074394 | Time: 858ms
==> Save the model at epoch 052 with test loss 0.0074394
Epoch: 053 | Train Loss: 0.0074136 | Grad norm: 0.506082 | Time: 9s6ms
Epoch: 053 | Test Loss: 0.0074074 | Time: 790ms
==> Save the model at epoch 053 with test loss 0.0074074
Epoch: 054 | Train Loss: 0.0073200 | Grad norm: 0.420277 | Time: 9s75ms
Epoch: 054 | Test Loss: 0.0073157 | Time: 813ms
==> Save the model at epoch 054 with test loss 0.0073157
Epoch: 055 | Train Loss: 0.0073001 | Grad norm: 0.386011 | Time: 9s51ms
Epoch: 055 | Test Loss: 0.0072956 | Time: 798ms
==> Save the model at epoch 055 with test loss 0.0072956
Epoch: 056 | Train Loss: 0.0072135 | Grad norm: 0.303228 | Time: 10s774ms
Epoch: 056 | Test Loss: 0.0072322 | Time: 796ms
==> Save the model at epoch 056 with test loss 0.0072322
Epoch: 057 | Train Loss: 0.0071903 | Grad norm: 0.297078 | Time: 10s893ms
Epoch: 057 | Test Loss: 0.0072472 | Time: 885ms
Epoch: 058 | Train Loss: 0.0071582 | Grad norm: 0.285596 | Time: 10s660ms
Epoch: 058 | Test Loss: 0.0072379 | Time: 834ms
Epoch: 059 | Train Loss: 0.0071298 | Grad norm: 0.250095 | Time: 11s421ms
Epoch: 059 | Test Loss: 0.0072024 | Time: 803ms
==> Save the model at epoch 059 with test loss 0.0072024
Epoch: 060 | Train Loss: 0.0071130 | Grad norm: 0.242792 | Time: 12s265ms
Epoch: 060 | Test Loss: 0.0071960 | Time: 824ms
==> Save the model at epoch 060 with test loss 0.0071960
Total time: 10m48s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
