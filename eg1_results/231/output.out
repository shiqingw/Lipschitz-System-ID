==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 17.5096416 | L2 loss: 17.4643373 | Lip loss: 0.0453042 | Grad norm: 5.822536 | Time: 21s51ms
Epoch: 001 | Test Loss: 6.7695658 | Time: 609ms
==> Save the model at epoch 001 with test loss 6.7695658
Epoch: 002 | Loss: 1.0563068 | L2 loss: 0.8409213 | Lip loss: 0.2153855 | Grad norm: 4.820218 | Time: 21s337ms
Epoch: 002 | Test Loss: 0.0132051 | Time: 535ms
==> Save the model at epoch 002 with test loss 0.0132051
Epoch: 003 | Loss: 0.2127999 | L2 loss: 0.0071152 | Lip loss: 0.2056847 | Grad norm: 1.958343 | Time: 20s968ms
Epoch: 003 | Test Loss: 0.0066825 | Time: 531ms
==> Save the model at epoch 003 with test loss 0.0066825
Epoch: 004 | Loss: 0.2111062 | L2 loss: 0.0064200 | Lip loss: 0.2046862 | Grad norm: 2.346915 | Time: 20s982ms
Epoch: 004 | Test Loss: 0.0101116 | Time: 603ms
Epoch: 005 | Loss: 0.2105348 | L2 loss: 0.0060975 | Lip loss: 0.2044374 | Grad norm: 2.354965 | Time: 20s667ms
Epoch: 005 | Test Loss: 0.0054645 | Time: 550ms
==> Save the model at epoch 005 with test loss 0.0054645
Epoch: 006 | Loss: 0.2083837 | L2 loss: 0.0043687 | Lip loss: 0.2040149 | Grad norm: 1.161248 | Time: 20s687ms
Epoch: 006 | Test Loss: 0.0042814 | Time: 536ms
==> Save the model at epoch 006 with test loss 0.0042814
Epoch: 007 | Loss: 0.2080522 | L2 loss: 0.0043355 | Lip loss: 0.2037167 | Grad norm: 1.127295 | Time: 20s538ms
Epoch: 007 | Test Loss: 0.0044150 | Time: 549ms
Epoch: 008 | Loss: 0.2077748 | L2 loss: 0.0043197 | Lip loss: 0.2034551 | Grad norm: 1.232707 | Time: 20s382ms
Epoch: 008 | Test Loss: 0.0044740 | Time: 550ms
Epoch: 009 | Loss: 0.2078259 | L2 loss: 0.0042330 | Lip loss: 0.2035928 | Grad norm: 1.211788 | Time: 20s227ms
Epoch: 009 | Test Loss: 0.0044298 | Time: 531ms
Epoch: 010 | Loss: 0.2073456 | L2 loss: 0.0041898 | Lip loss: 0.2031558 | Grad norm: 1.151405 | Time: 20s661ms
Epoch: 010 | Test Loss: 0.0040242 | Time: 539ms
==> Save the model at epoch 010 with test loss 0.0040242
Epoch: 011 | Loss: 0.2073023 | L2 loss: 0.0040297 | Lip loss: 0.2032726 | Grad norm: 1.009071 | Time: 20s423ms
Epoch: 011 | Test Loss: 0.0040585 | Time: 542ms
Epoch: 012 | Loss: 0.2074269 | L2 loss: 0.0040369 | Lip loss: 0.2033901 | Grad norm: 0.999492 | Time: 20s891ms
Epoch: 012 | Test Loss: 0.0040587 | Time: 529ms
Epoch: 013 | Loss: 0.2073316 | L2 loss: 0.0040236 | Lip loss: 0.2033080 | Grad norm: 0.994768 | Time: 20s409ms
Epoch: 013 | Test Loss: 0.0040719 | Time: 539ms
Epoch: 014 | Loss: 0.2073443 | L2 loss: 0.0040382 | Lip loss: 0.2033061 | Grad norm: 1.021578 | Time: 20s444ms
Epoch: 014 | Test Loss: 0.0040753 | Time: 603ms
Epoch: 015 | Loss: 0.2071391 | L2 loss: 0.0040170 | Lip loss: 0.2031221 | Grad norm: 0.982350 | Time: 20s110ms
Epoch: 015 | Test Loss: 0.0040358 | Time: 545ms
Epoch: 016 | Loss: 0.2072108 | L2 loss: 0.0040083 | Lip loss: 0.2032025 | Grad norm: 0.962766 | Time: 20s18ms
Epoch: 016 | Test Loss: 0.0040290 | Time: 535ms
Epoch: 017 | Loss: 0.2073102 | L2 loss: 0.0040016 | Lip loss: 0.2033086 | Grad norm: 0.963118 | Time: 20s418ms
Epoch: 017 | Test Loss: 0.0040250 | Time: 603ms
Epoch: 018 | Loss: 0.2069072 | L2 loss: 0.0039998 | Lip loss: 0.2029074 | Grad norm: 0.938755 | Time: 20s547ms
Epoch: 018 | Test Loss: 0.0040259 | Time: 538ms
Epoch: 019 | Loss: 0.2072567 | L2 loss: 0.0040040 | Lip loss: 0.2032527 | Grad norm: 0.931131 | Time: 20s577ms
Epoch: 019 | Test Loss: 0.0040268 | Time: 541ms
Epoch: 020 | Loss: 0.2074557 | L2 loss: 0.0040049 | Lip loss: 0.2034507 | Grad norm: 0.974575 | Time: 20s353ms
Epoch: 020 | Test Loss: 0.0040338 | Time: 538ms
Epoch: 021 | Loss: 0.2071985 | L2 loss: 0.0040074 | Lip loss: 0.2031911 | Grad norm: 0.989405 | Time: 20s684ms
Epoch: 021 | Test Loss: 0.0040337 | Time: 680ms
Epoch: 022 | Loss: 0.2074341 | L2 loss: 0.0040064 | Lip loss: 0.2034276 | Grad norm: 0.968426 | Time: 20s582ms
Epoch: 022 | Test Loss: 0.0040320 | Time: 645ms
Epoch: 023 | Loss: 0.2072922 | L2 loss: 0.0040048 | Lip loss: 0.2032874 | Grad norm: 0.925352 | Time: 20s743ms
Epoch: 023 | Test Loss: 0.0040314 | Time: 551ms
Epoch: 024 | Loss: 0.2076452 | L2 loss: 0.0040046 | Lip loss: 0.2036406 | Grad norm: 0.997009 | Time: 20s667ms
Epoch: 024 | Test Loss: 0.0040314 | Time: 562ms
Epoch: 025 | Loss: 0.2074090 | L2 loss: 0.0040049 | Lip loss: 0.2034041 | Grad norm: 0.996976 | Time: 20s472ms
Epoch: 025 | Test Loss: 0.0040321 | Time: 544ms
Epoch: 026 | Loss: 0.2072578 | L2 loss: 0.0040051 | Lip loss: 0.2032527 | Grad norm: 0.970807 | Time: 20s891ms
Epoch: 026 | Test Loss: 0.0040321 | Time: 537ms
Epoch: 027 | Loss: 0.2071557 | L2 loss: 0.0040053 | Lip loss: 0.2031504 | Grad norm: 0.962882 | Time: 20s808ms
Epoch: 027 | Test Loss: 0.0040321 | Time: 610ms
Epoch: 028 | Loss: 0.2072342 | L2 loss: 0.0040050 | Lip loss: 0.2032292 | Grad norm: 0.938539 | Time: 20s378ms
Epoch: 028 | Test Loss: 0.0040321 | Time: 531ms
Epoch: 029 | Loss: 0.2075133 | L2 loss: 0.0040052 | Lip loss: 0.2035081 | Grad norm: 0.979175 | Time: 20s552ms
Epoch: 029 | Test Loss: 0.0040321 | Time: 529ms
Epoch: 030 | Loss: 0.2074297 | L2 loss: 0.0040053 | Lip loss: 0.2034244 | Grad norm: 0.957585 | Time: 20s669ms
Epoch: 030 | Test Loss: 0.0040322 | Time: 614ms
Epoch: 031 | Loss: 0.2072432 | L2 loss: 0.0040049 | Lip loss: 0.2032384 | Grad norm: 0.936385 | Time: 20s663ms
Epoch: 031 | Test Loss: 0.0040322 | Time: 541ms
Epoch: 032 | Loss: 0.2071034 | L2 loss: 0.0040052 | Lip loss: 0.2030983 | Grad norm: 0.939386 | Time: 20s670ms
Epoch: 032 | Test Loss: 0.0040322 | Time: 536ms
Epoch: 033 | Loss: 0.2075441 | L2 loss: 0.0040052 | Lip loss: 0.2035388 | Grad norm: 0.960168 | Time: 20s581ms
Epoch: 033 | Test Loss: 0.0040322 | Time: 537ms
Epoch: 034 | Loss: 0.2073559 | L2 loss: 0.0040052 | Lip loss: 0.2033507 | Grad norm: 0.967006 | Time: 20s613ms
Epoch: 034 | Test Loss: 0.0040322 | Time: 529ms
Epoch: 035 | Loss: 0.2073123 | L2 loss: 0.0040052 | Lip loss: 0.2033071 | Grad norm: 0.996997 | Time: 20s493ms
Epoch: 035 | Test Loss: 0.0040322 | Time: 530ms
Epoch: 036 | Loss: 0.2074892 | L2 loss: 0.0040051 | Lip loss: 0.2034841 | Grad norm: 0.999451 | Time: 20s786ms
Epoch: 036 | Test Loss: 0.0040322 | Time: 536ms
Epoch: 037 | Loss: 0.2070215 | L2 loss: 0.0040050 | Lip loss: 0.2030165 | Grad norm: 0.974092 | Time: 20s887ms
Epoch: 037 | Test Loss: 0.0040322 | Time: 542ms
Epoch: 038 | Loss: 0.2070935 | L2 loss: 0.0040051 | Lip loss: 0.2030884 | Grad norm: 0.969664 | Time: 20s588ms
Epoch: 038 | Test Loss: 0.0040322 | Time: 530ms
Epoch: 039 | Loss: 0.2071384 | L2 loss: 0.0040050 | Lip loss: 0.2031333 | Grad norm: 0.952255 | Time: 20s989ms
Epoch: 039 | Test Loss: 0.0040322 | Time: 531ms
Epoch: 040 | Loss: 0.2074070 | L2 loss: 0.0040051 | Lip loss: 0.2034020 | Grad norm: 0.968152 | Time: 21s667ms
Epoch: 040 | Test Loss: 0.0040322 | Time: 534ms
Total time: 14m8s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
