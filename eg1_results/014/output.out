==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Lipschitz constant: 1.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (M): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.8164900 | Grad norm: 8.500493 | Time: 19s550ms
Epoch: 001 | Test Loss: 31.1807396 | Time: 814ms
==> Save the model at epoch 001 with test loss 31.1807396
Epoch: 002 | Train Loss: 6.7701091 | Grad norm: 11.720938 | Time: 20s731ms
Epoch: 002 | Test Loss: 2.8457257 | Time: 809ms
==> Save the model at epoch 002 with test loss 2.8457257
Epoch: 003 | Train Loss: 1.4195724 | Grad norm: 5.876523 | Time: 18s808ms
Epoch: 003 | Test Loss: 0.6238393 | Time: 807ms
==> Save the model at epoch 003 with test loss 0.6238393
Epoch: 004 | Train Loss: 0.4371474 | Grad norm: 12.244146 | Time: 19s101ms
Epoch: 004 | Test Loss: 0.3178959 | Time: 781ms
==> Save the model at epoch 004 with test loss 0.3178959
Epoch: 005 | Train Loss: 0.2247575 | Grad norm: 13.078894 | Time: 21s139ms
Epoch: 005 | Test Loss: 0.1834529 | Time: 818ms
==> Save the model at epoch 005 with test loss 0.1834529
Epoch: 006 | Train Loss: 0.1385356 | Grad norm: 13.796775 | Time: 16s791ms
Epoch: 006 | Test Loss: 0.1478509 | Time: 777ms
==> Save the model at epoch 006 with test loss 0.1478509
Epoch: 007 | Train Loss: 0.1039782 | Grad norm: 14.007813 | Time: 16s767ms
Epoch: 007 | Test Loss: 0.1181347 | Time: 775ms
==> Save the model at epoch 007 with test loss 0.1181347
Epoch: 008 | Train Loss: 0.0915685 | Grad norm: 13.829432 | Time: 17s406ms
Epoch: 008 | Test Loss: 0.1544452 | Time: 781ms
Epoch: 009 | Train Loss: 0.0845706 | Grad norm: 13.281225 | Time: 17s392ms
Epoch: 009 | Test Loss: 0.0870930 | Time: 811ms
==> Save the model at epoch 009 with test loss 0.0870930
Epoch: 010 | Train Loss: 0.0685130 | Grad norm: 11.363990 | Time: 17s194ms
Epoch: 010 | Test Loss: 0.1429908 | Time: 821ms
Epoch: 011 | Train Loss: 0.0582182 | Grad norm: 9.826625 | Time: 21s254ms
Epoch: 011 | Test Loss: 0.0404601 | Time: 824ms
==> Save the model at epoch 011 with test loss 0.0404601
Epoch: 012 | Train Loss: 0.0509692 | Grad norm: 8.663463 | Time: 21s584ms
Epoch: 012 | Test Loss: 0.1000379 | Time: 793ms
Epoch: 013 | Train Loss: 0.0593390 | Grad norm: 9.397866 | Time: 17s663ms
Epoch: 013 | Test Loss: 0.0608252 | Time: 778ms
Epoch: 014 | Train Loss: 0.0710326 | Grad norm: 10.507615 | Time: 17s576ms
Epoch: 014 | Test Loss: 0.0406857 | Time: 814ms
Epoch: 015 | Train Loss: 0.0282227 | Grad norm: 4.961498 | Time: 17s446ms
Epoch: 015 | Test Loss: 0.0130885 | Time: 809ms
==> Save the model at epoch 015 with test loss 0.0130885
Epoch: 016 | Train Loss: 0.0124281 | Grad norm: 2.973113 | Time: 17s625ms
Epoch: 016 | Test Loss: 0.0093437 | Time: 818ms
==> Save the model at epoch 016 with test loss 0.0093437
Epoch: 017 | Train Loss: 0.0115755 | Grad norm: 2.772952 | Time: 17s661ms
Epoch: 017 | Test Loss: 0.0105712 | Time: 916ms
Epoch: 018 | Train Loss: 0.0122441 | Grad norm: 2.803789 | Time: 17s667ms
Epoch: 018 | Test Loss: 0.0279462 | Time: 816ms
Epoch: 019 | Train Loss: 0.0185188 | Grad norm: 4.195186 | Time: 17s149ms
Epoch: 019 | Test Loss: 0.0237236 | Time: 812ms
Epoch: 020 | Train Loss: 0.0292288 | Grad norm: 5.467563 | Time: 21s280ms
Epoch: 020 | Test Loss: 0.0130281 | Time: 813ms
Epoch: 021 | Train Loss: 0.0266633 | Grad norm: 5.287717 | Time: 20s111ms
Epoch: 021 | Test Loss: 0.0149006 | Time: 810ms
Epoch: 022 | Train Loss: 0.0187036 | Grad norm: 3.785354 | Time: 22s319ms
Epoch: 022 | Test Loss: 0.0128556 | Time: 807ms
Epoch: 023 | Train Loss: 0.0095763 | Grad norm: 1.569593 | Time: 20s282ms
Epoch: 023 | Test Loss: 0.0126070 | Time: 809ms
Epoch: 024 | Train Loss: 0.0081713 | Grad norm: 0.984635 | Time: 17s805ms
Epoch: 024 | Test Loss: 0.0077318 | Time: 783ms
==> Save the model at epoch 024 with test loss 0.0077318
Epoch: 025 | Train Loss: 0.0087306 | Grad norm: 1.245221 | Time: 17s563ms
Epoch: 025 | Test Loss: 0.0078748 | Time: 776ms
Epoch: 026 | Train Loss: 0.0079552 | Grad norm: 0.923262 | Time: 17s748ms
Epoch: 026 | Test Loss: 0.0080145 | Time: 790ms
Epoch: 027 | Train Loss: 0.0075491 | Grad norm: 0.661863 | Time: 18s448ms
Epoch: 027 | Test Loss: 0.0076198 | Time: 777ms
==> Save the model at epoch 027 with test loss 0.0076198
Epoch: 028 | Train Loss: 0.0075288 | Grad norm: 0.640572 | Time: 17s530ms
Epoch: 028 | Test Loss: 0.0077979 | Time: 777ms
Epoch: 029 | Train Loss: 0.0076653 | Grad norm: 0.709779 | Time: 17s814ms
Epoch: 029 | Test Loss: 0.0079325 | Time: 775ms
Epoch: 030 | Train Loss: 0.0073230 | Grad norm: 0.441598 | Time: 18s423ms
Epoch: 030 | Test Loss: 0.0072643 | Time: 795ms
==> Save the model at epoch 030 with test loss 0.0072643
Epoch: 031 | Train Loss: 0.0074218 | Grad norm: 0.522594 | Time: 20s440ms
Epoch: 031 | Test Loss: 0.0073330 | Time: 865ms
Epoch: 032 | Train Loss: 0.0073636 | Grad norm: 0.489145 | Time: 18s46ms
Epoch: 032 | Test Loss: 0.0072032 | Time: 809ms
==> Save the model at epoch 032 with test loss 0.0072032
Epoch: 033 | Train Loss: 0.0073273 | Grad norm: 0.438298 | Time: 19s911ms
Epoch: 033 | Test Loss: 0.0071635 | Time: 814ms
==> Save the model at epoch 033 with test loss 0.0071635
Epoch: 034 | Train Loss: 0.0072640 | Grad norm: 0.350127 | Time: 19s461ms
Epoch: 034 | Test Loss: 0.0073541 | Time: 838ms
Epoch: 035 | Train Loss: 0.0072273 | Grad norm: 0.298108 | Time: 17s385ms
Epoch: 035 | Test Loss: 0.0072121 | Time: 778ms
Epoch: 036 | Train Loss: 0.0072007 | Grad norm: 0.255503 | Time: 17s794ms
Epoch: 036 | Test Loss: 0.0072142 | Time: 790ms
Epoch: 037 | Train Loss: 0.0071909 | Grad norm: 0.233568 | Time: 20s544ms
Epoch: 037 | Test Loss: 0.0071776 | Time: 871ms
Epoch: 038 | Train Loss: 0.0071781 | Grad norm: 0.206788 | Time: 21s731ms
Epoch: 038 | Test Loss: 0.0073441 | Time: 825ms
Epoch: 039 | Train Loss: 0.0071651 | Grad norm: 0.171718 | Time: 21s633ms
Epoch: 039 | Test Loss: 0.0071542 | Time: 782ms
==> Save the model at epoch 039 with test loss 0.0071542
Epoch: 040 | Train Loss: 0.0071531 | Grad norm: 0.126861 | Time: 17s791ms
Epoch: 040 | Test Loss: 0.0071543 | Time: 879ms
Total time: 13m6s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
