==> torch device:  cuda:0
==> Lipschitz constant: 1.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 128]                  16,897
│    └─SandwichFc: 2-2                   [1, 128]                  33,025
│    └─SandwichFc: 2-3                   [1, 128]                  33,025
│    └─SandwichFc: 2-4                   [1, 128]                  33,025
│    └─SandwichFc: 2-5                   [1, 128]                  33,025
│    └─SandwichFc: 2-6                   [1, 128]                  33,025
│    └─SandwichFc: 2-7                   [1, 128]                  33,025
│    └─SandwichLin: 2-8                  [1, 2]                    263
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 128]                  (recursive)
│    └─SandwichFc: 2-10                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-11                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-12                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-13                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-14                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-15                  [1, 128]                  (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 215,316
Trainable params: 215,310
Non-trainable params: 6
Total mult-adds (M): 0.43
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.50
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 31.0125592 | Grad norm: 7.669208 | Time: 11s131ms
Epoch: 001 | Test Loss: 31.0111956 | Time: 769ms
==> Save the model at epoch 001 with test loss 31.0111956
Epoch: 002 | Train Loss: 9.4713114 | Grad norm: 15.145477 | Time: 10s116ms
Epoch: 002 | Test Loss: 4.9573449 | Time: 766ms
==> Save the model at epoch 002 with test loss 4.9573449
Epoch: 003 | Train Loss: 3.5618533 | Grad norm: 9.999773 | Time: 10s910ms
Epoch: 003 | Test Loss: 2.4741597 | Time: 803ms
==> Save the model at epoch 003 with test loss 2.4741597
Epoch: 004 | Train Loss: 1.6834465 | Grad norm: 10.842111 | Time: 10s693ms
Epoch: 004 | Test Loss: 1.1429809 | Time: 795ms
==> Save the model at epoch 004 with test loss 1.1429809
Epoch: 005 | Train Loss: 0.8342233 | Grad norm: 12.354911 | Time: 9s882ms
Epoch: 005 | Test Loss: 0.5934658 | Time: 769ms
==> Save the model at epoch 005 with test loss 0.5934658
Epoch: 006 | Train Loss: 0.4492622 | Grad norm: 13.321395 | Time: 9s724ms
Epoch: 006 | Test Loss: 0.3388871 | Time: 772ms
==> Save the model at epoch 006 with test loss 0.3388871
Epoch: 007 | Train Loss: 0.2703427 | Grad norm: 13.907471 | Time: 10s625ms
Epoch: 007 | Test Loss: 0.2203596 | Time: 758ms
==> Save the model at epoch 007 with test loss 0.2203596
Epoch: 008 | Train Loss: 0.1852008 | Grad norm: 14.155236 | Time: 8s614ms
Epoch: 008 | Test Loss: 0.1525941 | Time: 766ms
==> Save the model at epoch 008 with test loss 0.1525941
Epoch: 009 | Train Loss: 0.1416480 | Grad norm: 14.026410 | Time: 9s306ms
Epoch: 009 | Test Loss: 0.1303308 | Time: 774ms
==> Save the model at epoch 009 with test loss 0.1303308
Epoch: 010 | Train Loss: 0.1159613 | Grad norm: 13.583163 | Time: 11s294ms
Epoch: 010 | Test Loss: 0.1122572 | Time: 786ms
==> Save the model at epoch 010 with test loss 0.1122572
Epoch: 011 | Train Loss: 0.0995092 | Grad norm: 13.017506 | Time: 10s963ms
Epoch: 011 | Test Loss: 0.0960858 | Time: 792ms
==> Save the model at epoch 011 with test loss 0.0960858
Epoch: 012 | Train Loss: 0.0779402 | Grad norm: 11.386600 | Time: 8s717ms
Epoch: 012 | Test Loss: 0.0785078 | Time: 791ms
==> Save the model at epoch 012 with test loss 0.0785078
Epoch: 013 | Train Loss: 0.0635142 | Grad norm: 10.180560 | Time: 8s750ms
Epoch: 013 | Test Loss: 0.0617212 | Time: 765ms
==> Save the model at epoch 013 with test loss 0.0617212
Epoch: 014 | Train Loss: 0.0527452 | Grad norm: 9.167121 | Time: 8s617ms
Epoch: 014 | Test Loss: 0.0506590 | Time: 756ms
==> Save the model at epoch 014 with test loss 0.0506590
Epoch: 015 | Train Loss: 0.0446744 | Grad norm: 8.315511 | Time: 8s631ms
Epoch: 015 | Test Loss: 0.0424667 | Time: 762ms
==> Save the model at epoch 015 with test loss 0.0424667
Epoch: 016 | Train Loss: 0.0386955 | Grad norm: 7.622413 | Time: 10s466ms
Epoch: 016 | Test Loss: 0.0364798 | Time: 766ms
==> Save the model at epoch 016 with test loss 0.0364798
Epoch: 017 | Train Loss: 0.0340182 | Grad norm: 7.030414 | Time: 10s319ms
Epoch: 017 | Test Loss: 0.0344159 | Time: 853ms
==> Save the model at epoch 017 with test loss 0.0344159
Epoch: 018 | Train Loss: 0.0302182 | Grad norm: 6.500475 | Time: 10s800ms
Epoch: 018 | Test Loss: 0.0298922 | Time: 761ms
==> Save the model at epoch 018 with test loss 0.0298922
Epoch: 019 | Train Loss: 0.0273565 | Grad norm: 6.063201 | Time: 11s496ms
Epoch: 019 | Test Loss: 0.0258137 | Time: 773ms
==> Save the model at epoch 019 with test loss 0.0258137
Epoch: 020 | Train Loss: 0.0249499 | Grad norm: 5.663157 | Time: 10s913ms
Epoch: 020 | Test Loss: 0.0226201 | Time: 760ms
==> Save the model at epoch 020 with test loss 0.0226201
Epoch: 021 | Train Loss: 0.0226944 | Grad norm: 5.252962 | Time: 10s719ms
Epoch: 021 | Test Loss: 0.0195179 | Time: 896ms
==> Save the model at epoch 021 with test loss 0.0195179
Epoch: 022 | Train Loss: 0.0210578 | Grad norm: 4.930614 | Time: 10s503ms
Epoch: 022 | Test Loss: 0.0176732 | Time: 822ms
==> Save the model at epoch 022 with test loss 0.0176732
Epoch: 023 | Train Loss: 0.0193083 | Grad norm: 4.593442 | Time: 8s604ms
Epoch: 023 | Test Loss: 0.0169032 | Time: 760ms
==> Save the model at epoch 023 with test loss 0.0169032
Epoch: 024 | Train Loss: 0.0180207 | Grad norm: 4.311260 | Time: 9s747ms
Epoch: 024 | Test Loss: 0.0150034 | Time: 836ms
==> Save the model at epoch 024 with test loss 0.0150034
Epoch: 025 | Train Loss: 0.0168593 | Grad norm: 4.038357 | Time: 10s106ms
Epoch: 025 | Test Loss: 0.0137935 | Time: 773ms
==> Save the model at epoch 025 with test loss 0.0137935
Epoch: 026 | Train Loss: 0.0159007 | Grad norm: 3.796086 | Time: 10s54ms
Epoch: 026 | Test Loss: 0.0127553 | Time: 798ms
==> Save the model at epoch 026 with test loss 0.0127553
Epoch: 027 | Train Loss: 0.0148816 | Grad norm: 3.532000 | Time: 9s128ms
Epoch: 027 | Test Loss: 0.0117649 | Time: 820ms
==> Save the model at epoch 027 with test loss 0.0117649
Epoch: 028 | Train Loss: 0.0140967 | Grad norm: 3.334236 | Time: 8s631ms
Epoch: 028 | Test Loss: 0.0122395 | Time: 758ms
Epoch: 029 | Train Loss: 0.0132482 | Grad norm: 3.142460 | Time: 8s552ms
Epoch: 029 | Test Loss: 0.0111487 | Time: 757ms
==> Save the model at epoch 029 with test loss 0.0111487
Epoch: 030 | Train Loss: 0.0124754 | Grad norm: 2.949491 | Time: 9s13ms
Epoch: 030 | Test Loss: 0.0117097 | Time: 762ms
Epoch: 031 | Train Loss: 0.0118132 | Grad norm: 2.744751 | Time: 8s549ms
Epoch: 031 | Test Loss: 0.0107282 | Time: 759ms
==> Save the model at epoch 031 with test loss 0.0107282
Epoch: 032 | Train Loss: 0.0113268 | Grad norm: 2.577633 | Time: 8s524ms
Epoch: 032 | Test Loss: 0.0101002 | Time: 822ms
==> Save the model at epoch 032 with test loss 0.0101002
Epoch: 033 | Train Loss: 0.0108650 | Grad norm: 2.412229 | Time: 8s541ms
Epoch: 033 | Test Loss: 0.0096905 | Time: 758ms
==> Save the model at epoch 033 with test loss 0.0096905
Epoch: 034 | Train Loss: 0.0104552 | Grad norm: 2.274123 | Time: 8s555ms
Epoch: 034 | Test Loss: 0.0098992 | Time: 757ms
Epoch: 035 | Train Loss: 0.0100751 | Grad norm: 2.122211 | Time: 8s583ms
Epoch: 035 | Test Loss: 0.0111680 | Time: 761ms
Epoch: 036 | Train Loss: 0.0096528 | Grad norm: 1.951093 | Time: 8s532ms
Epoch: 036 | Test Loss: 0.0103716 | Time: 760ms
Epoch: 037 | Train Loss: 0.0092705 | Grad norm: 1.805680 | Time: 8s606ms
Epoch: 037 | Test Loss: 0.0088789 | Time: 858ms
==> Save the model at epoch 037 with test loss 0.0088789
Epoch: 038 | Train Loss: 0.0089697 | Grad norm: 1.662083 | Time: 8s560ms
Epoch: 038 | Test Loss: 0.0086841 | Time: 759ms
==> Save the model at epoch 038 with test loss 0.0086841
Epoch: 039 | Train Loss: 0.0093260 | Grad norm: 1.550932 | Time: 8s563ms
Epoch: 039 | Test Loss: 0.0129897 | Time: 760ms
Epoch: 040 | Train Loss: 0.0090561 | Grad norm: 1.352811 | Time: 10s175ms
Epoch: 040 | Test Loss: 0.0078895 | Time: 765ms
==> Save the model at epoch 040 with test loss 0.0078895
Epoch: 041 | Train Loss: 0.0081324 | Grad norm: 1.045180 | Time: 11s389ms
Epoch: 041 | Test Loss: 0.0072565 | Time: 760ms
==> Save the model at epoch 041 with test loss 0.0072565
Epoch: 042 | Train Loss: 0.0083605 | Grad norm: 1.024803 | Time: 11s958ms
Epoch: 042 | Test Loss: 0.0080018 | Time: 831ms
Epoch: 043 | Train Loss: 0.0080404 | Grad norm: 0.952439 | Time: 10s56ms
Epoch: 043 | Test Loss: 0.0110272 | Time: 774ms
Epoch: 044 | Train Loss: 0.0081052 | Grad norm: 0.875809 | Time: 8s961ms
Epoch: 044 | Test Loss: 0.0081045 | Time: 758ms
Epoch: 045 | Train Loss: 0.0075145 | Grad norm: 0.536151 | Time: 8s700ms
Epoch: 045 | Test Loss: 0.0072514 | Time: 793ms
==> Save the model at epoch 045 with test loss 0.0072514
Epoch: 046 | Train Loss: 0.0075022 | Grad norm: 0.531641 | Time: 8s525ms
Epoch: 046 | Test Loss: 0.0075093 | Time: 762ms
Epoch: 047 | Train Loss: 0.0075130 | Grad norm: 0.581473 | Time: 8s653ms
Epoch: 047 | Test Loss: 0.0072459 | Time: 856ms
==> Save the model at epoch 047 with test loss 0.0072459
Epoch: 048 | Train Loss: 0.0074264 | Grad norm: 0.451781 | Time: 10s308ms
Epoch: 048 | Test Loss: 0.0072716 | Time: 766ms
Epoch: 049 | Train Loss: 0.0074256 | Grad norm: 0.474658 | Time: 9s334ms
Epoch: 049 | Test Loss: 0.0073759 | Time: 763ms
Epoch: 050 | Train Loss: 0.0073416 | Grad norm: 0.359262 | Time: 8s591ms
Epoch: 050 | Test Loss: 0.0076873 | Time: 762ms
Epoch: 051 | Train Loss: 0.0073566 | Grad norm: 0.388491 | Time: 8s538ms
Epoch: 051 | Test Loss: 0.0072551 | Time: 758ms
Epoch: 052 | Train Loss: 0.0073736 | Grad norm: 0.412047 | Time: 8s787ms
Epoch: 052 | Test Loss: 0.0072719 | Time: 860ms
Epoch: 053 | Train Loss: 0.0073558 | Grad norm: 0.397393 | Time: 8s531ms
Epoch: 053 | Test Loss: 0.0073478 | Time: 761ms
Epoch: 054 | Train Loss: 0.0072867 | Grad norm: 0.303795 | Time: 8s546ms
Epoch: 054 | Test Loss: 0.0072358 | Time: 758ms
==> Save the model at epoch 054 with test loss 0.0072358
Epoch: 055 | Train Loss: 0.0072831 | Grad norm: 0.264998 | Time: 8s604ms
Epoch: 055 | Test Loss: 0.0072377 | Time: 761ms
Epoch: 056 | Train Loss: 0.0072410 | Grad norm: 0.199544 | Time: 8s582ms
Epoch: 056 | Test Loss: 0.0072349 | Time: 763ms
==> Save the model at epoch 056 with test loss 0.0072349
Epoch: 057 | Train Loss: 0.0072470 | Grad norm: 0.195732 | Time: 9s733ms
Epoch: 057 | Test Loss: 0.0072528 | Time: 905ms
Epoch: 058 | Train Loss: 0.0072403 | Grad norm: 0.203989 | Time: 10s225ms
Epoch: 058 | Test Loss: 0.0072600 | Time: 794ms
Epoch: 059 | Train Loss: 0.0072262 | Grad norm: 0.156304 | Time: 10s577ms
Epoch: 059 | Test Loss: 0.0072409 | Time: 762ms
Epoch: 060 | Train Loss: 0.0072220 | Grad norm: 0.150827 | Time: 10s120ms
Epoch: 060 | Test Loss: 0.0072402 | Time: 763ms
Total time: 10m20s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
