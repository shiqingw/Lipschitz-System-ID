==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 20.5496902 | L2 loss: 20.5462485 | Lip loss: 0.0034417 | Grad norm: 1.590979 | Time: 9s866ms
Epoch: 001 | Test Loss: 19.3991659 | Time: 527ms
==> Save the model at epoch 001 with test loss 19.3991659
Epoch: 002 | Loss: 14.4407426 | L2 loss: 14.3503568 | Lip loss: 0.0903858 | Grad norm: 9.975193 | Time: 9s944ms
Epoch: 002 | Test Loss: 6.7784524 | Time: 523ms
==> Save the model at epoch 002 with test loss 6.7784524
Epoch: 003 | Loss: 1.8565682 | L2 loss: 1.6337522 | Lip loss: 0.2228160 | Grad norm: 7.335602 | Time: 10s383ms
Epoch: 003 | Test Loss: 0.0193890 | Time: 527ms
==> Save the model at epoch 003 with test loss 0.0193890
Epoch: 004 | Loss: 0.2230681 | L2 loss: 0.0123619 | Lip loss: 0.2107061 | Grad norm: 2.478841 | Time: 10s154ms
Epoch: 004 | Test Loss: 0.0089196 | Time: 595ms
==> Save the model at epoch 004 with test loss 0.0089196
Epoch: 005 | Loss: 0.2143503 | L2 loss: 0.0077071 | Lip loss: 0.2066433 | Grad norm: 2.088845 | Time: 9s815ms
Epoch: 005 | Test Loss: 0.0086005 | Time: 521ms
==> Save the model at epoch 005 with test loss 0.0086005
Epoch: 006 | Loss: 0.2116695 | L2 loss: 0.0058675 | Lip loss: 0.2058020 | Grad norm: 1.209676 | Time: 9s799ms
Epoch: 006 | Test Loss: 0.0058284 | Time: 518ms
==> Save the model at epoch 006 with test loss 0.0058284
Epoch: 007 | Loss: 0.2112497 | L2 loss: 0.0056449 | Lip loss: 0.2056048 | Grad norm: 1.141864 | Time: 9s904ms
Epoch: 007 | Test Loss: 0.0055684 | Time: 517ms
==> Save the model at epoch 007 with test loss 0.0055684
Epoch: 008 | Loss: 0.2116934 | L2 loss: 0.0054399 | Lip loss: 0.2062535 | Grad norm: 1.102935 | Time: 9s771ms
Epoch: 008 | Test Loss: 0.0053590 | Time: 521ms
==> Save the model at epoch 008 with test loss 0.0053590
Epoch: 009 | Loss: 0.2108026 | L2 loss: 0.0054840 | Lip loss: 0.2053186 | Grad norm: 1.242007 | Time: 9s943ms
Epoch: 009 | Test Loss: 0.0054537 | Time: 524ms
Epoch: 010 | Loss: 0.2105536 | L2 loss: 0.0053176 | Lip loss: 0.2052360 | Grad norm: 1.133087 | Time: 9s813ms
Epoch: 010 | Test Loss: 0.0052495 | Time: 521ms
==> Save the model at epoch 010 with test loss 0.0052495
Epoch: 011 | Loss: 0.2099899 | L2 loss: 0.0051265 | Lip loss: 0.2048635 | Grad norm: 1.020650 | Time: 9s803ms
Epoch: 011 | Test Loss: 0.0051275 | Time: 595ms
==> Save the model at epoch 011 with test loss 0.0051275
Epoch: 012 | Loss: 0.2106787 | L2 loss: 0.0051269 | Lip loss: 0.2055518 | Grad norm: 0.952774 | Time: 9s849ms
Epoch: 012 | Test Loss: 0.0051697 | Time: 526ms
Epoch: 013 | Loss: 0.2101499 | L2 loss: 0.0051539 | Lip loss: 0.2049960 | Grad norm: 0.980480 | Time: 9s902ms
Epoch: 013 | Test Loss: 0.0051706 | Time: 523ms
Epoch: 014 | Loss: 0.2099124 | L2 loss: 0.0051369 | Lip loss: 0.2047755 | Grad norm: 1.055248 | Time: 9s742ms
Epoch: 014 | Test Loss: 0.0051544 | Time: 519ms
Epoch: 015 | Loss: 0.2104121 | L2 loss: 0.0051294 | Lip loss: 0.2052828 | Grad norm: 0.955339 | Time: 9s802ms
Epoch: 015 | Test Loss: 0.0051341 | Time: 531ms
Epoch: 016 | Loss: 0.2095526 | L2 loss: 0.0051141 | Lip loss: 0.2044385 | Grad norm: 0.993024 | Time: 9s866ms
Epoch: 016 | Test Loss: 0.0051233 | Time: 548ms
==> Save the model at epoch 016 with test loss 0.0051233
Epoch: 017 | Loss: 0.2100690 | L2 loss: 0.0051079 | Lip loss: 0.2049611 | Grad norm: 0.983176 | Time: 9s824ms
Epoch: 017 | Test Loss: 0.0051222 | Time: 522ms
==> Save the model at epoch 017 with test loss 0.0051222
Epoch: 018 | Loss: 0.2108474 | L2 loss: 0.0051120 | Lip loss: 0.2057354 | Grad norm: 1.002115 | Time: 9s728ms
Epoch: 018 | Test Loss: 0.0051232 | Time: 519ms
Epoch: 019 | Loss: 0.2101935 | L2 loss: 0.0051091 | Lip loss: 0.2050844 | Grad norm: 0.985098 | Time: 9s886ms
Epoch: 019 | Test Loss: 0.0051165 | Time: 519ms
==> Save the model at epoch 019 with test loss 0.0051165
Epoch: 020 | Loss: 0.2102952 | L2 loss: 0.0050966 | Lip loss: 0.2051986 | Grad norm: 0.975621 | Time: 9s702ms
Epoch: 020 | Test Loss: 0.0051153 | Time: 597ms
==> Save the model at epoch 020 with test loss 0.0051153
Epoch: 021 | Loss: 0.2101424 | L2 loss: 0.0051045 | Lip loss: 0.2050379 | Grad norm: 0.960320 | Time: 9s721ms
Epoch: 021 | Test Loss: 0.0051146 | Time: 523ms
==> Save the model at epoch 021 with test loss 0.0051146
Epoch: 022 | Loss: 0.2098992 | L2 loss: 0.0051010 | Lip loss: 0.2047982 | Grad norm: 1.009192 | Time: 10s4ms
Epoch: 022 | Test Loss: 0.0051145 | Time: 527ms
==> Save the model at epoch 022 with test loss 0.0051145
Epoch: 023 | Loss: 0.2105934 | L2 loss: 0.0051015 | Lip loss: 0.2054919 | Grad norm: 0.978376 | Time: 9s854ms
Epoch: 023 | Test Loss: 0.0051143 | Time: 528ms
==> Save the model at epoch 023 with test loss 0.0051143
Epoch: 024 | Loss: 0.2101125 | L2 loss: 0.0050994 | Lip loss: 0.2050131 | Grad norm: 0.963131 | Time: 9s822ms
Epoch: 024 | Test Loss: 0.0051142 | Time: 524ms
==> Save the model at epoch 024 with test loss 0.0051142
Epoch: 025 | Loss: 0.2106200 | L2 loss: 0.0050977 | Lip loss: 0.2055224 | Grad norm: 1.000211 | Time: 9s690ms
Epoch: 025 | Test Loss: 0.0051143 | Time: 521ms
Epoch: 026 | Loss: 0.2108093 | L2 loss: 0.0051025 | Lip loss: 0.2057068 | Grad norm: 0.949428 | Time: 9s702ms
Epoch: 026 | Test Loss: 0.0051143 | Time: 527ms
Epoch: 027 | Loss: 0.2105844 | L2 loss: 0.0051021 | Lip loss: 0.2054823 | Grad norm: 0.930721 | Time: 9s799ms
Epoch: 027 | Test Loss: 0.0051143 | Time: 524ms
Epoch: 028 | Loss: 0.2097570 | L2 loss: 0.0050957 | Lip loss: 0.2046613 | Grad norm: 0.944606 | Time: 9s757ms
Epoch: 028 | Test Loss: 0.0051143 | Time: 532ms
Epoch: 029 | Loss: 0.2101800 | L2 loss: 0.0050957 | Lip loss: 0.2050843 | Grad norm: 0.954729 | Time: 9s733ms
Epoch: 029 | Test Loss: 0.0051143 | Time: 593ms
Epoch: 030 | Loss: 0.2106410 | L2 loss: 0.0051013 | Lip loss: 0.2055397 | Grad norm: 0.974490 | Time: 9s828ms
Epoch: 030 | Test Loss: 0.0051143 | Time: 524ms
Epoch: 031 | Loss: 0.2103822 | L2 loss: 0.0051000 | Lip loss: 0.2052822 | Grad norm: 0.982356 | Time: 9s928ms
Epoch: 031 | Test Loss: 0.0051143 | Time: 526ms
Epoch: 032 | Loss: 0.2100261 | L2 loss: 0.0050998 | Lip loss: 0.2049263 | Grad norm: 0.940759 | Time: 10s50ms
Epoch: 032 | Test Loss: 0.0051143 | Time: 546ms
Epoch: 033 | Loss: 0.2100563 | L2 loss: 0.0050993 | Lip loss: 0.2049570 | Grad norm: 0.968720 | Time: 9s747ms
Epoch: 033 | Test Loss: 0.0051143 | Time: 596ms
Epoch: 034 | Loss: 0.2105695 | L2 loss: 0.0050970 | Lip loss: 0.2054725 | Grad norm: 0.992521 | Time: 9s979ms
Epoch: 034 | Test Loss: 0.0051143 | Time: 537ms
Epoch: 035 | Loss: 0.2102989 | L2 loss: 0.0051025 | Lip loss: 0.2051964 | Grad norm: 0.996658 | Time: 9s802ms
Epoch: 035 | Test Loss: 0.0051143 | Time: 520ms
Epoch: 036 | Loss: 0.2105570 | L2 loss: 0.0051015 | Lip loss: 0.2054555 | Grad norm: 0.962146 | Time: 9s754ms
Epoch: 036 | Test Loss: 0.0051143 | Time: 523ms
Epoch: 037 | Loss: 0.2103549 | L2 loss: 0.0051001 | Lip loss: 0.2052548 | Grad norm: 0.957647 | Time: 9s846ms
Epoch: 037 | Test Loss: 0.0051143 | Time: 529ms
Epoch: 038 | Loss: 0.2105212 | L2 loss: 0.0050970 | Lip loss: 0.2054242 | Grad norm: 0.984901 | Time: 10s218ms
Epoch: 038 | Test Loss: 0.0051143 | Time: 625ms
Epoch: 039 | Loss: 0.2101039 | L2 loss: 0.0051005 | Lip loss: 0.2050034 | Grad norm: 0.881125 | Time: 10s609ms
Epoch: 039 | Test Loss: 0.0051143 | Time: 533ms
Epoch: 040 | Loss: 0.2103655 | L2 loss: 0.0051060 | Lip loss: 0.2052595 | Grad norm: 0.968479 | Time: 10s220ms
Epoch: 040 | Test Loss: 0.0051143 | Time: 540ms
Total time: 6m57s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
