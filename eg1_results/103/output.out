==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Lipschitz constant: 0.50
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (M): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 31.0321065 | Grad norm: 5.317039 | Time: 20s900ms
Epoch: 001 | Test Loss: 30.7557544 | Time: 803ms
==> Save the model at epoch 001 with test loss 30.7557544
Epoch: 002 | Train Loss: 15.9616047 | Grad norm: 9.992002 | Time: 17s789ms
Epoch: 002 | Test Loss: 12.7463564 | Time: 790ms
==> Save the model at epoch 002 with test loss 12.7463564
Epoch: 003 | Train Loss: 11.0635813 | Grad norm: 7.143662 | Time: 17s635ms
Epoch: 003 | Test Loss: 9.5925215 | Time: 792ms
==> Save the model at epoch 003 with test loss 9.5925215
Epoch: 004 | Train Loss: 8.7240415 | Grad norm: 4.457422 | Time: 17s530ms
Epoch: 004 | Test Loss: 8.0754154 | Time: 799ms
==> Save the model at epoch 004 with test loss 8.0754154
Epoch: 005 | Train Loss: 8.0069789 | Grad norm: 6.015635 | Time: 17s590ms
Epoch: 005 | Test Loss: 7.8189343 | Time: 765ms
==> Save the model at epoch 005 with test loss 7.8189343
Epoch: 006 | Train Loss: 7.8410242 | Grad norm: 5.408593 | Time: 18s136ms
Epoch: 006 | Test Loss: 7.7419077 | Time: 755ms
==> Save the model at epoch 006 with test loss 7.7419077
Epoch: 007 | Train Loss: 7.7930086 | Grad norm: 5.218866 | Time: 20s880ms
Epoch: 007 | Test Loss: 7.7264787 | Time: 795ms
==> Save the model at epoch 007 with test loss 7.7264787
Epoch: 008 | Train Loss: 7.7840233 | Grad norm: 5.065289 | Time: 21s613ms
Epoch: 008 | Test Loss: 7.7062250 | Time: 790ms
==> Save the model at epoch 008 with test loss 7.7062250
Epoch: 009 | Train Loss: 7.7819485 | Grad norm: 4.907197 | Time: 19s223ms
Epoch: 009 | Test Loss: 7.7073872 | Time: 769ms
Epoch: 010 | Train Loss: 7.7798636 | Grad norm: 4.208967 | Time: 19s448ms
Epoch: 010 | Test Loss: 7.7063794 | Time: 805ms
Epoch: 011 | Train Loss: 7.7763925 | Grad norm: 3.678654 | Time: 21s762ms
Epoch: 011 | Test Loss: 7.7041546 | Time: 798ms
==> Save the model at epoch 011 with test loss 7.7041546
Epoch: 012 | Train Loss: 7.7791922 | Grad norm: 3.215481 | Time: 20s769ms
Epoch: 012 | Test Loss: 7.7017420 | Time: 801ms
==> Save the model at epoch 012 with test loss 7.7017420
Epoch: 013 | Train Loss: 7.7718547 | Grad norm: 2.901746 | Time: 21s459ms
Epoch: 013 | Test Loss: 7.7030991 | Time: 796ms
Epoch: 014 | Train Loss: 7.7749127 | Grad norm: 2.496214 | Time: 20s796ms
Epoch: 014 | Test Loss: 7.6999611 | Time: 793ms
==> Save the model at epoch 014 with test loss 7.6999611
Epoch: 015 | Train Loss: 7.7711128 | Grad norm: 2.414679 | Time: 19s179ms
Epoch: 015 | Test Loss: 7.6993164 | Time: 794ms
==> Save the model at epoch 015 with test loss 7.6993164
Epoch: 016 | Train Loss: 7.7704444 | Grad norm: 2.122401 | Time: 20s754ms
Epoch: 016 | Test Loss: 7.7082046 | Time: 795ms
Epoch: 017 | Train Loss: 7.7696841 | Grad norm: 1.774356 | Time: 20s706ms
Epoch: 017 | Test Loss: 7.6993833 | Time: 796ms
Epoch: 018 | Train Loss: 7.7682714 | Grad norm: 1.849220 | Time: 21s793ms
Epoch: 018 | Test Loss: 7.6984535 | Time: 796ms
==> Save the model at epoch 018 with test loss 7.6984535
Epoch: 019 | Train Loss: 7.7679271 | Grad norm: 1.520922 | Time: 20s774ms
Epoch: 019 | Test Loss: 7.6983514 | Time: 784ms
==> Save the model at epoch 019 with test loss 7.6983514
Epoch: 020 | Train Loss: 7.7687420 | Grad norm: 1.013355 | Time: 19s168ms
Epoch: 020 | Test Loss: 7.6966796 | Time: 761ms
==> Save the model at epoch 020 with test loss 7.6966796
Epoch: 021 | Train Loss: 7.7674542 | Grad norm: 1.354823 | Time: 18s89ms
Epoch: 021 | Test Loss: 7.6973382 | Time: 798ms
Epoch: 022 | Train Loss: 7.7670887 | Grad norm: 1.315597 | Time: 18s46ms
Epoch: 022 | Test Loss: 7.6956862 | Time: 798ms
==> Save the model at epoch 022 with test loss 7.6956862
Epoch: 023 | Train Loss: 7.7656073 | Grad norm: 0.561644 | Time: 18s9ms
Epoch: 023 | Test Loss: 7.6956800 | Time: 795ms
==> Save the model at epoch 023 with test loss 7.6956800
Epoch: 024 | Train Loss: 7.7655722 | Grad norm: 0.490320 | Time: 18s687ms
Epoch: 024 | Test Loss: 7.7007274 | Time: 793ms
Epoch: 025 | Train Loss: 7.7653606 | Grad norm: 0.444661 | Time: 21s49ms
Epoch: 025 | Test Loss: 7.6961549 | Time: 800ms
Epoch: 026 | Train Loss: 7.7667090 | Grad norm: 0.676982 | Time: 20s942ms
Epoch: 026 | Test Loss: 7.6976445 | Time: 765ms
Epoch: 027 | Train Loss: 7.7655000 | Grad norm: 0.456053 | Time: 17s746ms
Epoch: 027 | Test Loss: 7.6955417 | Time: 768ms
==> Save the model at epoch 027 with test loss 7.6955417
Epoch: 028 | Train Loss: 7.7651455 | Grad norm: 0.307291 | Time: 17s706ms
Epoch: 028 | Test Loss: 7.6956388 | Time: 858ms
Epoch: 029 | Train Loss: 7.7652472 | Grad norm: 0.388026 | Time: 17s604ms
Epoch: 029 | Test Loss: 7.6968902 | Time: 794ms
Epoch: 030 | Train Loss: 7.7651617 | Grad norm: 0.316029 | Time: 17s827ms
Epoch: 030 | Test Loss: 7.6963352 | Time: 788ms
Epoch: 031 | Train Loss: 7.7651503 | Grad norm: 0.319772 | Time: 17s773ms
Epoch: 031 | Test Loss: 7.6957917 | Time: 853ms
Epoch: 032 | Train Loss: 7.7651460 | Grad norm: 0.305681 | Time: 18s221ms
Epoch: 032 | Test Loss: 7.6955617 | Time: 775ms
Epoch: 033 | Train Loss: 7.7650889 | Grad norm: 0.240875 | Time: 20s867ms
Epoch: 033 | Test Loss: 7.6955568 | Time: 786ms
Epoch: 034 | Train Loss: 7.7650609 | Grad norm: 0.203600 | Time: 20s483ms
Epoch: 034 | Test Loss: 7.6955722 | Time: 852ms
Epoch: 035 | Train Loss: 7.7650286 | Grad norm: 0.160141 | Time: 21s612ms
Epoch: 035 | Test Loss: 7.6955426 | Time: 786ms
Epoch: 036 | Train Loss: 7.7650272 | Grad norm: 0.152595 | Time: 20s811ms
Epoch: 036 | Test Loss: 7.6956331 | Time: 805ms
Epoch: 037 | Train Loss: 7.7650169 | Grad norm: 0.141962 | Time: 19s293ms
Epoch: 037 | Test Loss: 7.6955415 | Time: 857ms
==> Save the model at epoch 037 with test loss 7.6955415
Epoch: 038 | Train Loss: 7.7650101 | Grad norm: 0.119324 | Time: 17s772ms
Epoch: 038 | Test Loss: 7.6955514 | Time: 795ms
Epoch: 039 | Train Loss: 7.7650030 | Grad norm: 0.099046 | Time: 18s993ms
Epoch: 039 | Test Loss: 7.6955406 | Time: 798ms
==> Save the model at epoch 039 with test loss 7.6955406
Epoch: 040 | Train Loss: 7.7649970 | Grad norm: 0.080842 | Time: 21s522ms
Epoch: 040 | Test Loss: 7.6955339 | Time: 842ms
==> Save the model at epoch 040 with test loss 7.6955339
Total time: 13m32s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
