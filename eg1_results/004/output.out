==> torch device:  cuda:3
==> Lipschitz constant: 8.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 128]                  16,897
│    └─SandwichFc: 2-2                   [1, 128]                  33,025
│    └─SandwichFc: 2-3                   [1, 128]                  33,025
│    └─SandwichFc: 2-4                   [1, 128]                  33,025
│    └─SandwichFc: 2-5                   [1, 128]                  33,025
│    └─SandwichFc: 2-6                   [1, 128]                  33,025
│    └─SandwichFc: 2-7                   [1, 128]                  33,025
│    └─SandwichLin: 2-8                  [1, 2]                    263
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 128]                  (recursive)
│    └─SandwichFc: 2-10                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-11                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-12                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-13                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-14                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-15                  [1, 128]                  (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 215,316
Trainable params: 215,310
Non-trainable params: 6
Total mult-adds (M): 0.43
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.25
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.9393677 | Grad norm: 61.601983 | Time: 4s332ms
Epoch: 001 | Test Loss: 30.9308201 | Time: 778ms
==> Save the model at epoch 001 with test loss 30.9308201
Epoch: 002 | Train Loss: 3.4711943 | Grad norm: 26.360832 | Time: 4s488ms
Epoch: 002 | Test Loss: 0.0147494 | Time: 816ms
==> Save the model at epoch 002 with test loss 0.0147494
Epoch: 003 | Train Loss: 0.0626611 | Grad norm: 16.887363 | Time: 4s311ms
Epoch: 003 | Test Loss: 0.0447192 | Time: 773ms
Epoch: 004 | Train Loss: 0.0632214 | Grad norm: 14.947181 | Time: 4s361ms
Epoch: 004 | Test Loss: 0.0693570 | Time: 786ms
Epoch: 005 | Train Loss: 0.0854580 | Grad norm: 17.832525 | Time: 4s363ms
Epoch: 005 | Test Loss: 0.0874182 | Time: 782ms
Epoch: 006 | Train Loss: 0.0979070 | Grad norm: 19.835986 | Time: 4s314ms
Epoch: 006 | Test Loss: 0.1165684 | Time: 785ms
Epoch: 007 | Train Loss: 0.1124540 | Grad norm: 20.031171 | Time: 4s323ms
Epoch: 007 | Test Loss: 0.0664905 | Time: 840ms
Epoch: 008 | Train Loss: 0.1233261 | Grad norm: 20.285889 | Time: 4s319ms
Epoch: 008 | Test Loss: 0.1028395 | Time: 772ms
Epoch: 009 | Train Loss: 0.1649215 | Grad norm: 20.396555 | Time: 4s318ms
Epoch: 009 | Test Loss: 0.1904864 | Time: 818ms
Epoch: 010 | Train Loss: 0.1680871 | Grad norm: 21.118065 | Time: 4s329ms
Epoch: 010 | Test Loss: 0.1469978 | Time: 778ms
Epoch: 011 | Train Loss: 0.1526674 | Grad norm: 20.736469 | Time: 4s360ms
Epoch: 011 | Test Loss: 0.1188008 | Time: 842ms
Epoch: 012 | Train Loss: 0.1278012 | Grad norm: 18.019195 | Time: 4s317ms
Epoch: 012 | Test Loss: 0.0764755 | Time: 771ms
Epoch: 013 | Train Loss: 0.1464760 | Grad norm: 16.741434 | Time: 4s331ms
Epoch: 013 | Test Loss: 0.1088155 | Time: 768ms
Epoch: 014 | Train Loss: 0.1006498 | Grad norm: 15.324577 | Time: 4s317ms
Epoch: 014 | Test Loss: 0.0914307 | Time: 773ms
Epoch: 015 | Train Loss: 0.0839765 | Grad norm: 13.906352 | Time: 4s374ms
Epoch: 015 | Test Loss: 0.0688331 | Time: 773ms
Epoch: 016 | Train Loss: 0.0792342 | Grad norm: 12.946252 | Time: 4s703ms
Epoch: 016 | Test Loss: 0.0703182 | Time: 826ms
Epoch: 017 | Train Loss: 0.0845273 | Grad norm: 11.649041 | Time: 5s96ms
Epoch: 017 | Test Loss: 0.1053589 | Time: 783ms
Epoch: 018 | Train Loss: 0.0641681 | Grad norm: 11.727136 | Time: 5s13ms
Epoch: 018 | Test Loss: 0.0342692 | Time: 809ms
Epoch: 019 | Train Loss: 0.0594765 | Grad norm: 10.761110 | Time: 5s471ms
Epoch: 019 | Test Loss: 0.0516311 | Time: 806ms
Epoch: 020 | Train Loss: 0.0619651 | Grad norm: 10.211267 | Time: 4s321ms
Epoch: 020 | Test Loss: 0.0363095 | Time: 777ms
Epoch: 021 | Train Loss: 0.0534250 | Grad norm: 9.719327 | Time: 4s338ms
Epoch: 021 | Test Loss: 0.0397635 | Time: 804ms
Epoch: 022 | Train Loss: 0.0584424 | Grad norm: 8.838526 | Time: 5s534ms
Epoch: 022 | Test Loss: 0.0472642 | Time: 868ms
Epoch: 023 | Train Loss: 0.0436800 | Grad norm: 8.879499 | Time: 5s10ms
Epoch: 023 | Test Loss: 0.0257327 | Time: 766ms
Epoch: 024 | Train Loss: 0.0544950 | Grad norm: 8.292547 | Time: 5s52ms
Epoch: 024 | Test Loss: 0.0250720 | Time: 757ms
Epoch: 025 | Train Loss: 0.0425183 | Grad norm: 7.844165 | Time: 5s306ms
Epoch: 025 | Test Loss: 0.0280636 | Time: 770ms
Epoch: 026 | Train Loss: 0.0344788 | Grad norm: 7.538747 | Time: 4s518ms
Epoch: 026 | Test Loss: 0.0289591 | Time: 776ms
Epoch: 027 | Train Loss: 0.0308240 | Grad norm: 7.140713 | Time: 4s410ms
Epoch: 027 | Test Loss: 0.0321715 | Time: 781ms
Epoch: 028 | Train Loss: 0.0276184 | Grad norm: 6.611534 | Time: 5s112ms
Epoch: 028 | Test Loss: 0.0222739 | Time: 772ms
Epoch: 029 | Train Loss: 0.0269792 | Grad norm: 6.352513 | Time: 5s46ms
Epoch: 029 | Test Loss: 0.0334284 | Time: 777ms
Epoch: 030 | Train Loss: 0.0311029 | Grad norm: 5.633994 | Time: 5s93ms
Epoch: 030 | Test Loss: 0.0636838 | Time: 770ms
Epoch: 031 | Train Loss: 0.0272542 | Grad norm: 6.075608 | Time: 5s51ms
Epoch: 031 | Test Loss: 0.0185592 | Time: 769ms
Epoch: 032 | Train Loss: 0.0225699 | Grad norm: 5.542166 | Time: 5s61ms
Epoch: 032 | Test Loss: 0.0219014 | Time: 777ms
Epoch: 033 | Train Loss: 0.0237028 | Grad norm: 5.029449 | Time: 5s40ms
Epoch: 033 | Test Loss: 0.0273534 | Time: 826ms
Epoch: 034 | Train Loss: 0.0186128 | Grad norm: 4.943032 | Time: 5s33ms
Epoch: 034 | Test Loss: 0.0199767 | Time: 765ms
Epoch: 035 | Train Loss: 0.0181522 | Grad norm: 4.541358 | Time: 5s17ms
Epoch: 035 | Test Loss: 0.0196200 | Time: 781ms
Epoch: 036 | Train Loss: 0.0251615 | Grad norm: 5.024145 | Time: 5s373ms
Epoch: 036 | Test Loss: 0.0131273 | Time: 774ms
==> Save the model at epoch 036 with test loss 0.0131273
Epoch: 037 | Train Loss: 0.0149630 | Grad norm: 4.057811 | Time: 5s281ms
Epoch: 037 | Test Loss: 0.0128718 | Time: 854ms
==> Save the model at epoch 037 with test loss 0.0128718
Epoch: 038 | Train Loss: 0.0160242 | Grad norm: 3.778530 | Time: 5s228ms
Epoch: 038 | Test Loss: 0.0165760 | Time: 779ms
Epoch: 039 | Train Loss: 0.0224538 | Grad norm: 4.192073 | Time: 4s560ms
Epoch: 039 | Test Loss: 0.0338407 | Time: 779ms
Epoch: 040 | Train Loss: 0.0231065 | Grad norm: 5.996473 | Time: 4s330ms
Epoch: 040 | Test Loss: 0.0269679 | Time: 775ms
Epoch: 041 | Train Loss: 0.0125066 | Grad norm: 3.080378 | Time: 4s441ms
Epoch: 041 | Test Loss: 0.0117158 | Time: 777ms
==> Save the model at epoch 041 with test loss 0.0117158
Epoch: 042 | Train Loss: 0.0119914 | Grad norm: 2.693515 | Time: 4s324ms
Epoch: 042 | Test Loss: 0.0173187 | Time: 775ms
Epoch: 043 | Train Loss: 0.0121914 | Grad norm: 2.365375 | Time: 4s334ms
Epoch: 043 | Test Loss: 0.0133426 | Time: 815ms
Epoch: 044 | Train Loss: 0.0106994 | Grad norm: 1.915907 | Time: 4s323ms
Epoch: 044 | Test Loss: 0.0091603 | Time: 837ms
==> Save the model at epoch 044 with test loss 0.0091603
Epoch: 045 | Train Loss: 0.0123609 | Grad norm: 3.070711 | Time: 4s349ms
Epoch: 045 | Test Loss: 0.0098129 | Time: 772ms
Epoch: 046 | Train Loss: 0.0096132 | Grad norm: 1.822989 | Time: 4s329ms
Epoch: 046 | Test Loss: 0.0102779 | Time: 772ms
Epoch: 047 | Train Loss: 0.0096689 | Grad norm: 1.804653 | Time: 4s332ms
Epoch: 047 | Test Loss: 0.0105462 | Time: 767ms
Epoch: 048 | Train Loss: 0.0090128 | Grad norm: 1.643289 | Time: 4s326ms
Epoch: 048 | Test Loss: 0.0086896 | Time: 828ms
==> Save the model at epoch 048 with test loss 0.0086896
Epoch: 049 | Train Loss: 0.0081282 | Grad norm: 1.195243 | Time: 4s336ms
Epoch: 049 | Test Loss: 0.0089623 | Time: 775ms
Epoch: 050 | Train Loss: 0.0082371 | Grad norm: 1.229965 | Time: 4s331ms
Epoch: 050 | Test Loss: 0.0082869 | Time: 779ms
==> Save the model at epoch 050 with test loss 0.0082869
Epoch: 051 | Train Loss: 0.0079510 | Grad norm: 1.105655 | Time: 4s784ms
Epoch: 051 | Test Loss: 0.0084732 | Time: 764ms
Epoch: 052 | Train Loss: 0.0077101 | Grad norm: 0.977941 | Time: 5s646ms
Epoch: 052 | Test Loss: 0.0080002 | Time: 784ms
==> Save the model at epoch 052 with test loss 0.0080002
Epoch: 053 | Train Loss: 0.0076271 | Grad norm: 0.952756 | Time: 5s822ms
Epoch: 053 | Test Loss: 0.0079594 | Time: 781ms
==> Save the model at epoch 053 with test loss 0.0079594
Epoch: 054 | Train Loss: 0.0074758 | Grad norm: 0.883213 | Time: 5s23ms
Epoch: 054 | Test Loss: 0.0075764 | Time: 773ms
==> Save the model at epoch 054 with test loss 0.0075764
Epoch: 055 | Train Loss: 0.0072621 | Grad norm: 0.662445 | Time: 4s755ms
Epoch: 055 | Test Loss: 0.0076658 | Time: 914ms
Epoch: 056 | Train Loss: 0.0071646 | Grad norm: 0.609082 | Time: 5s480ms
Epoch: 056 | Test Loss: 0.0073897 | Time: 769ms
==> Save the model at epoch 056 with test loss 0.0073897
Epoch: 057 | Train Loss: 0.0070731 | Grad norm: 0.527922 | Time: 5s40ms
Epoch: 057 | Test Loss: 0.0073160 | Time: 770ms
==> Save the model at epoch 057 with test loss 0.0073160
Epoch: 058 | Train Loss: 0.0069859 | Grad norm: 0.453280 | Time: 5s54ms
Epoch: 058 | Test Loss: 0.0072937 | Time: 769ms
==> Save the model at epoch 058 with test loss 0.0072937
Epoch: 059 | Train Loss: 0.0069369 | Grad norm: 0.388372 | Time: 5s960ms
Epoch: 059 | Test Loss: 0.0072556 | Time: 827ms
==> Save the model at epoch 059 with test loss 0.0072556
Epoch: 060 | Train Loss: 0.0069157 | Grad norm: 0.377810 | Time: 5s878ms
Epoch: 060 | Test Loss: 0.0072390 | Time: 814ms
==> Save the model at epoch 060 with test loss 0.0072390
Total time: 5m33s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
