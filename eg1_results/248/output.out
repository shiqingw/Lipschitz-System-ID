==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 8.4138052 | L2 loss: 8.4137921 | Lip loss: 0.0000131 | Grad norm: 4.489441 | Time: 40s143ms
Epoch: 001 | Test Loss: 0.0045640 | Time: 551ms
==> Save the model at epoch 001 with test loss 0.0045640
Epoch: 002 | Loss: 0.0039677 | L2 loss: 0.0039471 | Lip loss: 0.0000206 | Grad norm: 0.851135 | Time: 41s45ms
Epoch: 002 | Test Loss: 0.0042273 | Time: 544ms
==> Save the model at epoch 002 with test loss 0.0042273
Epoch: 003 | Loss: 0.0035433 | L2 loss: 0.0035228 | Lip loss: 0.0000205 | Grad norm: 0.767848 | Time: 40s456ms
Epoch: 003 | Test Loss: 0.0032244 | Time: 534ms
==> Save the model at epoch 003 with test loss 0.0032244
Epoch: 004 | Loss: 0.0033362 | L2 loss: 0.0033157 | Lip loss: 0.0000205 | Grad norm: 0.682323 | Time: 40s466ms
Epoch: 004 | Test Loss: 0.0032421 | Time: 545ms
Epoch: 005 | Loss: 0.0033072 | L2 loss: 0.0032868 | Lip loss: 0.0000204 | Grad norm: 0.759871 | Time: 40s407ms
Epoch: 005 | Test Loss: 0.0031401 | Time: 536ms
==> Save the model at epoch 005 with test loss 0.0031401
Epoch: 006 | Loss: 0.0030160 | L2 loss: 0.0029956 | Lip loss: 0.0000204 | Grad norm: 0.269634 | Time: 40s682ms
Epoch: 006 | Test Loss: 0.0030043 | Time: 615ms
==> Save the model at epoch 006 with test loss 0.0030043
Epoch: 007 | Loss: 0.0030121 | L2 loss: 0.0029917 | Lip loss: 0.0000204 | Grad norm: 0.267355 | Time: 39s684ms
Epoch: 007 | Test Loss: 0.0030128 | Time: 549ms
Epoch: 008 | Loss: 0.0030092 | L2 loss: 0.0029888 | Lip loss: 0.0000204 | Grad norm: 0.278530 | Time: 40s72ms
Epoch: 008 | Test Loss: 0.0029892 | Time: 556ms
==> Save the model at epoch 008 with test loss 0.0029892
Epoch: 009 | Loss: 0.0030117 | L2 loss: 0.0029913 | Lip loss: 0.0000204 | Grad norm: 0.300270 | Time: 40s954ms
Epoch: 009 | Test Loss: 0.0030279 | Time: 543ms
Epoch: 010 | Loss: 0.0030024 | L2 loss: 0.0029820 | Lip loss: 0.0000204 | Grad norm: 0.291453 | Time: 40s32ms
Epoch: 010 | Test Loss: 0.0030337 | Time: 545ms
Epoch: 011 | Loss: 0.0029727 | L2 loss: 0.0029523 | Lip loss: 0.0000204 | Grad norm: 0.199550 | Time: 40s736ms
Epoch: 011 | Test Loss: 0.0029748 | Time: 542ms
==> Save the model at epoch 011 with test loss 0.0029748
Epoch: 012 | Loss: 0.0029727 | L2 loss: 0.0029523 | Lip loss: 0.0000204 | Grad norm: 0.205881 | Time: 41s9ms
Epoch: 012 | Test Loss: 0.0029733 | Time: 544ms
==> Save the model at epoch 012 with test loss 0.0029733
Epoch: 013 | Loss: 0.0029721 | L2 loss: 0.0029517 | Lip loss: 0.0000204 | Grad norm: 0.205852 | Time: 41s127ms
Epoch: 013 | Test Loss: 0.0029803 | Time: 620ms
Epoch: 014 | Loss: 0.0029722 | L2 loss: 0.0029518 | Lip loss: 0.0000204 | Grad norm: 0.210692 | Time: 40s872ms
Epoch: 014 | Test Loss: 0.0029748 | Time: 535ms
Epoch: 015 | Loss: 0.0029710 | L2 loss: 0.0029506 | Lip loss: 0.0000204 | Grad norm: 0.209628 | Time: 40s911ms
Epoch: 015 | Test Loss: 0.0029810 | Time: 542ms
Epoch: 016 | Loss: 0.0029665 | L2 loss: 0.0029462 | Lip loss: 0.0000204 | Grad norm: 0.196014 | Time: 40s573ms
Epoch: 016 | Test Loss: 0.0029734 | Time: 609ms
Epoch: 017 | Loss: 0.0029661 | L2 loss: 0.0029457 | Lip loss: 0.0000204 | Grad norm: 0.196716 | Time: 40s721ms
Epoch: 017 | Test Loss: 0.0029713 | Time: 533ms
==> Save the model at epoch 017 with test loss 0.0029713
Epoch: 018 | Loss: 0.0029661 | L2 loss: 0.0029457 | Lip loss: 0.0000204 | Grad norm: 0.197947 | Time: 40s432ms
Epoch: 018 | Test Loss: 0.0029709 | Time: 540ms
==> Save the model at epoch 018 with test loss 0.0029709
Epoch: 019 | Loss: 0.0029659 | L2 loss: 0.0029455 | Lip loss: 0.0000204 | Grad norm: 0.190149 | Time: 40s596ms
Epoch: 019 | Test Loss: 0.0029705 | Time: 542ms
==> Save the model at epoch 019 with test loss 0.0029705
Epoch: 020 | Loss: 0.0029658 | L2 loss: 0.0029454 | Lip loss: 0.0000204 | Grad norm: 0.189111 | Time: 41s308ms
Epoch: 020 | Test Loss: 0.0029718 | Time: 531ms
Epoch: 021 | Loss: 0.0029656 | L2 loss: 0.0029453 | Lip loss: 0.0000204 | Grad norm: 0.180072 | Time: 40s869ms
Epoch: 021 | Test Loss: 0.0029710 | Time: 535ms
Epoch: 022 | Loss: 0.0029655 | L2 loss: 0.0029451 | Lip loss: 0.0000204 | Grad norm: 0.183938 | Time: 39s999ms
Epoch: 022 | Test Loss: 0.0029710 | Time: 529ms
Epoch: 023 | Loss: 0.0029652 | L2 loss: 0.0029448 | Lip loss: 0.0000204 | Grad norm: 0.188789 | Time: 41s110ms
Epoch: 023 | Test Loss: 0.0029708 | Time: 605ms
Epoch: 024 | Loss: 0.0029653 | L2 loss: 0.0029449 | Lip loss: 0.0000204 | Grad norm: 0.190206 | Time: 39s882ms
Epoch: 024 | Test Loss: 0.0029708 | Time: 537ms
Epoch: 025 | Loss: 0.0029653 | L2 loss: 0.0029449 | Lip loss: 0.0000204 | Grad norm: 0.189790 | Time: 40s415ms
Epoch: 025 | Test Loss: 0.0029709 | Time: 537ms
Epoch: 026 | Loss: 0.0029651 | L2 loss: 0.0029447 | Lip loss: 0.0000204 | Grad norm: 0.190932 | Time: 40s608ms
Epoch: 026 | Test Loss: 0.0029708 | Time: 547ms
Epoch: 027 | Loss: 0.0029653 | L2 loss: 0.0029449 | Lip loss: 0.0000204 | Grad norm: 0.194336 | Time: 41s87ms
Epoch: 027 | Test Loss: 0.0029708 | Time: 535ms
Epoch: 028 | Loss: 0.0029652 | L2 loss: 0.0029448 | Lip loss: 0.0000204 | Grad norm: 0.196308 | Time: 39s997ms
Epoch: 028 | Test Loss: 0.0029708 | Time: 544ms
Epoch: 029 | Loss: 0.0029654 | L2 loss: 0.0029450 | Lip loss: 0.0000204 | Grad norm: 0.184800 | Time: 40s576ms
Epoch: 029 | Test Loss: 0.0029708 | Time: 549ms
Epoch: 030 | Loss: 0.0029651 | L2 loss: 0.0029447 | Lip loss: 0.0000204 | Grad norm: 0.195138 | Time: 40s293ms
Epoch: 030 | Test Loss: 0.0029708 | Time: 540ms
Epoch: 031 | Loss: 0.0029652 | L2 loss: 0.0029448 | Lip loss: 0.0000204 | Grad norm: 0.186327 | Time: 40s746ms
Epoch: 031 | Test Loss: 0.0029708 | Time: 536ms
Epoch: 032 | Loss: 0.0029655 | L2 loss: 0.0029451 | Lip loss: 0.0000204 | Grad norm: 0.190143 | Time: 40s824ms
Epoch: 032 | Test Loss: 0.0029708 | Time: 537ms
Epoch: 033 | Loss: 0.0029651 | L2 loss: 0.0029447 | Lip loss: 0.0000204 | Grad norm: 0.184705 | Time: 41s127ms
Epoch: 033 | Test Loss: 0.0029708 | Time: 543ms
Epoch: 034 | Loss: 0.0029651 | L2 loss: 0.0029448 | Lip loss: 0.0000204 | Grad norm: 0.186657 | Time: 41s672ms
Epoch: 034 | Test Loss: 0.0029708 | Time: 544ms
Epoch: 035 | Loss: 0.0029654 | L2 loss: 0.0029450 | Lip loss: 0.0000204 | Grad norm: 0.191050 | Time: 41s106ms
Epoch: 035 | Test Loss: 0.0029708 | Time: 545ms
Epoch: 036 | Loss: 0.0029652 | L2 loss: 0.0029447 | Lip loss: 0.0000204 | Grad norm: 0.188090 | Time: 41s215ms
Epoch: 036 | Test Loss: 0.0029708 | Time: 532ms
Epoch: 037 | Loss: 0.0029649 | L2 loss: 0.0029446 | Lip loss: 0.0000204 | Grad norm: 0.188053 | Time: 40s819ms
Epoch: 037 | Test Loss: 0.0029708 | Time: 612ms
Epoch: 038 | Loss: 0.0029652 | L2 loss: 0.0029448 | Lip loss: 0.0000204 | Grad norm: 0.190895 | Time: 40s624ms
Epoch: 038 | Test Loss: 0.0029708 | Time: 538ms
Epoch: 039 | Loss: 0.0029653 | L2 loss: 0.0029449 | Lip loss: 0.0000204 | Grad norm: 0.189775 | Time: 40s912ms
Epoch: 039 | Test Loss: 0.0029708 | Time: 551ms
Epoch: 040 | Loss: 0.0029652 | L2 loss: 0.0029448 | Lip loss: 0.0000204 | Grad norm: 0.186734 | Time: 40s912ms
Epoch: 040 | Test Loss: 0.0029708 | Time: 542ms
Total time: 27m29s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
