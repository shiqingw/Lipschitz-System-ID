==> torch device:  cuda:2
==> Lipschitz constant: 4.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 128]                  16,897
│    └─SandwichFc: 2-2                   [1, 128]                  33,025
│    └─SandwichFc: 2-3                   [1, 128]                  33,025
│    └─SandwichFc: 2-4                   [1, 128]                  33,025
│    └─SandwichFc: 2-5                   [1, 128]                  33,025
│    └─SandwichFc: 2-6                   [1, 128]                  33,025
│    └─SandwichFc: 2-7                   [1, 128]                  33,025
│    └─SandwichLin: 2-8                  [1, 2]                    263
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 128]                  (recursive)
│    └─SandwichFc: 2-10                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-11                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-12                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-13                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-14                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-15                  [1, 128]                  (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 215,316
Trainable params: 215,310
Non-trainable params: 6
Total mult-adds (M): 0.43
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.25
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.9794943 | Grad norm: 30.783164 | Time: 5s467ms
Epoch: 001 | Test Loss: 30.9740978 | Time: 786ms
==> Save the model at epoch 001 with test loss 30.9740978
Epoch: 002 | Train Loss: 4.8352959 | Grad norm: 18.359323 | Time: 5s470ms
Epoch: 002 | Test Loss: 0.0211095 | Time: 775ms
==> Save the model at epoch 002 with test loss 0.0211095
Epoch: 003 | Train Loss: 0.0788426 | Grad norm: 16.656618 | Time: 5s295ms
Epoch: 003 | Test Loss: 0.0209760 | Time: 783ms
==> Save the model at epoch 003 with test loss 0.0209760
Epoch: 004 | Train Loss: 0.0857387 | Grad norm: 14.987860 | Time: 5s513ms
Epoch: 004 | Test Loss: 0.0249455 | Time: 871ms
Epoch: 005 | Train Loss: 0.0699056 | Grad norm: 13.990222 | Time: 5s18ms
Epoch: 005 | Test Loss: 0.0394190 | Time: 769ms
Epoch: 006 | Train Loss: 0.0905816 | Grad norm: 13.486857 | Time: 5s92ms
Epoch: 006 | Test Loss: 0.1531193 | Time: 770ms
Epoch: 007 | Train Loss: 0.1034303 | Grad norm: 18.502472 | Time: 5s25ms
Epoch: 007 | Test Loss: 0.0939425 | Time: 831ms
Epoch: 008 | Train Loss: 0.1033097 | Grad norm: 18.430695 | Time: 5s13ms
Epoch: 008 | Test Loss: 0.1095889 | Time: 765ms
Epoch: 009 | Train Loss: 0.1155085 | Grad norm: 18.189049 | Time: 5s141ms
Epoch: 009 | Test Loss: 0.1470638 | Time: 858ms
Epoch: 010 | Train Loss: 0.1168147 | Grad norm: 17.940806 | Time: 5s316ms
Epoch: 010 | Test Loss: 0.0857386 | Time: 782ms
Epoch: 011 | Train Loss: 0.1308935 | Grad norm: 17.560493 | Time: 5s46ms
Epoch: 011 | Test Loss: 0.1494767 | Time: 834ms
Epoch: 012 | Train Loss: 0.1102877 | Grad norm: 15.711538 | Time: 4s994ms
Epoch: 012 | Test Loss: 0.0821175 | Time: 759ms
Epoch: 013 | Train Loss: 0.0973868 | Grad norm: 14.176955 | Time: 4s988ms
Epoch: 013 | Test Loss: 0.1213118 | Time: 757ms
Epoch: 014 | Train Loss: 0.0863955 | Grad norm: 13.145312 | Time: 5s191ms
Epoch: 014 | Test Loss: 0.1628717 | Time: 758ms
Epoch: 015 | Train Loss: 0.0797635 | Grad norm: 12.121418 | Time: 5s51ms
Epoch: 015 | Test Loss: 0.0876713 | Time: 758ms
Epoch: 016 | Train Loss: 0.0715987 | Grad norm: 11.194598 | Time: 5s165ms
Epoch: 016 | Test Loss: 0.0565980 | Time: 784ms
Epoch: 017 | Train Loss: 0.1293210 | Grad norm: 12.607943 | Time: 5s347ms
Epoch: 017 | Test Loss: 0.1912144 | Time: 769ms
Epoch: 018 | Train Loss: 0.0889387 | Grad norm: 11.669180 | Time: 4s987ms
Epoch: 018 | Test Loss: 0.0656574 | Time: 771ms
Epoch: 019 | Train Loss: 0.0612728 | Grad norm: 9.972006 | Time: 5s400ms
Epoch: 019 | Test Loss: 0.0877986 | Time: 827ms
Epoch: 020 | Train Loss: 0.0543521 | Grad norm: 9.432777 | Time: 5s839ms
Epoch: 020 | Test Loss: 0.0351891 | Time: 768ms
Epoch: 021 | Train Loss: 0.0455680 | Grad norm: 8.705388 | Time: 5s782ms
Epoch: 021 | Test Loss: 0.0409789 | Time: 768ms
Epoch: 022 | Train Loss: 0.0417149 | Grad norm: 8.194152 | Time: 5s512ms
Epoch: 022 | Test Loss: 0.0499879 | Time: 832ms
Epoch: 023 | Train Loss: 0.0391004 | Grad norm: 7.860203 | Time: 4s988ms
Epoch: 023 | Test Loss: 0.0342314 | Time: 766ms
Epoch: 024 | Train Loss: 0.0343804 | Grad norm: 7.387817 | Time: 5s131ms
Epoch: 024 | Test Loss: 0.0406282 | Time: 790ms
Epoch: 025 | Train Loss: 0.0324985 | Grad norm: 6.892009 | Time: 5s835ms
Epoch: 025 | Test Loss: 0.0308850 | Time: 763ms
Epoch: 026 | Train Loss: 0.0297277 | Grad norm: 6.627577 | Time: 4s374ms
Epoch: 026 | Test Loss: 0.0370123 | Time: 762ms
Epoch: 027 | Train Loss: 0.0289819 | Grad norm: 6.328000 | Time: 4s309ms
Epoch: 027 | Test Loss: 0.0249985 | Time: 764ms
Epoch: 028 | Train Loss: 0.0257458 | Grad norm: 5.971678 | Time: 4s320ms
Epoch: 028 | Test Loss: 0.0256981 | Time: 761ms
Epoch: 029 | Train Loss: 0.0245071 | Grad norm: 5.661593 | Time: 4s309ms
Epoch: 029 | Test Loss: 0.0252313 | Time: 764ms
Epoch: 030 | Train Loss: 0.0219651 | Grad norm: 5.265850 | Time: 4s362ms
Epoch: 030 | Test Loss: 0.0196623 | Time: 764ms
==> Save the model at epoch 030 with test loss 0.0196623
Epoch: 031 | Train Loss: 0.0226712 | Grad norm: 5.021266 | Time: 4s307ms
Epoch: 031 | Test Loss: 0.0252672 | Time: 768ms
Epoch: 032 | Train Loss: 0.0211788 | Grad norm: 4.869809 | Time: 4s310ms
Epoch: 032 | Test Loss: 0.0219190 | Time: 762ms
Epoch: 033 | Train Loss: 0.0192562 | Grad norm: 4.497836 | Time: 4s306ms
Epoch: 033 | Test Loss: 0.0201462 | Time: 828ms
Epoch: 034 | Train Loss: 0.0187218 | Grad norm: 4.343105 | Time: 4s303ms
Epoch: 034 | Test Loss: 0.0218660 | Time: 761ms
Epoch: 035 | Train Loss: 0.0162353 | Grad norm: 4.023738 | Time: 4s308ms
Epoch: 035 | Test Loss: 0.0173373 | Time: 763ms
==> Save the model at epoch 035 with test loss 0.0173373
Epoch: 036 | Train Loss: 0.0153800 | Grad norm: 3.753322 | Time: 4s310ms
Epoch: 036 | Test Loss: 0.0185854 | Time: 765ms
Epoch: 037 | Train Loss: 0.0180504 | Grad norm: 3.608602 | Time: 4s306ms
Epoch: 037 | Test Loss: 0.0408791 | Time: 824ms
Epoch: 038 | Train Loss: 0.0178201 | Grad norm: 3.689683 | Time: 4s307ms
Epoch: 038 | Test Loss: 0.0153313 | Time: 765ms
==> Save the model at epoch 038 with test loss 0.0153313
Epoch: 039 | Train Loss: 0.0128103 | Grad norm: 3.071250 | Time: 4s312ms
Epoch: 039 | Test Loss: 0.0140995 | Time: 768ms
==> Save the model at epoch 039 with test loss 0.0140995
Epoch: 040 | Train Loss: 0.0123089 | Grad norm: 2.834973 | Time: 4s360ms
Epoch: 040 | Test Loss: 0.0119279 | Time: 767ms
==> Save the model at epoch 040 with test loss 0.0119279
Epoch: 041 | Train Loss: 0.0122917 | Grad norm: 2.311675 | Time: 5s419ms
Epoch: 041 | Test Loss: 0.0110060 | Time: 788ms
==> Save the model at epoch 041 with test loss 0.0110060
Epoch: 042 | Train Loss: 0.0125427 | Grad norm: 2.286175 | Time: 4s856ms
Epoch: 042 | Test Loss: 0.0106417 | Time: 766ms
==> Save the model at epoch 042 with test loss 0.0106417
Epoch: 043 | Train Loss: 0.0112047 | Grad norm: 2.081743 | Time: 4s399ms
Epoch: 043 | Test Loss: 0.0127837 | Time: 777ms
Epoch: 044 | Train Loss: 0.0097280 | Grad norm: 1.762321 | Time: 4s368ms
Epoch: 044 | Test Loss: 0.0090196 | Time: 762ms
==> Save the model at epoch 044 with test loss 0.0090196
Epoch: 045 | Train Loss: 0.0110053 | Grad norm: 1.862148 | Time: 4s421ms
Epoch: 045 | Test Loss: 0.0149416 | Time: 763ms
Epoch: 046 | Train Loss: 0.0118053 | Grad norm: 3.032539 | Time: 4s359ms
Epoch: 046 | Test Loss: 0.0138333 | Time: 763ms
Epoch: 047 | Train Loss: 0.0097211 | Grad norm: 2.092227 | Time: 4s378ms
Epoch: 047 | Test Loss: 0.0102048 | Time: 772ms
Epoch: 048 | Train Loss: 0.0089348 | Grad norm: 1.527893 | Time: 4s362ms
Epoch: 048 | Test Loss: 0.0095870 | Time: 826ms
Epoch: 049 | Train Loss: 0.0080820 | Grad norm: 1.104947 | Time: 5s64ms
Epoch: 049 | Test Loss: 0.0082304 | Time: 780ms
==> Save the model at epoch 049 with test loss 0.0082304
Epoch: 050 | Train Loss: 0.0078608 | Grad norm: 0.920319 | Time: 5s466ms
Epoch: 050 | Test Loss: 0.0080131 | Time: 817ms
==> Save the model at epoch 050 with test loss 0.0080131
Epoch: 051 | Train Loss: 0.0080151 | Grad norm: 1.035686 | Time: 5s681ms
Epoch: 051 | Test Loss: 0.0083162 | Time: 799ms
Epoch: 052 | Train Loss: 0.0075407 | Grad norm: 0.758288 | Time: 5s328ms
Epoch: 052 | Test Loss: 0.0077973 | Time: 798ms
==> Save the model at epoch 052 with test loss 0.0077973
Epoch: 053 | Train Loss: 0.0074601 | Grad norm: 0.778552 | Time: 5s221ms
Epoch: 053 | Test Loss: 0.0078324 | Time: 761ms
Epoch: 054 | Train Loss: 0.0073031 | Grad norm: 0.645960 | Time: 5s352ms
Epoch: 054 | Test Loss: 0.0074458 | Time: 764ms
==> Save the model at epoch 054 with test loss 0.0074458
Epoch: 055 | Train Loss: 0.0071548 | Grad norm: 0.499422 | Time: 4s304ms
Epoch: 055 | Test Loss: 0.0076751 | Time: 773ms
Epoch: 056 | Train Loss: 0.0070865 | Grad norm: 0.475637 | Time: 4s356ms
Epoch: 056 | Test Loss: 0.0073111 | Time: 765ms
==> Save the model at epoch 056 with test loss 0.0073111
Epoch: 057 | Train Loss: 0.0070505 | Grad norm: 0.453710 | Time: 4s313ms
Epoch: 057 | Test Loss: 0.0072844 | Time: 769ms
==> Save the model at epoch 057 with test loss 0.0072844
Epoch: 058 | Train Loss: 0.0069707 | Grad norm: 0.375123 | Time: 5s633ms
Epoch: 058 | Test Loss: 0.0072834 | Time: 753ms
==> Save the model at epoch 058 with test loss 0.0072834
Epoch: 059 | Train Loss: 0.0069312 | Grad norm: 0.320910 | Time: 5s463ms
Epoch: 059 | Test Loss: 0.0072520 | Time: 1s76ms
==> Save the model at epoch 059 with test loss 0.0072520
Epoch: 060 | Train Loss: 0.0069157 | Grad norm: 0.324447 | Time: 5s412ms
Epoch: 060 | Test Loss: 0.0072328 | Time: 857ms
==> Save the model at epoch 060 with test loss 0.0072328
Total time: 5m41s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
