==> torch device:  cuda:3
==> Lipschitz constant: 8.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 128]                  16,897
│    └─SandwichFc: 2-2                   [1, 128]                  33,025
│    └─SandwichFc: 2-3                   [1, 128]                  33,025
│    └─SandwichFc: 2-4                   [1, 128]                  33,025
│    └─SandwichFc: 2-5                   [1, 128]                  33,025
│    └─SandwichFc: 2-6                   [1, 128]                  33,025
│    └─SandwichFc: 2-7                   [1, 128]                  33,025
│    └─SandwichLin: 2-8                  [1, 2]                    263
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 128]                  (recursive)
│    └─SandwichFc: 2-10                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-11                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-12                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-13                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-14                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-15                  [1, 128]                  (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 215,316
Trainable params: 215,310
Non-trainable params: 6
Total mult-adds (M): 0.43
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.50
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.9388250 | Grad norm: 61.405038 | Time: 11s921ms
Epoch: 001 | Test Loss: 30.9308201 | Time: 806ms
==> Save the model at epoch 001 with test loss 30.9308201
Epoch: 002 | Train Loss: 1.7764104 | Grad norm: 18.535209 | Time: 11s107ms
Epoch: 002 | Test Loss: 0.0401601 | Time: 776ms
==> Save the model at epoch 002 with test loss 0.0401601
Epoch: 003 | Train Loss: 0.0446209 | Grad norm: 12.702181 | Time: 8s897ms
Epoch: 003 | Test Loss: 0.0280516 | Time: 777ms
==> Save the model at epoch 003 with test loss 0.0280516
Epoch: 004 | Train Loss: 0.0489547 | Grad norm: 13.070831 | Time: 9s65ms
Epoch: 004 | Test Loss: 0.0536955 | Time: 782ms
Epoch: 005 | Train Loss: 0.0677382 | Grad norm: 15.543600 | Time: 10s213ms
Epoch: 005 | Test Loss: 0.0627390 | Time: 789ms
Epoch: 006 | Train Loss: 0.0782348 | Grad norm: 16.498550 | Time: 10s914ms
Epoch: 006 | Test Loss: 0.0447140 | Time: 790ms
Epoch: 007 | Train Loss: 0.1001305 | Grad norm: 15.425831 | Time: 11s506ms
Epoch: 007 | Test Loss: 0.0784358 | Time: 782ms
Epoch: 008 | Train Loss: 0.0979729 | Grad norm: 16.815006 | Time: 10s881ms
Epoch: 008 | Test Loss: 0.0711070 | Time: 793ms
Epoch: 009 | Train Loss: 0.0991596 | Grad norm: 15.890821 | Time: 10s219ms
Epoch: 009 | Test Loss: 0.1230168 | Time: 781ms
Epoch: 010 | Train Loss: 0.1194795 | Grad norm: 16.062489 | Time: 11s499ms
Epoch: 010 | Test Loss: 0.0577890 | Time: 817ms
Epoch: 011 | Train Loss: 0.0951381 | Grad norm: 15.076774 | Time: 11s196ms
Epoch: 011 | Test Loss: 0.0831004 | Time: 823ms
Epoch: 012 | Train Loss: 0.0903950 | Grad norm: 13.038550 | Time: 9s19ms
Epoch: 012 | Test Loss: 0.0764141 | Time: 815ms
Epoch: 013 | Train Loss: 0.0802116 | Grad norm: 12.198912 | Time: 10s104ms
Epoch: 013 | Test Loss: 0.0694729 | Time: 812ms
Epoch: 014 | Train Loss: 0.0655946 | Grad norm: 10.957348 | Time: 9s601ms
Epoch: 014 | Test Loss: 0.0532572 | Time: 780ms
Epoch: 015 | Train Loss: 0.0562790 | Grad norm: 10.165969 | Time: 8s906ms
Epoch: 015 | Test Loss: 0.0467450 | Time: 779ms
Epoch: 016 | Train Loss: 0.0513882 | Grad norm: 9.245630 | Time: 9s295ms
Epoch: 016 | Test Loss: 0.0530297 | Time: 782ms
Epoch: 017 | Train Loss: 0.0507901 | Grad norm: 8.972802 | Time: 8s884ms
Epoch: 017 | Test Loss: 0.0346009 | Time: 775ms
Epoch: 018 | Train Loss: 0.0391908 | Grad norm: 8.295870 | Time: 11s491ms
Epoch: 018 | Test Loss: 0.0408878 | Time: 789ms
Epoch: 019 | Train Loss: 0.0341774 | Grad norm: 7.747950 | Time: 10s915ms
Epoch: 019 | Test Loss: 0.0324514 | Time: 819ms
Epoch: 020 | Train Loss: 0.0322762 | Grad norm: 7.265064 | Time: 11s97ms
Epoch: 020 | Test Loss: 0.0343113 | Time: 802ms
Epoch: 021 | Train Loss: 0.0334196 | Grad norm: 6.785579 | Time: 11s460ms
Epoch: 021 | Test Loss: 0.0282240 | Time: 859ms
Epoch: 022 | Train Loss: 0.0285585 | Grad norm: 6.665018 | Time: 11s622ms
Epoch: 022 | Test Loss: 0.0221078 | Time: 872ms
==> Save the model at epoch 022 with test loss 0.0221078
Epoch: 023 | Train Loss: 0.0332074 | Grad norm: 5.741787 | Time: 10s251ms
Epoch: 023 | Test Loss: 0.0231973 | Time: 769ms
Epoch: 024 | Train Loss: 0.0263388 | Grad norm: 5.980793 | Time: 11s113ms
Epoch: 024 | Test Loss: 0.0200272 | Time: 807ms
==> Save the model at epoch 024 with test loss 0.0200272
Epoch: 025 | Train Loss: 0.0221338 | Grad norm: 5.813448 | Time: 11s728ms
Epoch: 025 | Test Loss: 0.0183393 | Time: 1s47ms
==> Save the model at epoch 025 with test loss 0.0183393
Epoch: 026 | Train Loss: 0.0256967 | Grad norm: 5.805534 | Time: 10s603ms
Epoch: 026 | Test Loss: 0.0156298 | Time: 817ms
==> Save the model at epoch 026 with test loss 0.0156298
Epoch: 027 | Train Loss: 0.0210989 | Grad norm: 5.134908 | Time: 10s402ms
Epoch: 027 | Test Loss: 0.0165099 | Time: 858ms
Epoch: 028 | Train Loss: 0.0186577 | Grad norm: 4.956211 | Time: 10s237ms
Epoch: 028 | Test Loss: 0.0169284 | Time: 783ms
Epoch: 029 | Train Loss: 0.0230956 | Grad norm: 4.489146 | Time: 11s16ms
Epoch: 029 | Test Loss: 0.0376921 | Time: 780ms
Epoch: 030 | Train Loss: 0.0196532 | Grad norm: 4.151490 | Time: 10s285ms
Epoch: 030 | Test Loss: 0.0172757 | Time: 773ms
Epoch: 031 | Train Loss: 0.0158700 | Grad norm: 4.328610 | Time: 10s256ms
Epoch: 031 | Test Loss: 0.0355014 | Time: 783ms
Epoch: 032 | Train Loss: 0.0161695 | Grad norm: 3.951741 | Time: 10s240ms
Epoch: 032 | Test Loss: 0.0142601 | Time: 854ms
==> Save the model at epoch 032 with test loss 0.0142601
Epoch: 033 | Train Loss: 0.0139463 | Grad norm: 3.753305 | Time: 10s207ms
Epoch: 033 | Test Loss: 0.0140055 | Time: 784ms
==> Save the model at epoch 033 with test loss 0.0140055
Epoch: 034 | Train Loss: 0.0137598 | Grad norm: 3.712400 | Time: 10s257ms
Epoch: 034 | Test Loss: 0.0147206 | Time: 791ms
Epoch: 035 | Train Loss: 0.0149835 | Grad norm: 3.778771 | Time: 10s529ms
Epoch: 035 | Test Loss: 0.0110997 | Time: 784ms
==> Save the model at epoch 035 with test loss 0.0110997
Epoch: 036 | Train Loss: 0.0131893 | Grad norm: 3.253379 | Time: 10s229ms
Epoch: 036 | Test Loss: 0.0145563 | Time: 782ms
Epoch: 037 | Train Loss: 0.0117366 | Grad norm: 2.940468 | Time: 10s587ms
Epoch: 037 | Test Loss: 0.0087792 | Time: 842ms
==> Save the model at epoch 037 with test loss 0.0087792
Epoch: 038 | Train Loss: 0.0115817 | Grad norm: 2.627340 | Time: 10s994ms
Epoch: 038 | Test Loss: 0.0112600 | Time: 822ms
Epoch: 039 | Train Loss: 0.0117477 | Grad norm: 2.359173 | Time: 9s563ms
Epoch: 039 | Test Loss: 0.0091652 | Time: 813ms
Epoch: 040 | Train Loss: 0.0107527 | Grad norm: 2.220917 | Time: 10s478ms
Epoch: 040 | Test Loss: 0.0186640 | Time: 817ms
Epoch: 041 | Train Loss: 0.0140012 | Grad norm: 3.982098 | Time: 10s904ms
Epoch: 041 | Test Loss: 0.0106338 | Time: 808ms
Epoch: 042 | Train Loss: 0.0113951 | Grad norm: 2.775148 | Time: 8s851ms
Epoch: 042 | Test Loss: 0.0087886 | Time: 838ms
Epoch: 043 | Train Loss: 0.0093291 | Grad norm: 1.656967 | Time: 8s965ms
Epoch: 043 | Test Loss: 0.0100882 | Time: 782ms
Epoch: 044 | Train Loss: 0.0094143 | Grad norm: 1.701528 | Time: 10s155ms
Epoch: 044 | Test Loss: 0.0095883 | Time: 792ms
Epoch: 045 | Train Loss: 0.0087445 | Grad norm: 1.274601 | Time: 10s904ms
Epoch: 045 | Test Loss: 0.0086824 | Time: 780ms
==> Save the model at epoch 045 with test loss 0.0086824
Epoch: 046 | Train Loss: 0.0085654 | Grad norm: 1.367065 | Time: 10s768ms
Epoch: 046 | Test Loss: 0.0087923 | Time: 812ms
Epoch: 047 | Train Loss: 0.0085009 | Grad norm: 1.327209 | Time: 10s476ms
Epoch: 047 | Test Loss: 0.0085264 | Time: 850ms
==> Save the model at epoch 047 with test loss 0.0085264
Epoch: 048 | Train Loss: 0.0081457 | Grad norm: 1.071898 | Time: 10s472ms
Epoch: 048 | Test Loss: 0.0085672 | Time: 793ms
Epoch: 049 | Train Loss: 0.0078595 | Grad norm: 0.940760 | Time: 10s664ms
Epoch: 049 | Test Loss: 0.0078711 | Time: 792ms
==> Save the model at epoch 049 with test loss 0.0078711
Epoch: 050 | Train Loss: 0.0078540 | Grad norm: 0.956442 | Time: 10s501ms
Epoch: 050 | Test Loss: 0.0078701 | Time: 796ms
==> Save the model at epoch 050 with test loss 0.0078701
Epoch: 051 | Train Loss: 0.0077109 | Grad norm: 0.888185 | Time: 10s445ms
Epoch: 051 | Test Loss: 0.0077145 | Time: 792ms
==> Save the model at epoch 051 with test loss 0.0077145
Epoch: 052 | Train Loss: 0.0076035 | Grad norm: 0.832284 | Time: 10s473ms
Epoch: 052 | Test Loss: 0.0075585 | Time: 853ms
==> Save the model at epoch 052 with test loss 0.0075585
Epoch: 053 | Train Loss: 0.0075596 | Grad norm: 0.846167 | Time: 10s502ms
Epoch: 053 | Test Loss: 0.0074861 | Time: 764ms
==> Save the model at epoch 053 with test loss 0.0074861
Epoch: 054 | Train Loss: 0.0073787 | Grad norm: 0.650545 | Time: 9s37ms
Epoch: 054 | Test Loss: 0.0073914 | Time: 812ms
==> Save the model at epoch 054 with test loss 0.0073914
Epoch: 055 | Train Loss: 0.0073898 | Grad norm: 0.692806 | Time: 9s341ms
Epoch: 055 | Test Loss: 0.0073768 | Time: 782ms
==> Save the model at epoch 055 with test loss 0.0073768
Epoch: 056 | Train Loss: 0.0072387 | Grad norm: 0.486746 | Time: 8s985ms
Epoch: 056 | Test Loss: 0.0072229 | Time: 816ms
==> Save the model at epoch 056 with test loss 0.0072229
Epoch: 057 | Train Loss: 0.0071981 | Grad norm: 0.469941 | Time: 10s813ms
Epoch: 057 | Test Loss: 0.0073148 | Time: 828ms
Epoch: 058 | Train Loss: 0.0071627 | Grad norm: 0.437009 | Time: 11s688ms
Epoch: 058 | Test Loss: 0.0072386 | Time: 873ms
Epoch: 059 | Train Loss: 0.0071255 | Grad norm: 0.386169 | Time: 11s791ms
Epoch: 059 | Test Loss: 0.0071949 | Time: 803ms
==> Save the model at epoch 059 with test loss 0.0071949
Epoch: 060 | Train Loss: 0.0071049 | Grad norm: 0.375303 | Time: 12s35ms
Epoch: 060 | Test Loss: 0.0071847 | Time: 809ms
==> Save the model at epoch 060 with test loss 0.0071847
Total time: 11m15s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
