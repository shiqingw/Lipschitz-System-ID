==> torch device:  cuda:3
==> Lipschitz constant: 8.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (M): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.50
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 31.5424155 | Grad norm: 77.909774 | Time: 10s284ms
Epoch: 001 | Test Loss: 31.5610125 | Time: 850ms
==> Save the model at epoch 001 with test loss 31.5610125
Epoch: 002 | Train Loss: 2.4809925 | Grad norm: 16.153676 | Time: 9s8ms
Epoch: 002 | Test Loss: 0.0136645 | Time: 836ms
==> Save the model at epoch 002 with test loss 0.0136645
Epoch: 003 | Train Loss: 0.0243504 | Grad norm: 9.316762 | Time: 10s881ms
Epoch: 003 | Test Loss: 0.0206821 | Time: 825ms
Epoch: 004 | Train Loss: 0.0296207 | Grad norm: 10.949932 | Time: 9s854ms
Epoch: 004 | Test Loss: 0.0290455 | Time: 800ms
Epoch: 005 | Train Loss: 0.0395389 | Grad norm: 11.525863 | Time: 9s502ms
Epoch: 005 | Test Loss: 0.0285481 | Time: 801ms
Epoch: 006 | Train Loss: 0.0466025 | Grad norm: 12.957738 | Time: 10s83ms
Epoch: 006 | Test Loss: 0.0494636 | Time: 817ms
Epoch: 007 | Train Loss: 0.0596836 | Grad norm: 14.369418 | Time: 10s761ms
Epoch: 007 | Test Loss: 0.0416852 | Time: 825ms
Epoch: 008 | Train Loss: 0.0714615 | Grad norm: 14.233729 | Time: 10s186ms
Epoch: 008 | Test Loss: 0.0631859 | Time: 842ms
Epoch: 009 | Train Loss: 0.0767554 | Grad norm: 15.160735 | Time: 10s115ms
Epoch: 009 | Test Loss: 0.0691150 | Time: 829ms
Epoch: 010 | Train Loss: 0.0558919 | Grad norm: 13.029854 | Time: 10s165ms
Epoch: 010 | Test Loss: 0.0493614 | Time: 844ms
Epoch: 011 | Train Loss: 0.0503554 | Grad norm: 11.669737 | Time: 10s978ms
Epoch: 011 | Test Loss: 0.0471692 | Time: 800ms
Epoch: 012 | Train Loss: 0.0508574 | Grad norm: 10.383180 | Time: 9s453ms
Epoch: 012 | Test Loss: 0.0372202 | Time: 827ms
Epoch: 013 | Train Loss: 0.0396293 | Grad norm: 9.993390 | Time: 9s166ms
Epoch: 013 | Test Loss: 0.0462335 | Time: 807ms
Epoch: 014 | Train Loss: 0.0383162 | Grad norm: 8.952431 | Time: 10s218ms
Epoch: 014 | Test Loss: 0.0268973 | Time: 807ms
Epoch: 015 | Train Loss: 0.0349800 | Grad norm: 8.558468 | Time: 10s251ms
Epoch: 015 | Test Loss: 0.0338037 | Time: 806ms
Epoch: 016 | Train Loss: 0.0298127 | Grad norm: 7.735535 | Time: 8s794ms
Epoch: 016 | Test Loss: 0.0231713 | Time: 813ms
Epoch: 017 | Train Loss: 0.0353209 | Grad norm: 7.410924 | Time: 8s816ms
Epoch: 017 | Test Loss: 0.0375529 | Time: 843ms
Epoch: 018 | Train Loss: 0.0266147 | Grad norm: 6.910629 | Time: 8s848ms
Epoch: 018 | Test Loss: 0.0279196 | Time: 843ms
Epoch: 019 | Train Loss: 0.0229572 | Grad norm: 6.389100 | Time: 9s997ms
Epoch: 019 | Test Loss: 0.0285725 | Time: 839ms
Epoch: 020 | Train Loss: 0.0229734 | Grad norm: 5.549439 | Time: 10s797ms
Epoch: 020 | Test Loss: 0.0166449 | Time: 805ms
Epoch: 021 | Train Loss: 0.0208854 | Grad norm: 5.488378 | Time: 8s690ms
Epoch: 021 | Test Loss: 0.0218551 | Time: 838ms
Epoch: 022 | Train Loss: 0.0196893 | Grad norm: 4.775043 | Time: 8s636ms
Epoch: 022 | Test Loss: 0.0216645 | Time: 863ms
Epoch: 023 | Train Loss: 0.0161290 | Grad norm: 4.511530 | Time: 8s519ms
Epoch: 023 | Test Loss: 0.0116685 | Time: 833ms
==> Save the model at epoch 023 with test loss 0.0116685
Epoch: 024 | Train Loss: 0.0150312 | Grad norm: 4.179295 | Time: 8s674ms
Epoch: 024 | Test Loss: 0.0141948 | Time: 796ms
Epoch: 025 | Train Loss: 0.0140550 | Grad norm: 3.782830 | Time: 10s932ms
Epoch: 025 | Test Loss: 0.0158633 | Time: 810ms
Epoch: 026 | Train Loss: 0.0135371 | Grad norm: 3.393558 | Time: 9s76ms
Epoch: 026 | Test Loss: 0.0107238 | Time: 800ms
==> Save the model at epoch 026 with test loss 0.0107238
Epoch: 027 | Train Loss: 0.0121832 | Grad norm: 3.096490 | Time: 9s22ms
Epoch: 027 | Test Loss: 0.0144944 | Time: 862ms
Epoch: 028 | Train Loss: 0.0105603 | Grad norm: 2.563445 | Time: 10s425ms
Epoch: 028 | Test Loss: 0.0092308 | Time: 809ms
==> Save the model at epoch 028 with test loss 0.0092308
Epoch: 029 | Train Loss: 0.0107220 | Grad norm: 2.312076 | Time: 10s110ms
Epoch: 029 | Test Loss: 0.0100880 | Time: 809ms
Epoch: 030 | Train Loss: 0.0096669 | Grad norm: 1.939732 | Time: 10s127ms
Epoch: 030 | Test Loss: 0.0103268 | Time: 805ms
Epoch: 031 | Train Loss: 0.0094936 | Grad norm: 1.853754 | Time: 11s244ms
Epoch: 031 | Test Loss: 0.0085505 | Time: 805ms
==> Save the model at epoch 031 with test loss 0.0085505
Epoch: 032 | Train Loss: 0.0086340 | Grad norm: 1.368832 | Time: 10s737ms
Epoch: 032 | Test Loss: 0.0077868 | Time: 893ms
==> Save the model at epoch 032 with test loss 0.0077868
Epoch: 033 | Train Loss: 0.0084518 | Grad norm: 1.354916 | Time: 11s121ms
Epoch: 033 | Test Loss: 0.0081925 | Time: 809ms
Epoch: 034 | Train Loss: 0.0081027 | Grad norm: 1.199271 | Time: 10s226ms
Epoch: 034 | Test Loss: 0.0085506 | Time: 815ms
Epoch: 035 | Train Loss: 0.0078041 | Grad norm: 1.006227 | Time: 11s199ms
Epoch: 035 | Test Loss: 0.0077876 | Time: 806ms
Epoch: 036 | Train Loss: 0.0075823 | Grad norm: 0.876105 | Time: 10s116ms
Epoch: 036 | Test Loss: 0.0075432 | Time: 806ms
==> Save the model at epoch 036 with test loss 0.0075432
Epoch: 037 | Train Loss: 0.0073870 | Grad norm: 0.686654 | Time: 10s575ms
Epoch: 037 | Test Loss: 0.0075539 | Time: 921ms
Epoch: 038 | Train Loss: 0.0072598 | Grad norm: 0.579773 | Time: 11s74ms
Epoch: 038 | Test Loss: 0.0073657 | Time: 798ms
==> Save the model at epoch 038 with test loss 0.0073657
Epoch: 039 | Train Loss: 0.0071841 | Grad norm: 0.514475 | Time: 10s505ms
Epoch: 039 | Test Loss: 0.0072672 | Time: 823ms
==> Save the model at epoch 039 with test loss 0.0072672
Epoch: 040 | Train Loss: 0.0071283 | Grad norm: 0.432206 | Time: 9s185ms
Epoch: 040 | Test Loss: 0.0072233 | Time: 829ms
==> Save the model at epoch 040 with test loss 0.0072233
Total time: 7m11s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
