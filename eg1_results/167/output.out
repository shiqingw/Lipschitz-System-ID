==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.51668906 0.51668906]
==> Ouput transform to be applied to the neural network:
[3.8759 3.8759]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─ReLU: 2-2                         [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─ReLU: 2-4                         [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─ReLU: 2-6                         [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─ReLU: 2-8                         [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─ReLU: 2-10                        [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─ReLU: 2-12                        [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─ReLU: 2-14                        [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─Linear: 2-16                      [1, 64]                   (recursive)
│    └─ReLU: 2-17                        [1, 64]                   --
│    └─Linear: 2-18                      [1, 64]                   (recursive)
│    └─ReLU: 2-19                        [1, 64]                   --
│    └─Linear: 2-20                      [1, 64]                   (recursive)
│    └─ReLU: 2-21                        [1, 64]                   --
│    └─Linear: 2-22                      [1, 64]                   (recursive)
│    └─ReLU: 2-23                        [1, 64]                   --
│    └─Linear: 2-24                      [1, 64]                   (recursive)
│    └─ReLU: 2-25                        [1, 64]                   --
│    └─Linear: 2-26                      [1, 64]                   (recursive)
│    └─ReLU: 2-27                        [1, 64]                   --
│    └─Linear: 2-28                      [1, 64]                   (recursive)
│    └─ReLU: 2-29                        [1, 64]                   --
│    └─Linear: 2-30                      [1, 2]                    (recursive)
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.05
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.10
Estimated Total Size (MB): 0.11
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.1190481 | Grad norm: 1.185523 | Time: 4s255ms
Epoch: 001 | Test Loss: 29.8928187 | Time: 456ms
==> Save the model at epoch 001 with test loss 29.8928187
Epoch: 002 | Train Loss: 7.0113123 | Grad norm: 4.373581 | Time: 4s99ms
Epoch: 002 | Test Loss: 0.0585186 | Time: 491ms
==> Save the model at epoch 002 with test loss 0.0585186
Epoch: 003 | Train Loss: 0.0958608 | Grad norm: 3.627586 | Time: 4s337ms
Epoch: 003 | Test Loss: 0.0835359 | Time: 479ms
Epoch: 004 | Train Loss: 0.1490068 | Grad norm: 4.772413 | Time: 4s305ms
Epoch: 004 | Test Loss: 0.1676087 | Time: 478ms
Epoch: 005 | Train Loss: 0.2508270 | Grad norm: 6.514810 | Time: 4s278ms
Epoch: 005 | Test Loss: 0.2401619 | Time: 464ms
Epoch: 006 | Train Loss: 0.2794535 | Grad norm: 6.734496 | Time: 3s912ms
Epoch: 006 | Test Loss: 0.1628651 | Time: 554ms
Epoch: 007 | Train Loss: 0.2949972 | Grad norm: 7.028481 | Time: 4s200ms
Epoch: 007 | Test Loss: 0.1466016 | Time: 464ms
Epoch: 008 | Train Loss: 0.3050983 | Grad norm: 7.121032 | Time: 4s209ms
Epoch: 008 | Test Loss: 0.3487309 | Time: 464ms
Epoch: 009 | Train Loss: 0.2995182 | Grad norm: 7.040660 | Time: 4s35ms
Epoch: 009 | Test Loss: 0.3224940 | Time: 488ms
Epoch: 010 | Train Loss: 0.2408196 | Grad norm: 6.132253 | Time: 4s91ms
Epoch: 010 | Test Loss: 0.2318394 | Time: 483ms
Epoch: 011 | Train Loss: 0.2509299 | Grad norm: 6.452402 | Time: 4s265ms
Epoch: 011 | Test Loss: 0.2483791 | Time: 465ms
Epoch: 012 | Train Loss: 0.1986499 | Grad norm: 5.509105 | Time: 4s179ms
Epoch: 012 | Test Loss: 0.1967232 | Time: 461ms
Epoch: 013 | Train Loss: 0.1925541 | Grad norm: 5.450837 | Time: 4s254ms
Epoch: 013 | Test Loss: 0.2141437 | Time: 462ms
Epoch: 014 | Train Loss: 0.1359347 | Grad norm: 4.351697 | Time: 4s122ms
Epoch: 014 | Test Loss: 0.1114227 | Time: 531ms
Epoch: 015 | Train Loss: 0.1301901 | Grad norm: 4.316220 | Time: 4s239ms
Epoch: 015 | Test Loss: 0.0747980 | Time: 522ms
Epoch: 016 | Train Loss: 0.1139143 | Grad norm: 3.967829 | Time: 4s177ms
Epoch: 016 | Test Loss: 0.0891942 | Time: 467ms
Epoch: 017 | Train Loss: 0.1211790 | Grad norm: 4.185057 | Time: 4s208ms
Epoch: 017 | Test Loss: 0.1193287 | Time: 486ms
Epoch: 018 | Train Loss: 0.1032611 | Grad norm: 3.717148 | Time: 4s138ms
Epoch: 018 | Test Loss: 0.1289671 | Time: 460ms
Epoch: 019 | Train Loss: 0.0945929 | Grad norm: 3.544273 | Time: 4s290ms
Epoch: 019 | Test Loss: 0.1303219 | Time: 465ms
Epoch: 020 | Train Loss: 0.0829902 | Grad norm: 3.277627 | Time: 4s36ms
Epoch: 020 | Test Loss: 0.1101675 | Time: 474ms
Epoch: 021 | Train Loss: 0.0799257 | Grad norm: 3.176624 | Time: 4s268ms
Epoch: 021 | Test Loss: 0.0754768 | Time: 466ms
Epoch: 022 | Train Loss: 0.0707694 | Grad norm: 2.951701 | Time: 4s135ms
Epoch: 022 | Test Loss: 0.0484063 | Time: 636ms
==> Save the model at epoch 022 with test loss 0.0484063
Epoch: 023 | Train Loss: 0.0557076 | Grad norm: 2.483019 | Time: 3s850ms
Epoch: 023 | Test Loss: 0.0604887 | Time: 469ms
Epoch: 024 | Train Loss: 0.0553607 | Grad norm: 2.506348 | Time: 4s148ms
Epoch: 024 | Test Loss: 0.1062436 | Time: 470ms
Epoch: 025 | Train Loss: 0.0536971 | Grad norm: 2.418890 | Time: 4s92ms
Epoch: 025 | Test Loss: 0.0281831 | Time: 575ms
==> Save the model at epoch 025 with test loss 0.0281831
Epoch: 026 | Train Loss: 0.0402058 | Grad norm: 1.966891 | Time: 3s902ms
Epoch: 026 | Test Loss: 0.0231234 | Time: 466ms
==> Save the model at epoch 026 with test loss 0.0231234
Epoch: 027 | Train Loss: 0.0367804 | Grad norm: 1.875351 | Time: 4s299ms
Epoch: 027 | Test Loss: 0.0393921 | Time: 461ms
Epoch: 028 | Train Loss: 0.0346901 | Grad norm: 1.779784 | Time: 4s258ms
Epoch: 028 | Test Loss: 0.0347792 | Time: 467ms
Epoch: 029 | Train Loss: 0.0299917 | Grad norm: 1.554663 | Time: 4s190ms
Epoch: 029 | Test Loss: 0.0250701 | Time: 477ms
Epoch: 030 | Train Loss: 0.0248787 | Grad norm: 1.346920 | Time: 4s217ms
Epoch: 030 | Test Loss: 0.0263020 | Time: 479ms
Epoch: 031 | Train Loss: 0.0240662 | Grad norm: 1.270069 | Time: 4s225ms
Epoch: 031 | Test Loss: 0.0169776 | Time: 483ms
==> Save the model at epoch 031 with test loss 0.0169776
Epoch: 032 | Train Loss: 0.0204365 | Grad norm: 1.086003 | Time: 4s116ms
Epoch: 032 | Test Loss: 0.0209001 | Time: 523ms
Epoch: 033 | Train Loss: 0.0196734 | Grad norm: 1.023602 | Time: 3s993ms
Epoch: 033 | Test Loss: 0.0173492 | Time: 534ms
Epoch: 034 | Train Loss: 0.0190350 | Grad norm: 0.932912 | Time: 4s140ms
Epoch: 034 | Test Loss: 0.0168169 | Time: 452ms
==> Save the model at epoch 034 with test loss 0.0168169
Epoch: 035 | Train Loss: 0.0177998 | Grad norm: 0.860696 | Time: 4s74ms
Epoch: 035 | Test Loss: 0.0163658 | Time: 460ms
==> Save the model at epoch 035 with test loss 0.0163658
Epoch: 036 | Train Loss: 0.0164496 | Grad norm: 0.757191 | Time: 4s155ms
Epoch: 036 | Test Loss: 0.0138798 | Time: 475ms
==> Save the model at epoch 036 with test loss 0.0138798
Epoch: 037 | Train Loss: 0.0160189 | Grad norm: 0.695735 | Time: 4s169ms
Epoch: 037 | Test Loss: 0.0150771 | Time: 478ms
Epoch: 038 | Train Loss: 0.0162685 | Grad norm: 0.682687 | Time: 4s22ms
Epoch: 038 | Test Loss: 0.0138435 | Time: 456ms
==> Save the model at epoch 038 with test loss 0.0138435
Epoch: 039 | Train Loss: 0.0153241 | Grad norm: 0.623851 | Time: 4s27ms
Epoch: 039 | Test Loss: 0.0151251 | Time: 458ms
Epoch: 040 | Train Loss: 0.0159507 | Grad norm: 0.595503 | Time: 4s149ms
Epoch: 040 | Test Loss: 0.0141598 | Time: 461ms
Total time: 3m5s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.51668906 0.51668906]
==> Output transform to be applied to the neural network (trained):
[3.8759 3.8759]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
