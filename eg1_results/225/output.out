==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 14.5926202 | L2 loss: 14.5904305 | Lip loss: 0.0021896 | Grad norm: 3.445396 | Time: 17s216ms
Epoch: 001 | Test Loss: 9.1980455 | Time: 462ms
==> Save the model at epoch 001 with test loss 9.1980455
Epoch: 002 | Loss: 1.6576603 | L2 loss: 1.6354040 | Lip loss: 0.0222563 | Grad norm: 5.405607 | Time: 17s1ms
Epoch: 002 | Test Loss: 0.0118686 | Time: 433ms
==> Save the model at epoch 002 with test loss 0.0118686
Epoch: 003 | Loss: 0.0301344 | L2 loss: 0.0091286 | Lip loss: 0.0210058 | Grad norm: 1.292349 | Time: 17s80ms
Epoch: 003 | Test Loss: 0.0080217 | Time: 587ms
==> Save the model at epoch 003 with test loss 0.0080217
Epoch: 004 | Loss: 0.0284496 | L2 loss: 0.0076222 | Lip loss: 0.0208275 | Grad norm: 1.138166 | Time: 17s33ms
Epoch: 004 | Test Loss: 0.0075108 | Time: 431ms
==> Save the model at epoch 004 with test loss 0.0075108
Epoch: 005 | Loss: 0.0277591 | L2 loss: 0.0069950 | Lip loss: 0.0207641 | Grad norm: 0.951078 | Time: 16s668ms
Epoch: 005 | Test Loss: 0.0077654 | Time: 446ms
Epoch: 006 | Loss: 0.0271201 | L2 loss: 0.0064275 | Lip loss: 0.0206926 | Grad norm: 0.447911 | Time: 17s79ms
Epoch: 006 | Test Loss: 0.0064156 | Time: 455ms
==> Save the model at epoch 006 with test loss 0.0064156
Epoch: 007 | Loss: 0.0270471 | L2 loss: 0.0063520 | Lip loss: 0.0206952 | Grad norm: 0.368924 | Time: 17s438ms
Epoch: 007 | Test Loss: 0.0064286 | Time: 441ms
Epoch: 008 | Loss: 0.0270201 | L2 loss: 0.0063355 | Lip loss: 0.0206846 | Grad norm: 0.415181 | Time: 17s73ms
Epoch: 008 | Test Loss: 0.0064046 | Time: 439ms
==> Save the model at epoch 008 with test loss 0.0064046
Epoch: 009 | Loss: 0.0269571 | L2 loss: 0.0063198 | Lip loss: 0.0206374 | Grad norm: 0.423352 | Time: 16s885ms
Epoch: 009 | Test Loss: 0.0063287 | Time: 436ms
==> Save the model at epoch 009 with test loss 0.0063287
Epoch: 010 | Loss: 0.0269033 | L2 loss: 0.0062679 | Lip loss: 0.0206355 | Grad norm: 0.370953 | Time: 16s906ms
Epoch: 010 | Test Loss: 0.0064485 | Time: 504ms
Epoch: 011 | Loss: 0.0268839 | L2 loss: 0.0062314 | Lip loss: 0.0206524 | Grad norm: 0.310560 | Time: 17s79ms
Epoch: 011 | Test Loss: 0.0063107 | Time: 441ms
==> Save the model at epoch 011 with test loss 0.0063107
Epoch: 012 | Loss: 0.0268299 | L2 loss: 0.0062245 | Lip loss: 0.0206053 | Grad norm: 0.319765 | Time: 17s129ms
Epoch: 012 | Test Loss: 0.0062954 | Time: 457ms
==> Save the model at epoch 012 with test loss 0.0062954
Epoch: 013 | Loss: 0.0268959 | L2 loss: 0.0062167 | Lip loss: 0.0206792 | Grad norm: 0.293689 | Time: 16s573ms
Epoch: 013 | Test Loss: 0.0062865 | Time: 437ms
==> Save the model at epoch 013 with test loss 0.0062865
Epoch: 014 | Loss: 0.0268505 | L2 loss: 0.0062174 | Lip loss: 0.0206331 | Grad norm: 0.317979 | Time: 16s852ms
Epoch: 014 | Test Loss: 0.0062832 | Time: 437ms
==> Save the model at epoch 014 with test loss 0.0062832
Epoch: 015 | Loss: 0.0268603 | L2 loss: 0.0062144 | Lip loss: 0.0206460 | Grad norm: 0.308085 | Time: 16s862ms
Epoch: 015 | Test Loss: 0.0062792 | Time: 440ms
==> Save the model at epoch 015 with test loss 0.0062792
Epoch: 016 | Loss: 0.0268764 | L2 loss: 0.0062068 | Lip loss: 0.0206696 | Grad norm: 0.302328 | Time: 17s230ms
Epoch: 016 | Test Loss: 0.0062803 | Time: 437ms
Epoch: 017 | Loss: 0.0268490 | L2 loss: 0.0062065 | Lip loss: 0.0206425 | Grad norm: 0.283417 | Time: 17s55ms
Epoch: 017 | Test Loss: 0.0062798 | Time: 517ms
Epoch: 018 | Loss: 0.0268645 | L2 loss: 0.0062050 | Lip loss: 0.0206595 | Grad norm: 0.291819 | Time: 17s225ms
Epoch: 018 | Test Loss: 0.0062815 | Time: 451ms
Epoch: 019 | Loss: 0.0268774 | L2 loss: 0.0062067 | Lip loss: 0.0206707 | Grad norm: 0.299829 | Time: 17s44ms
Epoch: 019 | Test Loss: 0.0062834 | Time: 444ms
Epoch: 020 | Loss: 0.0268606 | L2 loss: 0.0062088 | Lip loss: 0.0206517 | Grad norm: 0.304866 | Time: 17s
Epoch: 020 | Test Loss: 0.0062798 | Time: 440ms
Epoch: 021 | Loss: 0.0268782 | L2 loss: 0.0062048 | Lip loss: 0.0206734 | Grad norm: 0.299505 | Time: 17s54ms
Epoch: 021 | Test Loss: 0.0062800 | Time: 431ms
Epoch: 022 | Loss: 0.0268867 | L2 loss: 0.0062050 | Lip loss: 0.0206817 | Grad norm: 0.296261 | Time: 17s348ms
Epoch: 022 | Test Loss: 0.0062802 | Time: 448ms
Epoch: 023 | Loss: 0.0268599 | L2 loss: 0.0062041 | Lip loss: 0.0206558 | Grad norm: 0.281644 | Time: 17s355ms
Epoch: 023 | Test Loss: 0.0062801 | Time: 449ms
Epoch: 024 | Loss: 0.0268393 | L2 loss: 0.0062049 | Lip loss: 0.0206344 | Grad norm: 0.290742 | Time: 17s779ms
Epoch: 024 | Test Loss: 0.0062800 | Time: 517ms
Epoch: 025 | Loss: 0.0268281 | L2 loss: 0.0062055 | Lip loss: 0.0206226 | Grad norm: 0.280139 | Time: 17s290ms
Epoch: 025 | Test Loss: 0.0062803 | Time: 437ms
Epoch: 026 | Loss: 0.0268900 | L2 loss: 0.0062049 | Lip loss: 0.0206850 | Grad norm: 0.286330 | Time: 17s245ms
Epoch: 026 | Test Loss: 0.0062803 | Time: 447ms
Epoch: 027 | Loss: 0.0269050 | L2 loss: 0.0062059 | Lip loss: 0.0206991 | Grad norm: 0.297566 | Time: 17s36ms
Epoch: 027 | Test Loss: 0.0062803 | Time: 444ms
Epoch: 028 | Loss: 0.0269119 | L2 loss: 0.0062066 | Lip loss: 0.0207053 | Grad norm: 0.300315 | Time: 17s450ms
Epoch: 028 | Test Loss: 0.0062803 | Time: 434ms
Epoch: 029 | Loss: 0.0268494 | L2 loss: 0.0062027 | Lip loss: 0.0206467 | Grad norm: 0.289101 | Time: 17s163ms
Epoch: 029 | Test Loss: 0.0062803 | Time: 440ms
Epoch: 030 | Loss: 0.0268642 | L2 loss: 0.0062058 | Lip loss: 0.0206585 | Grad norm: 0.287562 | Time: 17s738ms
Epoch: 030 | Test Loss: 0.0062803 | Time: 445ms
Epoch: 031 | Loss: 0.0268171 | L2 loss: 0.0062087 | Lip loss: 0.0206084 | Grad norm: 0.281005 | Time: 16s731ms
Epoch: 031 | Test Loss: 0.0062803 | Time: 439ms
Epoch: 032 | Loss: 0.0268650 | L2 loss: 0.0062061 | Lip loss: 0.0206589 | Grad norm: 0.289406 | Time: 16s700ms
Epoch: 032 | Test Loss: 0.0062803 | Time: 453ms
Epoch: 033 | Loss: 0.0268300 | L2 loss: 0.0062073 | Lip loss: 0.0206227 | Grad norm: 0.306755 | Time: 17s13ms
Epoch: 033 | Test Loss: 0.0062803 | Time: 444ms
Epoch: 034 | Loss: 0.0269163 | L2 loss: 0.0062061 | Lip loss: 0.0207102 | Grad norm: 0.295733 | Time: 16s775ms
Epoch: 034 | Test Loss: 0.0062803 | Time: 445ms
Epoch: 035 | Loss: 0.0268437 | L2 loss: 0.0062034 | Lip loss: 0.0206403 | Grad norm: 0.301599 | Time: 17s332ms
Epoch: 035 | Test Loss: 0.0062803 | Time: 450ms
Epoch: 036 | Loss: 0.0268588 | L2 loss: 0.0062084 | Lip loss: 0.0206504 | Grad norm: 0.303245 | Time: 16s975ms
Epoch: 036 | Test Loss: 0.0062803 | Time: 516ms
Epoch: 037 | Loss: 0.0268941 | L2 loss: 0.0062077 | Lip loss: 0.0206864 | Grad norm: 0.286369 | Time: 17s588ms
Epoch: 037 | Test Loss: 0.0062803 | Time: 440ms
Epoch: 038 | Loss: 0.0268245 | L2 loss: 0.0062019 | Lip loss: 0.0206226 | Grad norm: 0.298260 | Time: 17s515ms
Epoch: 038 | Test Loss: 0.0062803 | Time: 443ms
Epoch: 039 | Loss: 0.0269203 | L2 loss: 0.0062027 | Lip loss: 0.0207177 | Grad norm: 0.308806 | Time: 16s973ms
Epoch: 039 | Test Loss: 0.0062803 | Time: 434ms
Epoch: 040 | Loss: 0.0268839 | L2 loss: 0.0062069 | Lip loss: 0.0206770 | Grad norm: 0.292833 | Time: 16s919ms
Epoch: 040 | Test Loss: 0.0062803 | Time: 451ms
Total time: 11m42s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
