==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Lipschitz constant: 1.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (M): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.8361953 | Grad norm: 10.644180 | Time: 4s939ms
Epoch: 001 | Test Loss: 30.7439608 | Time: 785ms
==> Save the model at epoch 001 with test loss 30.7439608
Epoch: 002 | Train Loss: 16.1938142 | Grad norm: 18.915752 | Time: 4s331ms
Epoch: 002 | Test Loss: 6.5233522 | Time: 806ms
==> Save the model at epoch 002 with test loss 6.5233522
Epoch: 003 | Train Loss: 4.9995469 | Grad norm: 12.541897 | Time: 4s263ms
Epoch: 003 | Test Loss: 3.9954730 | Time: 779ms
==> Save the model at epoch 003 with test loss 3.9954730
Epoch: 004 | Train Loss: 3.1024344 | Grad norm: 9.294176 | Time: 5s376ms
Epoch: 004 | Test Loss: 2.3496501 | Time: 790ms
==> Save the model at epoch 004 with test loss 2.3496501
Epoch: 005 | Train Loss: 1.7047986 | Grad norm: 7.903920 | Time: 5s112ms
Epoch: 005 | Test Loss: 1.2097655 | Time: 793ms
==> Save the model at epoch 005 with test loss 1.2097655
Epoch: 006 | Train Loss: 0.9991137 | Grad norm: 12.439890 | Time: 4s783ms
Epoch: 006 | Test Loss: 0.7977340 | Time: 804ms
==> Save the model at epoch 006 with test loss 0.7977340
Epoch: 007 | Train Loss: 0.6720417 | Grad norm: 13.048178 | Time: 4s184ms
Epoch: 007 | Test Loss: 0.5449426 | Time: 869ms
==> Save the model at epoch 007 with test loss 0.5449426
Epoch: 008 | Train Loss: 0.4586288 | Grad norm: 13.968507 | Time: 4s194ms
Epoch: 008 | Test Loss: 0.3777806 | Time: 811ms
==> Save the model at epoch 008 with test loss 0.3777806
Epoch: 009 | Train Loss: 0.3248699 | Grad norm: 14.809353 | Time: 4s829ms
Epoch: 009 | Test Loss: 0.2748814 | Time: 812ms
==> Save the model at epoch 009 with test loss 0.2748814
Epoch: 010 | Train Loss: 0.2310558 | Grad norm: 13.883509 | Time: 5s410ms
Epoch: 010 | Test Loss: 0.2066545 | Time: 822ms
==> Save the model at epoch 010 with test loss 0.2066545
Epoch: 011 | Train Loss: 0.1692223 | Grad norm: 13.027073 | Time: 4s913ms
Epoch: 011 | Test Loss: 0.1605804 | Time: 839ms
==> Save the model at epoch 011 with test loss 0.1605804
Epoch: 012 | Train Loss: 0.1280602 | Grad norm: 12.238927 | Time: 4s892ms
Epoch: 012 | Test Loss: 0.1232212 | Time: 779ms
==> Save the model at epoch 012 with test loss 0.1232212
Epoch: 013 | Train Loss: 0.1003754 | Grad norm: 11.542984 | Time: 5s192ms
Epoch: 013 | Test Loss: 0.0980617 | Time: 798ms
==> Save the model at epoch 013 with test loss 0.0980617
Epoch: 014 | Train Loss: 0.0818323 | Grad norm: 10.910718 | Time: 4s890ms
Epoch: 014 | Test Loss: 0.0799710 | Time: 804ms
==> Save the model at epoch 014 with test loss 0.0799710
Epoch: 015 | Train Loss: 0.0677757 | Grad norm: 10.218212 | Time: 4s980ms
Epoch: 015 | Test Loss: 0.0621106 | Time: 799ms
==> Save the model at epoch 015 with test loss 0.0621106
Epoch: 016 | Train Loss: 0.0574549 | Grad norm: 9.544607 | Time: 5s86ms
Epoch: 016 | Test Loss: 0.0584974 | Time: 808ms
==> Save the model at epoch 016 with test loss 0.0584974
Epoch: 017 | Train Loss: 0.0496936 | Grad norm: 8.948462 | Time: 5s81ms
Epoch: 017 | Test Loss: 0.0464095 | Time: 782ms
==> Save the model at epoch 017 with test loss 0.0464095
Epoch: 018 | Train Loss: 0.0432603 | Grad norm: 8.382115 | Time: 4s952ms
Epoch: 018 | Test Loss: 0.0454132 | Time: 781ms
==> Save the model at epoch 018 with test loss 0.0454132
Epoch: 019 | Train Loss: 0.0377376 | Grad norm: 7.741605 | Time: 5s190ms
Epoch: 019 | Test Loss: 0.0362457 | Time: 803ms
==> Save the model at epoch 019 with test loss 0.0362457
Epoch: 020 | Train Loss: 0.0331740 | Grad norm: 7.174155 | Time: 5s323ms
Epoch: 020 | Test Loss: 0.0347192 | Time: 800ms
==> Save the model at epoch 020 with test loss 0.0347192
Epoch: 021 | Train Loss: 0.0292506 | Grad norm: 6.599675 | Time: 4s972ms
Epoch: 021 | Test Loss: 0.0299617 | Time: 800ms
==> Save the model at epoch 021 with test loss 0.0299617
Epoch: 022 | Train Loss: 0.0259635 | Grad norm: 6.074225 | Time: 4s203ms
Epoch: 022 | Test Loss: 0.0267638 | Time: 839ms
==> Save the model at epoch 022 with test loss 0.0267638
Epoch: 023 | Train Loss: 0.0231343 | Grad norm: 5.546132 | Time: 4s245ms
Epoch: 023 | Test Loss: 0.0223889 | Time: 781ms
==> Save the model at epoch 023 with test loss 0.0223889
Epoch: 024 | Train Loss: 0.0205869 | Grad norm: 5.059754 | Time: 4s950ms
Epoch: 024 | Test Loss: 0.0217231 | Time: 788ms
==> Save the model at epoch 024 with test loss 0.0217231
Epoch: 025 | Train Loss: 0.0184050 | Grad norm: 4.537625 | Time: 5s309ms
Epoch: 025 | Test Loss: 0.0172127 | Time: 791ms
==> Save the model at epoch 025 with test loss 0.0172127
Epoch: 026 | Train Loss: 0.0164771 | Grad norm: 4.083963 | Time: 5s406ms
Epoch: 026 | Test Loss: 0.0155736 | Time: 784ms
==> Save the model at epoch 026 with test loss 0.0155736
Epoch: 027 | Train Loss: 0.0148664 | Grad norm: 3.639241 | Time: 5s710ms
Epoch: 027 | Test Loss: 0.0151434 | Time: 838ms
==> Save the model at epoch 027 with test loss 0.0151434
Epoch: 028 | Train Loss: 0.0134401 | Grad norm: 3.172000 | Time: 5s118ms
Epoch: 028 | Test Loss: 0.0135211 | Time: 870ms
==> Save the model at epoch 028 with test loss 0.0135211
Epoch: 029 | Train Loss: 0.0122965 | Grad norm: 2.756721 | Time: 5s169ms
Epoch: 029 | Test Loss: 0.0123614 | Time: 787ms
==> Save the model at epoch 029 with test loss 0.0123614
Epoch: 030 | Train Loss: 0.0113934 | Grad norm: 2.400757 | Time: 4s661ms
Epoch: 030 | Test Loss: 0.0113582 | Time: 777ms
==> Save the model at epoch 030 with test loss 0.0113582
Epoch: 031 | Train Loss: 0.0105142 | Grad norm: 1.963943 | Time: 4s283ms
Epoch: 031 | Test Loss: 0.0104749 | Time: 786ms
==> Save the model at epoch 031 with test loss 0.0104749
Epoch: 032 | Train Loss: 0.0098623 | Grad norm: 1.626648 | Time: 4s379ms
Epoch: 032 | Test Loss: 0.0098835 | Time: 780ms
==> Save the model at epoch 032 with test loss 0.0098835
Epoch: 033 | Train Loss: 0.0094737 | Grad norm: 1.212609 | Time: 4s262ms
Epoch: 033 | Test Loss: 0.0087477 | Time: 840ms
==> Save the model at epoch 033 with test loss 0.0087477
Epoch: 034 | Train Loss: 0.0094008 | Grad norm: 1.258175 | Time: 4s259ms
Epoch: 034 | Test Loss: 0.0085337 | Time: 778ms
==> Save the model at epoch 034 with test loss 0.0085337
Epoch: 035 | Train Loss: 0.0087505 | Grad norm: 0.698762 | Time: 4s251ms
Epoch: 035 | Test Loss: 0.0084691 | Time: 779ms
==> Save the model at epoch 035 with test loss 0.0084691
Epoch: 036 | Train Loss: 0.0085036 | Grad norm: 0.366430 | Time: 4s317ms
Epoch: 036 | Test Loss: 0.0083868 | Time: 785ms
==> Save the model at epoch 036 with test loss 0.0083868
Epoch: 037 | Train Loss: 0.0084795 | Grad norm: 0.463348 | Time: 5s53ms
Epoch: 037 | Test Loss: 0.0083066 | Time: 861ms
==> Save the model at epoch 037 with test loss 0.0083066
Epoch: 038 | Train Loss: 0.0084125 | Grad norm: 0.361697 | Time: 5s637ms
Epoch: 038 | Test Loss: 0.0082469 | Time: 783ms
==> Save the model at epoch 038 with test loss 0.0082469
Epoch: 039 | Train Loss: 0.0083645 | Grad norm: 0.300211 | Time: 5s125ms
Epoch: 039 | Test Loss: 0.0082327 | Time: 819ms
==> Save the model at epoch 039 with test loss 0.0082327
Epoch: 040 | Train Loss: 0.0083281 | Grad norm: 0.215009 | Time: 5s372ms
Epoch: 040 | Test Loss: 0.0082274 | Time: 818ms
==> Save the model at epoch 040 with test loss 0.0082274
Total time: 3m46s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
