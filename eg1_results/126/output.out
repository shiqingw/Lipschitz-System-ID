==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.7649939 0.7649939]
==> Ouput transform to be applied to the neural network:
[2.6276 2.6276]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─ReLU: 2-2                         [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─ReLU: 2-4                         [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─ReLU: 2-6                         [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─ReLU: 2-8                         [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─ReLU: 2-10                        [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─ReLU: 2-12                        [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─ReLU: 2-14                        [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─Linear: 2-16                      [1, 64]                   (recursive)
│    └─ReLU: 2-17                        [1, 64]                   --
│    └─Linear: 2-18                      [1, 64]                   (recursive)
│    └─ReLU: 2-19                        [1, 64]                   --
│    └─Linear: 2-20                      [1, 64]                   (recursive)
│    └─ReLU: 2-21                        [1, 64]                   --
│    └─Linear: 2-22                      [1, 64]                   (recursive)
│    └─ReLU: 2-23                        [1, 64]                   --
│    └─Linear: 2-24                      [1, 64]                   (recursive)
│    └─ReLU: 2-25                        [1, 64]                   --
│    └─Linear: 2-26                      [1, 64]                   (recursive)
│    └─ReLU: 2-27                        [1, 64]                   --
│    └─Linear: 2-28                      [1, 64]                   (recursive)
│    └─ReLU: 2-29                        [1, 64]                   --
│    └─Linear: 2-30                      [1, 2]                    (recursive)
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.05
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.10
Estimated Total Size (MB): 0.11
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 13.8369480 | Grad norm: 0.243423 | Time: 2s775ms
Epoch: 001 | Test Loss: 13.7794613 | Time: 539ms
==> Save the model at epoch 001 with test loss 13.7794613
Epoch: 002 | Train Loss: 1.1314876 | Grad norm: 3.480151 | Time: 2s418ms
Epoch: 002 | Test Loss: 0.0062979 | Time: 629ms
==> Save the model at epoch 002 with test loss 0.0062979
Epoch: 003 | Train Loss: 0.0155504 | Grad norm: 1.771712 | Time: 2s405ms
Epoch: 003 | Test Loss: 0.0042604 | Time: 604ms
==> Save the model at epoch 003 with test loss 0.0042604
Epoch: 004 | Train Loss: 0.0210011 | Grad norm: 2.055568 | Time: 2s423ms
Epoch: 004 | Test Loss: 0.0235259 | Time: 601ms
Epoch: 005 | Train Loss: 0.0303905 | Grad norm: 2.319954 | Time: 2s363ms
Epoch: 005 | Test Loss: 0.0376829 | Time: 527ms
Epoch: 006 | Train Loss: 0.0382500 | Grad norm: 2.576192 | Time: 2s670ms
Epoch: 006 | Test Loss: 0.0195813 | Time: 544ms
Epoch: 007 | Train Loss: 0.0559634 | Grad norm: 3.157278 | Time: 2s500ms
Epoch: 007 | Test Loss: 0.0299125 | Time: 592ms
Epoch: 008 | Train Loss: 0.0693401 | Grad norm: 3.447189 | Time: 2s444ms
Epoch: 008 | Test Loss: 0.0515979 | Time: 548ms
Epoch: 009 | Train Loss: 0.0786949 | Grad norm: 3.727811 | Time: 2s530ms
Epoch: 009 | Test Loss: 0.0731264 | Time: 530ms
Epoch: 010 | Train Loss: 0.0502057 | Grad norm: 3.145683 | Time: 2s296ms
Epoch: 010 | Test Loss: 0.0376063 | Time: 531ms
Epoch: 011 | Train Loss: 0.0455043 | Grad norm: 2.889066 | Time: 2s523ms
Epoch: 011 | Test Loss: 0.1204122 | Time: 598ms
Epoch: 012 | Train Loss: 0.0524645 | Grad norm: 3.125675 | Time: 2s425ms
Epoch: 012 | Test Loss: 0.0306778 | Time: 557ms
Epoch: 013 | Train Loss: 0.0298227 | Grad norm: 2.356289 | Time: 2s301ms
Epoch: 013 | Test Loss: 0.0057229 | Time: 759ms
Epoch: 014 | Train Loss: 0.0316156 | Grad norm: 2.455575 | Time: 2s381ms
Epoch: 014 | Test Loss: 0.0190682 | Time: 525ms
Epoch: 015 | Train Loss: 0.0259753 | Grad norm: 2.143769 | Time: 2s486ms
Epoch: 015 | Test Loss: 0.0090564 | Time: 545ms
Epoch: 016 | Train Loss: 0.0164795 | Grad norm: 1.784915 | Time: 2s452ms
Epoch: 016 | Test Loss: 0.0063818 | Time: 560ms
Epoch: 017 | Train Loss: 0.0140814 | Grad norm: 1.585334 | Time: 2s397ms
Epoch: 017 | Test Loss: 0.0075997 | Time: 528ms
Epoch: 018 | Train Loss: 0.0113855 | Grad norm: 1.437856 | Time: 2s389ms
Epoch: 018 | Test Loss: 0.0280331 | Time: 537ms
Epoch: 019 | Train Loss: 0.0072337 | Grad norm: 1.123780 | Time: 2s223ms
Epoch: 019 | Test Loss: 0.0022329 | Time: 540ms
==> Save the model at epoch 019 with test loss 0.0022329
Epoch: 020 | Train Loss: 0.0055115 | Grad norm: 0.953869 | Time: 2s427ms
Epoch: 020 | Test Loss: 0.0044101 | Time: 610ms
Epoch: 021 | Train Loss: 0.0035652 | Grad norm: 0.740247 | Time: 2s342ms
Epoch: 021 | Test Loss: 0.0112196 | Time: 543ms
Epoch: 022 | Train Loss: 0.0040382 | Grad norm: 0.792541 | Time: 2s423ms
Epoch: 022 | Test Loss: 0.0101144 | Time: 529ms
Epoch: 023 | Train Loss: 0.0030609 | Grad norm: 0.616314 | Time: 2s502ms
Epoch: 023 | Test Loss: 0.0011203 | Time: 522ms
==> Save the model at epoch 023 with test loss 0.0011203
Epoch: 024 | Train Loss: 0.0023139 | Grad norm: 0.539777 | Time: 2s390ms
Epoch: 024 | Test Loss: 0.0030653 | Time: 527ms
Epoch: 025 | Train Loss: 0.0020465 | Grad norm: 0.544685 | Time: 2s448ms
Epoch: 025 | Test Loss: 0.0024723 | Time: 525ms
Epoch: 026 | Train Loss: 0.0017235 | Grad norm: 0.485441 | Time: 2s229ms
Epoch: 026 | Test Loss: 0.0008496 | Time: 539ms
==> Save the model at epoch 026 with test loss 0.0008496
Epoch: 027 | Train Loss: 0.0009734 | Grad norm: 0.280211 | Time: 2s476ms
Epoch: 027 | Test Loss: 0.0014378 | Time: 592ms
Epoch: 028 | Train Loss: 0.0013133 | Grad norm: 0.371373 | Time: 2s386ms
Epoch: 028 | Test Loss: 0.0009758 | Time: 555ms
Epoch: 029 | Train Loss: 0.0009824 | Grad norm: 0.286374 | Time: 2s580ms
Epoch: 029 | Test Loss: 0.0010521 | Time: 611ms
Epoch: 030 | Train Loss: 0.0008227 | Grad norm: 0.222373 | Time: 2s461ms
Epoch: 030 | Test Loss: 0.0013839 | Time: 527ms
Epoch: 031 | Train Loss: 0.0007499 | Grad norm: 0.186117 | Time: 2s496ms
Epoch: 031 | Test Loss: 0.0008401 | Time: 550ms
==> Save the model at epoch 031 with test loss 0.0008401
Epoch: 032 | Train Loss: 0.0006790 | Grad norm: 0.148695 | Time: 2s426ms
Epoch: 032 | Test Loss: 0.0007056 | Time: 525ms
==> Save the model at epoch 032 with test loss 0.0007056
Epoch: 033 | Train Loss: 0.0006746 | Grad norm: 0.144643 | Time: 2s382ms
Epoch: 033 | Test Loss: 0.0007707 | Time: 527ms
Epoch: 034 | Train Loss: 0.0007097 | Grad norm: 0.166121 | Time: 2s209ms
Epoch: 034 | Test Loss: 0.0006619 | Time: 531ms
==> Save the model at epoch 034 with test loss 0.0006619
Epoch: 035 | Train Loss: 0.0006483 | Grad norm: 0.122956 | Time: 2s538ms
Epoch: 035 | Test Loss: 0.0005994 | Time: 570ms
==> Save the model at epoch 035 with test loss 0.0005994
Epoch: 036 | Train Loss: 0.0006120 | Grad norm: 0.094109 | Time: 2s315ms
Epoch: 036 | Test Loss: 0.0006017 | Time: 594ms
Epoch: 037 | Train Loss: 0.0005943 | Grad norm: 0.078295 | Time: 2s391ms
Epoch: 037 | Test Loss: 0.0006359 | Time: 533ms
Epoch: 038 | Train Loss: 0.0005915 | Grad norm: 0.065503 | Time: 2s310ms
Epoch: 038 | Test Loss: 0.0005919 | Time: 655ms
==> Save the model at epoch 038 with test loss 0.0005919
Epoch: 039 | Train Loss: 0.0005798 | Grad norm: 0.051516 | Time: 2s377ms
Epoch: 039 | Test Loss: 0.0005924 | Time: 544ms
Epoch: 040 | Train Loss: 0.0005770 | Grad norm: 0.043979 | Time: 2s487ms
Epoch: 040 | Test Loss: 0.0005844 | Time: 543ms
==> Save the model at epoch 040 with test loss 0.0005844
Total time: 1m59s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.7649939 0.7649939]
==> Output transform to be applied to the neural network (trained):
[2.6276 2.6276]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
