==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.6219679 0.6219679]
==> Ouput transform to be applied to the neural network:
[3.225  3.2249]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─ReLU: 2-2                         [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─ReLU: 2-4                         [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─ReLU: 2-6                         [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─ReLU: 2-8                         [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─ReLU: 2-10                        [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─ReLU: 2-12                        [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─ReLU: 2-14                        [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─Linear: 2-16                      [1, 64]                   (recursive)
│    └─ReLU: 2-17                        [1, 64]                   --
│    └─Linear: 2-18                      [1, 64]                   (recursive)
│    └─ReLU: 2-19                        [1, 64]                   --
│    └─Linear: 2-20                      [1, 64]                   (recursive)
│    └─ReLU: 2-21                        [1, 64]                   --
│    └─Linear: 2-22                      [1, 64]                   (recursive)
│    └─ReLU: 2-23                        [1, 64]                   --
│    └─Linear: 2-24                      [1, 64]                   (recursive)
│    └─ReLU: 2-25                        [1, 64]                   --
│    └─Linear: 2-26                      [1, 64]                   (recursive)
│    └─ReLU: 2-27                        [1, 64]                   --
│    └─Linear: 2-28                      [1, 64]                   (recursive)
│    └─ReLU: 2-29                        [1, 64]                   --
│    └─Linear: 2-30                      [1, 2]                    (recursive)
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.05
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.10
Estimated Total Size (MB): 0.11
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 20.6999976 | Grad norm: 0.594942 | Time: 1s470ms
Epoch: 001 | Test Loss: 20.6280730 | Time: 549ms
==> Save the model at epoch 001 with test loss 20.6280730
Epoch: 002 | Train Loss: 3.6884175 | Grad norm: 7.744951 | Time: 1s221ms
Epoch: 002 | Test Loss: 0.0082901 | Time: 529ms
==> Save the model at epoch 002 with test loss 0.0082901
Epoch: 003 | Train Loss: 0.0294925 | Grad norm: 3.420107 | Time: 1s339ms
Epoch: 003 | Test Loss: 0.0228710 | Time: 531ms
Epoch: 004 | Train Loss: 0.0355453 | Grad norm: 3.882578 | Time: 1s210ms
Epoch: 004 | Test Loss: 0.0322087 | Time: 529ms
Epoch: 005 | Train Loss: 0.1024721 | Grad norm: 5.973695 | Time: 1s281ms
Epoch: 005 | Test Loss: 0.0515855 | Time: 528ms
Epoch: 006 | Train Loss: 0.0795564 | Grad norm: 5.294460 | Time: 1s217ms
Epoch: 006 | Test Loss: 0.0130367 | Time: 598ms
Epoch: 007 | Train Loss: 0.1249671 | Grad norm: 6.763718 | Time: 1s168ms
Epoch: 007 | Test Loss: 0.0768777 | Time: 526ms
Epoch: 008 | Train Loss: 0.1336005 | Grad norm: 6.935104 | Time: 1s176ms
Epoch: 008 | Test Loss: 0.2387536 | Time: 550ms
Epoch: 009 | Train Loss: 0.1261010 | Grad norm: 6.741187 | Time: 1s184ms
Epoch: 009 | Test Loss: 0.1266999 | Time: 526ms
Epoch: 010 | Train Loss: 0.1684625 | Grad norm: 8.217552 | Time: 1s272ms
Epoch: 010 | Test Loss: 0.0286352 | Time: 529ms
Epoch: 011 | Train Loss: 0.1021818 | Grad norm: 6.000462 | Time: 1s221ms
Epoch: 011 | Test Loss: 0.1787036 | Time: 528ms
Epoch: 012 | Train Loss: 0.1688587 | Grad norm: 8.543315 | Time: 1s228ms
Epoch: 012 | Test Loss: 0.0797772 | Time: 545ms
Epoch: 013 | Train Loss: 0.1093855 | Grad norm: 6.514307 | Time: 1s257ms
Epoch: 013 | Test Loss: 0.1132896 | Time: 535ms
Epoch: 014 | Train Loss: 0.0726591 | Grad norm: 5.174605 | Time: 1s192ms
Epoch: 014 | Test Loss: 0.0386486 | Time: 530ms
Epoch: 015 | Train Loss: 0.0393213 | Grad norm: 3.941912 | Time: 1s188ms
Epoch: 015 | Test Loss: 0.0301246 | Time: 529ms
Epoch: 016 | Train Loss: 0.0705630 | Grad norm: 4.894767 | Time: 1s224ms
Epoch: 016 | Test Loss: 0.1014394 | Time: 604ms
Epoch: 017 | Train Loss: 0.0489916 | Grad norm: 4.376655 | Time: 1s301ms
Epoch: 017 | Test Loss: 0.0329420 | Time: 593ms
Epoch: 018 | Train Loss: 0.0476512 | Grad norm: 4.256997 | Time: 1s326ms
Epoch: 018 | Test Loss: 0.0314694 | Time: 530ms
Epoch: 019 | Train Loss: 0.0313755 | Grad norm: 3.436132 | Time: 1s179ms
Epoch: 019 | Test Loss: 0.0056039 | Time: 598ms
==> Save the model at epoch 019 with test loss 0.0056039
Epoch: 020 | Train Loss: 0.1126845 | Grad norm: 6.517079 | Time: 1s255ms
Epoch: 020 | Test Loss: 0.1888836 | Time: 537ms
Epoch: 021 | Train Loss: 0.0498297 | Grad norm: 4.173619 | Time: 1s300ms
Epoch: 021 | Test Loss: 0.0249064 | Time: 561ms
Epoch: 022 | Train Loss: 0.0178334 | Grad norm: 2.494641 | Time: 1s279ms
Epoch: 022 | Test Loss: 0.0058385 | Time: 527ms
Epoch: 023 | Train Loss: 0.0149397 | Grad norm: 2.154970 | Time: 1s225ms
Epoch: 023 | Test Loss: 0.0181678 | Time: 545ms
Epoch: 024 | Train Loss: 0.0234238 | Grad norm: 2.759185 | Time: 1s235ms
Epoch: 024 | Test Loss: 0.0135310 | Time: 526ms
Epoch: 025 | Train Loss: 0.0127131 | Grad norm: 2.077176 | Time: 1s256ms
Epoch: 025 | Test Loss: 0.0081585 | Time: 529ms
Epoch: 026 | Train Loss: 0.0157492 | Grad norm: 1.972276 | Time: 1s203ms
Epoch: 026 | Test Loss: 0.0054040 | Time: 608ms
==> Save the model at epoch 026 with test loss 0.0054040
Epoch: 027 | Train Loss: 0.0064286 | Grad norm: 1.202695 | Time: 1s226ms
Epoch: 027 | Test Loss: 0.0065591 | Time: 532ms
Epoch: 028 | Train Loss: 0.0058221 | Grad norm: 1.114645 | Time: 1s229ms
Epoch: 028 | Test Loss: 0.0042733 | Time: 599ms
==> Save the model at epoch 028 with test loss 0.0042733
Epoch: 029 | Train Loss: 0.0044720 | Grad norm: 0.807836 | Time: 1s287ms
Epoch: 029 | Test Loss: 0.0031037 | Time: 609ms
==> Save the model at epoch 029 with test loss 0.0031037
Epoch: 030 | Train Loss: 0.0032065 | Grad norm: 0.425205 | Time: 1s180ms
Epoch: 030 | Test Loss: 0.0032972 | Time: 537ms
Epoch: 031 | Train Loss: 0.0034758 | Grad norm: 0.546135 | Time: 1s323ms
Epoch: 031 | Test Loss: 0.0037771 | Time: 546ms
Epoch: 032 | Train Loss: 0.0031810 | Grad norm: 0.415090 | Time: 1s277ms
Epoch: 032 | Test Loss: 0.0029889 | Time: 532ms
==> Save the model at epoch 032 with test loss 0.0029889
Epoch: 033 | Train Loss: 0.0029532 | Grad norm: 0.303956 | Time: 1s340ms
Epoch: 033 | Test Loss: 0.0035822 | Time: 527ms
Epoch: 034 | Train Loss: 0.0029885 | Grad norm: 0.334566 | Time: 1s195ms
Epoch: 034 | Test Loss: 0.0028925 | Time: 536ms
==> Save the model at epoch 034 with test loss 0.0028925
Epoch: 035 | Train Loss: 0.0028869 | Grad norm: 0.266866 | Time: 1s147ms
Epoch: 035 | Test Loss: 0.0028669 | Time: 560ms
==> Save the model at epoch 035 with test loss 0.0028669
Epoch: 036 | Train Loss: 0.0028006 | Grad norm: 0.192679 | Time: 1s373ms
Epoch: 036 | Test Loss: 0.0028807 | Time: 528ms
Epoch: 037 | Train Loss: 0.0027704 | Grad norm: 0.170063 | Time: 1s242ms
Epoch: 037 | Test Loss: 0.0027476 | Time: 530ms
==> Save the model at epoch 037 with test loss 0.0027476
Epoch: 038 | Train Loss: 0.0027451 | Grad norm: 0.132804 | Time: 1s179ms
Epoch: 038 | Test Loss: 0.0027695 | Time: 530ms
Epoch: 039 | Train Loss: 0.0027298 | Grad norm: 0.117090 | Time: 1s278ms
Epoch: 039 | Test Loss: 0.0027626 | Time: 605ms
Epoch: 040 | Train Loss: 0.0027262 | Grad norm: 0.108197 | Time: 1s266ms
Epoch: 040 | Test Loss: 0.0027288 | Time: 561ms
==> Save the model at epoch 040 with test loss 0.0027288
Total time: 1m11s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.6219679 0.6219679]
==> Output transform to be applied to the neural network (trained):
[3.225  3.2249]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
