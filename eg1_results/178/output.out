==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 18.9189916 | L2 loss: 18.9189916 | Lip loss: 0.0000000 | Grad norm: 0.869333 | Time: 7s154ms
Epoch: 001 | Test Loss: 18.9444759 | Time: 367ms
==> Save the model at epoch 001 with test loss 18.9444759
Epoch: 002 | Loss: 17.5464923 | L2 loss: 17.5464923 | Lip loss: 0.0000002 | Grad norm: 4.331251 | Time: 7s24ms
Epoch: 002 | Test Loss: 15.1920297 | Time: 357ms
==> Save the model at epoch 002 with test loss 15.1920297
Epoch: 003 | Loss: 9.7460218 | L2 loss: 9.7460204 | Lip loss: 0.0000014 | Grad norm: 12.259285 | Time: 6s912ms
Epoch: 003 | Test Loss: 3.6361763 | Time: 365ms
==> Save the model at epoch 003 with test loss 3.6361763
Epoch: 004 | Loss: 0.7289924 | L2 loss: 0.7289900 | Lip loss: 0.0000024 | Grad norm: 5.637606 | Time: 6s818ms
Epoch: 004 | Test Loss: 0.0245126 | Time: 373ms
==> Save the model at epoch 004 with test loss 0.0245126
Epoch: 005 | Loss: 0.0117029 | L2 loss: 0.0117008 | Lip loss: 0.0000022 | Grad norm: 1.910175 | Time: 6s788ms
Epoch: 005 | Test Loss: 0.0058676 | Time: 356ms
==> Save the model at epoch 005 with test loss 0.0058676
Epoch: 006 | Loss: 0.0050452 | L2 loss: 0.0050431 | Lip loss: 0.0000021 | Grad norm: 0.511021 | Time: 6s864ms
Epoch: 006 | Test Loss: 0.0049155 | Time: 429ms
==> Save the model at epoch 006 with test loss 0.0049155
Epoch: 007 | Loss: 0.0046643 | L2 loss: 0.0046622 | Lip loss: 0.0000021 | Grad norm: 0.478962 | Time: 6s761ms
Epoch: 007 | Test Loss: 0.0045248 | Time: 371ms
==> Save the model at epoch 007 with test loss 0.0045248
Epoch: 008 | Loss: 0.0042974 | L2 loss: 0.0042953 | Lip loss: 0.0000021 | Grad norm: 0.407053 | Time: 6s853ms
Epoch: 008 | Test Loss: 0.0042215 | Time: 354ms
==> Save the model at epoch 008 with test loss 0.0042215
Epoch: 009 | Loss: 0.0040225 | L2 loss: 0.0040204 | Lip loss: 0.0000021 | Grad norm: 0.412116 | Time: 6s949ms
Epoch: 009 | Test Loss: 0.0039211 | Time: 354ms
==> Save the model at epoch 009 with test loss 0.0039211
Epoch: 010 | Loss: 0.0037801 | L2 loss: 0.0037780 | Lip loss: 0.0000021 | Grad norm: 0.469423 | Time: 6s969ms
Epoch: 010 | Test Loss: 0.0036956 | Time: 354ms
==> Save the model at epoch 010 with test loss 0.0036956
Epoch: 011 | Loss: 0.0036070 | L2 loss: 0.0036049 | Lip loss: 0.0000021 | Grad norm: 0.310631 | Time: 7s8ms
Epoch: 011 | Test Loss: 0.0036489 | Time: 356ms
==> Save the model at epoch 011 with test loss 0.0036489
Epoch: 012 | Loss: 0.0035908 | L2 loss: 0.0035887 | Lip loss: 0.0000021 | Grad norm: 0.334891 | Time: 7s30ms
Epoch: 012 | Test Loss: 0.0036377 | Time: 360ms
==> Save the model at epoch 012 with test loss 0.0036377
Epoch: 013 | Loss: 0.0035561 | L2 loss: 0.0035540 | Lip loss: 0.0000021 | Grad norm: 0.335419 | Time: 7s176ms
Epoch: 013 | Test Loss: 0.0036008 | Time: 430ms
==> Save the model at epoch 013 with test loss 0.0036008
Epoch: 014 | Loss: 0.0035261 | L2 loss: 0.0035240 | Lip loss: 0.0000021 | Grad norm: 0.348561 | Time: 6s955ms
Epoch: 014 | Test Loss: 0.0035796 | Time: 366ms
==> Save the model at epoch 014 with test loss 0.0035796
Epoch: 015 | Loss: 0.0035041 | L2 loss: 0.0035020 | Lip loss: 0.0000021 | Grad norm: 0.318277 | Time: 7s179ms
Epoch: 015 | Test Loss: 0.0035554 | Time: 353ms
==> Save the model at epoch 015 with test loss 0.0035554
Epoch: 016 | Loss: 0.0035055 | L2 loss: 0.0035034 | Lip loss: 0.0000021 | Grad norm: 0.288860 | Time: 6s993ms
Epoch: 016 | Test Loss: 0.0035493 | Time: 381ms
==> Save the model at epoch 016 with test loss 0.0035493
Epoch: 017 | Loss: 0.0034822 | L2 loss: 0.0034801 | Lip loss: 0.0000021 | Grad norm: 0.284010 | Time: 7s267ms
Epoch: 017 | Test Loss: 0.0035474 | Time: 366ms
==> Save the model at epoch 017 with test loss 0.0035474
Epoch: 018 | Loss: 0.0034788 | L2 loss: 0.0034766 | Lip loss: 0.0000021 | Grad norm: 0.282572 | Time: 7s111ms
Epoch: 018 | Test Loss: 0.0035452 | Time: 385ms
==> Save the model at epoch 018 with test loss 0.0035452
Epoch: 019 | Loss: 0.0034864 | L2 loss: 0.0034843 | Lip loss: 0.0000021 | Grad norm: 0.289860 | Time: 7s396ms
Epoch: 019 | Test Loss: 0.0035416 | Time: 362ms
==> Save the model at epoch 019 with test loss 0.0035416
Epoch: 020 | Loss: 0.0035003 | L2 loss: 0.0034982 | Lip loss: 0.0000021 | Grad norm: 0.298636 | Time: 7s313ms
Epoch: 020 | Test Loss: 0.0035402 | Time: 367ms
==> Save the model at epoch 020 with test loss 0.0035402
Epoch: 021 | Loss: 0.0034881 | L2 loss: 0.0034860 | Lip loss: 0.0000021 | Grad norm: 0.308801 | Time: 6s898ms
Epoch: 021 | Test Loss: 0.0035398 | Time: 358ms
==> Save the model at epoch 021 with test loss 0.0035398
Epoch: 022 | Loss: 0.0034903 | L2 loss: 0.0034882 | Lip loss: 0.0000021 | Grad norm: 0.286746 | Time: 7s41ms
Epoch: 022 | Test Loss: 0.0035395 | Time: 356ms
==> Save the model at epoch 022 with test loss 0.0035395
Epoch: 023 | Loss: 0.0034660 | L2 loss: 0.0034639 | Lip loss: 0.0000021 | Grad norm: 0.278452 | Time: 6s917ms
Epoch: 023 | Test Loss: 0.0035393 | Time: 425ms
==> Save the model at epoch 023 with test loss 0.0035393
Epoch: 024 | Loss: 0.0034892 | L2 loss: 0.0034870 | Lip loss: 0.0000021 | Grad norm: 0.296498 | Time: 6s955ms
Epoch: 024 | Test Loss: 0.0035390 | Time: 361ms
==> Save the model at epoch 024 with test loss 0.0035390
Epoch: 025 | Loss: 0.0034782 | L2 loss: 0.0034761 | Lip loss: 0.0000021 | Grad norm: 0.289750 | Time: 7s74ms
Epoch: 025 | Test Loss: 0.0035387 | Time: 379ms
==> Save the model at epoch 025 with test loss 0.0035387
Epoch: 026 | Loss: 0.0034877 | L2 loss: 0.0034856 | Lip loss: 0.0000021 | Grad norm: 0.281422 | Time: 6s913ms
Epoch: 026 | Test Loss: 0.0035387 | Time: 367ms
==> Save the model at epoch 026 with test loss 0.0035387
Epoch: 027 | Loss: 0.0034737 | L2 loss: 0.0034716 | Lip loss: 0.0000021 | Grad norm: 0.281589 | Time: 7s245ms
Epoch: 027 | Test Loss: 0.0035387 | Time: 358ms
==> Save the model at epoch 027 with test loss 0.0035387
Epoch: 028 | Loss: 0.0034637 | L2 loss: 0.0034616 | Lip loss: 0.0000021 | Grad norm: 0.293617 | Time: 6s939ms
Epoch: 028 | Test Loss: 0.0035387 | Time: 366ms
==> Save the model at epoch 028 with test loss 0.0035387
Epoch: 029 | Loss: 0.0034947 | L2 loss: 0.0034926 | Lip loss: 0.0000021 | Grad norm: 0.309695 | Time: 7s151ms
Epoch: 029 | Test Loss: 0.0035387 | Time: 361ms
==> Save the model at epoch 029 with test loss 0.0035387
Epoch: 030 | Loss: 0.0034912 | L2 loss: 0.0034891 | Lip loss: 0.0000021 | Grad norm: 0.289759 | Time: 6s992ms
Epoch: 030 | Test Loss: 0.0035387 | Time: 446ms
==> Save the model at epoch 030 with test loss 0.0035387
Epoch: 031 | Loss: 0.0034834 | L2 loss: 0.0034813 | Lip loss: 0.0000021 | Grad norm: 0.303651 | Time: 6s911ms
Epoch: 031 | Test Loss: 0.0035387 | Time: 376ms
Epoch: 032 | Loss: 0.0034956 | L2 loss: 0.0034935 | Lip loss: 0.0000021 | Grad norm: 0.301669 | Time: 6s836ms
Epoch: 032 | Test Loss: 0.0035387 | Time: 379ms
Epoch: 033 | Loss: 0.0034785 | L2 loss: 0.0034764 | Lip loss: 0.0000021 | Grad norm: 0.279406 | Time: 6s886ms
Epoch: 033 | Test Loss: 0.0035387 | Time: 372ms
Epoch: 034 | Loss: 0.0034756 | L2 loss: 0.0034735 | Lip loss: 0.0000021 | Grad norm: 0.311771 | Time: 7s363ms
Epoch: 034 | Test Loss: 0.0035387 | Time: 358ms
==> Save the model at epoch 034 with test loss 0.0035387
Epoch: 035 | Loss: 0.0034848 | L2 loss: 0.0034827 | Lip loss: 0.0000021 | Grad norm: 0.284548 | Time: 7s116ms
Epoch: 035 | Test Loss: 0.0035387 | Time: 356ms
==> Save the model at epoch 035 with test loss 0.0035387
Epoch: 036 | Loss: 0.0034749 | L2 loss: 0.0034728 | Lip loss: 0.0000021 | Grad norm: 0.296504 | Time: 7s105ms
Epoch: 036 | Test Loss: 0.0035387 | Time: 375ms
==> Save the model at epoch 036 with test loss 0.0035387
Epoch: 037 | Loss: 0.0034750 | L2 loss: 0.0034729 | Lip loss: 0.0000021 | Grad norm: 0.289758 | Time: 7s485ms
Epoch: 037 | Test Loss: 0.0035387 | Time: 384ms
==> Save the model at epoch 037 with test loss 0.0035387
Epoch: 038 | Loss: 0.0034950 | L2 loss: 0.0034929 | Lip loss: 0.0000021 | Grad norm: 0.273799 | Time: 6s876ms
Epoch: 038 | Test Loss: 0.0035387 | Time: 357ms
Epoch: 039 | Loss: 0.0034793 | L2 loss: 0.0034772 | Lip loss: 0.0000021 | Grad norm: 0.306647 | Time: 7s149ms
Epoch: 039 | Test Loss: 0.0035387 | Time: 359ms
Epoch: 040 | Loss: 0.0034823 | L2 loss: 0.0034801 | Lip loss: 0.0000021 | Grad norm: 0.297423 | Time: 7s47ms
Epoch: 040 | Test Loss: 0.0035387 | Time: 365ms
Total time: 4m56s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
