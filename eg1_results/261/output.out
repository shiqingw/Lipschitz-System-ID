==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 9.5734683 | L2 loss: 9.4368460 | Lip loss: 0.1366223 | Grad norm: 5.410281 | Time: 40s938ms
Epoch: 001 | Test Loss: 0.0087001 | Time: 532ms
==> Save the model at epoch 001 with test loss 0.0087001
Epoch: 002 | Loss: 0.2137770 | L2 loss: 0.0073473 | Lip loss: 0.2064297 | Grad norm: 2.200023 | Time: 40s127ms
Epoch: 002 | Test Loss: 0.0054594 | Time: 540ms
==> Save the model at epoch 002 with test loss 0.0054594
Epoch: 003 | Loss: 0.2098517 | L2 loss: 0.0057746 | Lip loss: 0.2040771 | Grad norm: 2.122316 | Time: 40s34ms
Epoch: 003 | Test Loss: 0.0047612 | Time: 547ms
==> Save the model at epoch 003 with test loss 0.0047612
Epoch: 004 | Loss: 0.2093322 | L2 loss: 0.0055852 | Lip loss: 0.2037470 | Grad norm: 2.193068 | Time: 40s868ms
Epoch: 004 | Test Loss: 0.0044732 | Time: 538ms
==> Save the model at epoch 004 with test loss 0.0044732
Epoch: 005 | Loss: 0.2082454 | L2 loss: 0.0050884 | Lip loss: 0.2031570 | Grad norm: 2.075834 | Time: 41s80ms
Epoch: 005 | Test Loss: 0.0039181 | Time: 539ms
==> Save the model at epoch 005 with test loss 0.0039181
Epoch: 006 | Loss: 0.2062713 | L2 loss: 0.0038447 | Lip loss: 0.2024267 | Grad norm: 1.229991 | Time: 40s156ms
Epoch: 006 | Test Loss: 0.0036456 | Time: 600ms
==> Save the model at epoch 006 with test loss 0.0036456
Epoch: 007 | Loss: 0.2060671 | L2 loss: 0.0037959 | Lip loss: 0.2022712 | Grad norm: 1.106589 | Time: 40s401ms
Epoch: 007 | Test Loss: 0.0037487 | Time: 537ms
Epoch: 008 | Loss: 0.2060357 | L2 loss: 0.0037975 | Lip loss: 0.2022383 | Grad norm: 1.144490 | Time: 40s515ms
Epoch: 008 | Test Loss: 0.0036686 | Time: 532ms
Epoch: 009 | Loss: 0.2060519 | L2 loss: 0.0037935 | Lip loss: 0.2022584 | Grad norm: 1.164259 | Time: 40s759ms
Epoch: 009 | Test Loss: 0.0038028 | Time: 535ms
Epoch: 010 | Loss: 0.2057987 | L2 loss: 0.0037840 | Lip loss: 0.2020147 | Grad norm: 1.154382 | Time: 40s808ms
Epoch: 010 | Test Loss: 0.0036978 | Time: 543ms
Epoch: 011 | Loss: 0.2058454 | L2 loss: 0.0036316 | Lip loss: 0.2022138 | Grad norm: 1.000995 | Time: 40s513ms
Epoch: 011 | Test Loss: 0.0035764 | Time: 537ms
==> Save the model at epoch 011 with test loss 0.0035764
Epoch: 012 | Loss: 0.2056283 | L2 loss: 0.0036085 | Lip loss: 0.2020198 | Grad norm: 0.969644 | Time: 41s12ms
Epoch: 012 | Test Loss: 0.0035866 | Time: 523ms
Epoch: 013 | Loss: 0.2056410 | L2 loss: 0.0036144 | Lip loss: 0.2020266 | Grad norm: 0.954890 | Time: 40s379ms
Epoch: 013 | Test Loss: 0.0036257 | Time: 605ms
Epoch: 014 | Loss: 0.2057539 | L2 loss: 0.0036197 | Lip loss: 0.2021342 | Grad norm: 0.970304 | Time: 40s881ms
Epoch: 014 | Test Loss: 0.0036181 | Time: 524ms
Epoch: 015 | Loss: 0.2056707 | L2 loss: 0.0036167 | Lip loss: 0.2020541 | Grad norm: 0.989001 | Time: 40s727ms
Epoch: 015 | Test Loss: 0.0035792 | Time: 548ms
Epoch: 016 | Loss: 0.2055502 | L2 loss: 0.0035827 | Lip loss: 0.2019675 | Grad norm: 0.937007 | Time: 41s793ms
Epoch: 016 | Test Loss: 0.0035794 | Time: 616ms
Epoch: 017 | Loss: 0.2056672 | L2 loss: 0.0035962 | Lip loss: 0.2020710 | Grad norm: 0.967195 | Time: 40s438ms
Epoch: 017 | Test Loss: 0.0035861 | Time: 534ms
Epoch: 018 | Loss: 0.2057734 | L2 loss: 0.0035939 | Lip loss: 0.2021795 | Grad norm: 0.976206 | Time: 40s661ms
Epoch: 018 | Test Loss: 0.0035888 | Time: 533ms
Epoch: 019 | Loss: 0.2055348 | L2 loss: 0.0035982 | Lip loss: 0.2019366 | Grad norm: 0.972461 | Time: 40s539ms
Epoch: 019 | Test Loss: 0.0035873 | Time: 542ms
Epoch: 020 | Loss: 0.2056202 | L2 loss: 0.0035961 | Lip loss: 0.2020241 | Grad norm: 0.969482 | Time: 40s778ms
Epoch: 020 | Test Loss: 0.0035829 | Time: 542ms
Epoch: 021 | Loss: 0.2054944 | L2 loss: 0.0035946 | Lip loss: 0.2018998 | Grad norm: 0.948078 | Time: 40s446ms
Epoch: 021 | Test Loss: 0.0035846 | Time: 524ms
Epoch: 022 | Loss: 0.2055921 | L2 loss: 0.0035959 | Lip loss: 0.2019962 | Grad norm: 0.965456 | Time: 40s446ms
Epoch: 022 | Test Loss: 0.0035861 | Time: 527ms
Epoch: 023 | Loss: 0.2058325 | L2 loss: 0.0035975 | Lip loss: 0.2022350 | Grad norm: 0.968020 | Time: 41s22ms
Epoch: 023 | Test Loss: 0.0035871 | Time: 595ms
Epoch: 024 | Loss: 0.2056591 | L2 loss: 0.0035975 | Lip loss: 0.2020616 | Grad norm: 0.957939 | Time: 40s40ms
Epoch: 024 | Test Loss: 0.0035871 | Time: 530ms
Epoch: 025 | Loss: 0.2057320 | L2 loss: 0.0035981 | Lip loss: 0.2021339 | Grad norm: 0.963012 | Time: 40s800ms
Epoch: 025 | Test Loss: 0.0035881 | Time: 562ms
Epoch: 026 | Loss: 0.2056717 | L2 loss: 0.0035989 | Lip loss: 0.2020729 | Grad norm: 0.950747 | Time: 40s781ms
Epoch: 026 | Test Loss: 0.0035881 | Time: 538ms
Epoch: 027 | Loss: 0.2058543 | L2 loss: 0.0035990 | Lip loss: 0.2022552 | Grad norm: 0.947826 | Time: 40s777ms
Epoch: 027 | Test Loss: 0.0035881 | Time: 523ms
Epoch: 028 | Loss: 0.2057128 | L2 loss: 0.0035987 | Lip loss: 0.2021141 | Grad norm: 0.953845 | Time: 40s955ms
Epoch: 028 | Test Loss: 0.0035881 | Time: 538ms
Epoch: 029 | Loss: 0.2056278 | L2 loss: 0.0035988 | Lip loss: 0.2020291 | Grad norm: 0.945861 | Time: 40s392ms
Epoch: 029 | Test Loss: 0.0035881 | Time: 534ms
Epoch: 030 | Loss: 0.2056996 | L2 loss: 0.0035989 | Lip loss: 0.2021007 | Grad norm: 0.960923 | Time: 40s305ms
Epoch: 030 | Test Loss: 0.0035881 | Time: 531ms
Epoch: 031 | Loss: 0.2056654 | L2 loss: 0.0035988 | Lip loss: 0.2020666 | Grad norm: 0.954557 | Time: 41s266ms
Epoch: 031 | Test Loss: 0.0035881 | Time: 522ms
Epoch: 032 | Loss: 0.2056045 | L2 loss: 0.0035988 | Lip loss: 0.2020057 | Grad norm: 0.956793 | Time: 40s775ms
Epoch: 032 | Test Loss: 0.0035881 | Time: 541ms
Epoch: 033 | Loss: 0.2055397 | L2 loss: 0.0035989 | Lip loss: 0.2019408 | Grad norm: 0.979257 | Time: 41s425ms
Epoch: 033 | Test Loss: 0.0035881 | Time: 540ms
Epoch: 034 | Loss: 0.2054976 | L2 loss: 0.0035987 | Lip loss: 0.2018989 | Grad norm: 0.945794 | Time: 41s99ms
Epoch: 034 | Test Loss: 0.0035881 | Time: 524ms
Epoch: 035 | Loss: 0.2057204 | L2 loss: 0.0035988 | Lip loss: 0.2021216 | Grad norm: 0.964099 | Time: 40s568ms
Epoch: 035 | Test Loss: 0.0035881 | Time: 524ms
Epoch: 036 | Loss: 0.2056776 | L2 loss: 0.0035992 | Lip loss: 0.2020783 | Grad norm: 0.977475 | Time: 40s824ms
Epoch: 036 | Test Loss: 0.0035881 | Time: 535ms
Epoch: 037 | Loss: 0.2056051 | L2 loss: 0.0035989 | Lip loss: 0.2020062 | Grad norm: 0.953879 | Time: 41s265ms
Epoch: 037 | Test Loss: 0.0035881 | Time: 604ms
Epoch: 038 | Loss: 0.2057197 | L2 loss: 0.0035988 | Lip loss: 0.2021209 | Grad norm: 0.949546 | Time: 41s509ms
Epoch: 038 | Test Loss: 0.0035881 | Time: 537ms
Epoch: 039 | Loss: 0.2055221 | L2 loss: 0.0035989 | Lip loss: 0.2019232 | Grad norm: 0.948835 | Time: 40s438ms
Epoch: 039 | Test Loss: 0.0035881 | Time: 543ms
Epoch: 040 | Loss: 0.2056946 | L2 loss: 0.0035990 | Lip loss: 0.2020956 | Grad norm: 0.967365 | Time: 40s193ms
Epoch: 040 | Test Loss: 0.0035881 | Time: 542ms
Total time: 27m30s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
