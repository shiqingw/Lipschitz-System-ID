==> torch device:  cuda:3
==> Lipschitz constant: 8.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 128]                  16,897
│    └─SandwichFc: 2-2                   [1, 128]                  33,025
│    └─SandwichFc: 2-3                   [1, 128]                  33,025
│    └─SandwichFc: 2-4                   [1, 128]                  33,025
│    └─SandwichFc: 2-5                   [1, 128]                  33,025
│    └─SandwichFc: 2-6                   [1, 128]                  33,025
│    └─SandwichFc: 2-7                   [1, 128]                  33,025
│    └─SandwichLin: 2-8                  [1, 2]                    263
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 128]                  (recursive)
│    └─SandwichFc: 2-10                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-11                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-12                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-13                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-14                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-15                  [1, 128]                  (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 215,316
Trainable params: 215,310
Non-trainable params: 6
Total mult-adds (M): 0.43
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 1.00
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.8984457 | Grad norm: 61.367231 | Time: 20s298ms
Epoch: 001 | Test Loss: 30.9308201 | Time: 795ms
==> Save the model at epoch 001 with test loss 30.9308201
Epoch: 002 | Train Loss: 0.8642137 | Grad norm: 13.123916 | Time: 22s212ms
Epoch: 002 | Test Loss: 0.0107998 | Time: 835ms
==> Save the model at epoch 002 with test loss 0.0107998
Epoch: 003 | Train Loss: 0.0312210 | Grad norm: 10.143787 | Time: 22s799ms
Epoch: 003 | Test Loss: 0.0240947 | Time: 803ms
Epoch: 004 | Train Loss: 0.0429348 | Grad norm: 11.786129 | Time: 20s448ms
Epoch: 004 | Test Loss: 0.0369744 | Time: 793ms
Epoch: 005 | Train Loss: 0.0661953 | Grad norm: 13.298242 | Time: 18s635ms
Epoch: 005 | Test Loss: 0.0413073 | Time: 806ms
Epoch: 006 | Train Loss: 0.0586818 | Grad norm: 13.119220 | Time: 18s309ms
Epoch: 006 | Test Loss: 0.0571433 | Time: 791ms
Epoch: 007 | Train Loss: 0.1157750 | Grad norm: 16.880192 | Time: 19s902ms
Epoch: 007 | Test Loss: 0.0658031 | Time: 859ms
Epoch: 008 | Train Loss: 0.0584966 | Grad norm: 11.939123 | Time: 21s1ms
Epoch: 008 | Test Loss: 0.0403747 | Time: 826ms
Epoch: 009 | Train Loss: 0.0688091 | Grad norm: 11.640799 | Time: 18s280ms
Epoch: 009 | Test Loss: 0.0519217 | Time: 791ms
Epoch: 010 | Train Loss: 0.0566293 | Grad norm: 11.012266 | Time: 18s617ms
Epoch: 010 | Test Loss: 0.0642278 | Time: 812ms
Epoch: 011 | Train Loss: 0.0555680 | Grad norm: 10.215183 | Time: 18s573ms
Epoch: 011 | Test Loss: 0.0504326 | Time: 794ms
Epoch: 012 | Train Loss: 0.0443188 | Grad norm: 9.248738 | Time: 18s670ms
Epoch: 012 | Test Loss: 0.0323281 | Time: 825ms
Epoch: 013 | Train Loss: 0.0388736 | Grad norm: 8.210112 | Time: 18s79ms
Epoch: 013 | Test Loss: 0.0473704 | Time: 822ms
Epoch: 014 | Train Loss: 0.0347619 | Grad norm: 7.540168 | Time: 20s795ms
Epoch: 014 | Test Loss: 0.0295149 | Time: 794ms
Epoch: 015 | Train Loss: 0.0302290 | Grad norm: 6.921405 | Time: 22s678ms
Epoch: 015 | Test Loss: 0.0229527 | Time: 820ms
Epoch: 016 | Train Loss: 0.0509231 | Grad norm: 7.910509 | Time: 19s793ms
Epoch: 016 | Test Loss: 0.0287919 | Time: 796ms
Epoch: 017 | Train Loss: 0.0242657 | Grad norm: 6.241015 | Time: 21s459ms
Epoch: 017 | Test Loss: 0.0219308 | Time: 791ms
Epoch: 018 | Train Loss: 0.0223700 | Grad norm: 5.597123 | Time: 19s39ms
Epoch: 018 | Test Loss: 0.0233039 | Time: 823ms
Epoch: 019 | Train Loss: 0.0213062 | Grad norm: 5.099332 | Time: 19s949ms
Epoch: 019 | Test Loss: 0.0207819 | Time: 825ms
Epoch: 020 | Train Loss: 0.0190303 | Grad norm: 4.891223 | Time: 18s486ms
Epoch: 020 | Test Loss: 0.0181865 | Time: 898ms
Epoch: 021 | Train Loss: 0.0184256 | Grad norm: 4.481011 | Time: 20s984ms
Epoch: 021 | Test Loss: 0.0188240 | Time: 795ms
Epoch: 022 | Train Loss: 0.0186295 | Grad norm: 4.448337 | Time: 18s294ms
Epoch: 022 | Test Loss: 0.0104589 | Time: 834ms
==> Save the model at epoch 022 with test loss 0.0104589
Epoch: 023 | Train Loss: 0.0162648 | Grad norm: 4.129481 | Time: 18s845ms
Epoch: 023 | Test Loss: 0.0155913 | Time: 796ms
Epoch: 024 | Train Loss: 0.0154590 | Grad norm: 3.965607 | Time: 18s328ms
Epoch: 024 | Test Loss: 0.0134549 | Time: 799ms
Epoch: 025 | Train Loss: 0.0309396 | Grad norm: 5.366213 | Time: 18s264ms
Epoch: 025 | Test Loss: 0.0179234 | Time: 800ms
Epoch: 026 | Train Loss: 0.0150142 | Grad norm: 3.797524 | Time: 20s748ms
Epoch: 026 | Test Loss: 0.0137824 | Time: 804ms
Epoch: 027 | Train Loss: 0.0137005 | Grad norm: 3.380450 | Time: 21s674ms
Epoch: 027 | Test Loss: 0.0196980 | Time: 793ms
Epoch: 028 | Train Loss: 0.0128355 | Grad norm: 3.119006 | Time: 18s686ms
Epoch: 028 | Test Loss: 0.0131394 | Time: 792ms
Epoch: 029 | Train Loss: 0.0128984 | Grad norm: 3.049642 | Time: 18s558ms
Epoch: 029 | Test Loss: 0.0104119 | Time: 813ms
==> Save the model at epoch 029 with test loss 0.0104119
Epoch: 030 | Train Loss: 0.0126577 | Grad norm: 2.917585 | Time: 18s378ms
Epoch: 030 | Test Loss: 0.0118582 | Time: 798ms
Epoch: 031 | Train Loss: 0.0114652 | Grad norm: 2.639883 | Time: 18s475ms
Epoch: 031 | Test Loss: 0.0147797 | Time: 891ms
Epoch: 032 | Train Loss: 0.0131447 | Grad norm: 3.239379 | Time: 18s317ms
Epoch: 032 | Test Loss: 0.0089254 | Time: 791ms
==> Save the model at epoch 032 with test loss 0.0089254
Epoch: 033 | Train Loss: 0.0109200 | Grad norm: 2.378587 | Time: 18s397ms
Epoch: 033 | Test Loss: 0.0098271 | Time: 818ms
Epoch: 034 | Train Loss: 0.0101868 | Grad norm: 2.191018 | Time: 19s357ms
Epoch: 034 | Test Loss: 0.0090809 | Time: 859ms
Epoch: 035 | Train Loss: 0.0110773 | Grad norm: 2.381789 | Time: 21s878ms
Epoch: 035 | Test Loss: 0.0084156 | Time: 846ms
==> Save the model at epoch 035 with test loss 0.0084156
Epoch: 036 | Train Loss: 0.0097250 | Grad norm: 1.890143 | Time: 23s281ms
Epoch: 036 | Test Loss: 0.0089830 | Time: 807ms
Epoch: 037 | Train Loss: 0.0110280 | Grad norm: 2.576634 | Time: 22s551ms
Epoch: 037 | Test Loss: 0.0109596 | Time: 854ms
Epoch: 038 | Train Loss: 0.0107759 | Grad norm: 2.570159 | Time: 23s336ms
Epoch: 038 | Test Loss: 0.0082891 | Time: 804ms
==> Save the model at epoch 038 with test loss 0.0082891
Epoch: 039 | Train Loss: 0.0095751 | Grad norm: 1.984116 | Time: 22s938ms
Epoch: 039 | Test Loss: 0.0098277 | Time: 832ms
Epoch: 040 | Train Loss: 0.0092185 | Grad norm: 1.848038 | Time: 22s632ms
Epoch: 040 | Test Loss: 0.0092948 | Time: 869ms
Epoch: 041 | Train Loss: 0.0085580 | Grad norm: 1.401883 | Time: 23s1ms
Epoch: 041 | Test Loss: 0.0084230 | Time: 799ms
Epoch: 042 | Train Loss: 0.0082928 | Grad norm: 1.242865 | Time: 21s300ms
Epoch: 042 | Test Loss: 0.0075159 | Time: 920ms
==> Save the model at epoch 042 with test loss 0.0075159
Epoch: 043 | Train Loss: 0.0084201 | Grad norm: 1.416217 | Time: 18s960ms
Epoch: 043 | Test Loss: 0.0075909 | Time: 892ms
Epoch: 044 | Train Loss: 0.0082538 | Grad norm: 1.301024 | Time: 22s711ms
Epoch: 044 | Test Loss: 0.0089145 | Time: 863ms
Epoch: 045 | Train Loss: 0.0078822 | Grad norm: 1.016074 | Time: 21s974ms
Epoch: 045 | Test Loss: 0.0080716 | Time: 827ms
Epoch: 046 | Train Loss: 0.0077755 | Grad norm: 0.978401 | Time: 22s516ms
Epoch: 046 | Test Loss: 0.0078657 | Time: 890ms
Epoch: 047 | Train Loss: 0.0076598 | Grad norm: 0.886039 | Time: 22s757ms
Epoch: 047 | Test Loss: 0.0078870 | Time: 803ms
Epoch: 048 | Train Loss: 0.0077253 | Grad norm: 0.992619 | Time: 22s486ms
Epoch: 048 | Test Loss: 0.0078043 | Time: 828ms
Epoch: 049 | Train Loss: 0.0074930 | Grad norm: 0.786895 | Time: 22s992ms
Epoch: 049 | Test Loss: 0.0075550 | Time: 824ms
Epoch: 050 | Train Loss: 0.0074360 | Grad norm: 0.719080 | Time: 19s910ms
Epoch: 050 | Test Loss: 0.0075487 | Time: 828ms
Epoch: 051 | Train Loss: 0.0073903 | Grad norm: 0.693147 | Time: 18s498ms
Epoch: 051 | Test Loss: 0.0074897 | Time: 823ms
==> Save the model at epoch 051 with test loss 0.0074897
Epoch: 052 | Train Loss: 0.0073396 | Grad norm: 0.644657 | Time: 18s257ms
Epoch: 052 | Test Loss: 0.0072763 | Time: 792ms
==> Save the model at epoch 052 with test loss 0.0072763
Epoch: 053 | Train Loss: 0.0073135 | Grad norm: 0.613852 | Time: 18s723ms
Epoch: 053 | Test Loss: 0.0076032 | Time: 825ms
Epoch: 054 | Train Loss: 0.0072567 | Grad norm: 0.582305 | Time: 18s740ms
Epoch: 054 | Test Loss: 0.0073482 | Time: 828ms
Epoch: 055 | Train Loss: 0.0071809 | Grad norm: 0.486927 | Time: 18s773ms
Epoch: 055 | Test Loss: 0.0073071 | Time: 792ms
Epoch: 056 | Train Loss: 0.0071569 | Grad norm: 0.470835 | Time: 18s586ms
Epoch: 056 | Test Loss: 0.0072140 | Time: 787ms
==> Save the model at epoch 056 with test loss 0.0072140
Epoch: 057 | Train Loss: 0.0071337 | Grad norm: 0.427605 | Time: 19s230ms
Epoch: 057 | Test Loss: 0.0071958 | Time: 805ms
==> Save the model at epoch 057 with test loss 0.0071958
Epoch: 058 | Train Loss: 0.0071072 | Grad norm: 0.404083 | Time: 19s262ms
Epoch: 058 | Test Loss: 0.0072023 | Time: 790ms
Epoch: 059 | Train Loss: 0.0070869 | Grad norm: 0.368980 | Time: 19s151ms
Epoch: 059 | Test Loss: 0.0071835 | Time: 781ms
==> Save the model at epoch 059 with test loss 0.0071835
Epoch: 060 | Train Loss: 0.0070743 | Grad norm: 0.352604 | Time: 19s232ms
Epoch: 060 | Test Loss: 0.0071736 | Time: 788ms
==> Save the model at epoch 060 with test loss 0.0071736
Total time: 20m58s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
