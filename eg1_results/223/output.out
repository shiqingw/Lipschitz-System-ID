==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 17.4633501 | L2 loss: 17.4628871 | Lip loss: 0.0004630 | Grad norm: 5.866128 | Time: 20s825ms
Epoch: 001 | Test Loss: 6.7641074 | Time: 606ms
==> Save the model at epoch 001 with test loss 6.7641074
Epoch: 002 | Loss: 0.8431758 | L2 loss: 0.8409604 | Lip loss: 0.0022154 | Grad norm: 4.609815 | Time: 20s617ms
Epoch: 002 | Test Loss: 0.0071349 | Time: 544ms
==> Save the model at epoch 002 with test loss 0.0071349
Epoch: 003 | Loss: 0.0076612 | L2 loss: 0.0055721 | Lip loss: 0.0020891 | Grad norm: 1.335015 | Time: 20s504ms
Epoch: 003 | Test Loss: 0.0051943 | Time: 538ms
==> Save the model at epoch 003 with test loss 0.0051943
Epoch: 004 | Loss: 0.0065276 | L2 loss: 0.0044608 | Lip loss: 0.0020668 | Grad norm: 1.269170 | Time: 20s951ms
Epoch: 004 | Test Loss: 0.0055862 | Time: 615ms
Epoch: 005 | Loss: 0.0061278 | L2 loss: 0.0040642 | Lip loss: 0.0020637 | Grad norm: 1.215188 | Time: 21s29ms
Epoch: 005 | Test Loss: 0.0035736 | Time: 550ms
==> Save the model at epoch 005 with test loss 0.0035736
Epoch: 006 | Loss: 0.0055185 | L2 loss: 0.0034587 | Lip loss: 0.0020598 | Grad norm: 0.341889 | Time: 20s658ms
Epoch: 006 | Test Loss: 0.0034662 | Time: 557ms
==> Save the model at epoch 006 with test loss 0.0034662
Epoch: 007 | Loss: 0.0055014 | L2 loss: 0.0034468 | Lip loss: 0.0020546 | Grad norm: 0.381244 | Time: 21s77ms
Epoch: 007 | Test Loss: 0.0034605 | Time: 546ms
==> Save the model at epoch 007 with test loss 0.0034605
Epoch: 008 | Loss: 0.0054784 | L2 loss: 0.0034204 | Lip loss: 0.0020580 | Grad norm: 0.348239 | Time: 20s874ms
Epoch: 008 | Test Loss: 0.0034141 | Time: 551ms
==> Save the model at epoch 008 with test loss 0.0034141
Epoch: 009 | Loss: 0.0054598 | L2 loss: 0.0034040 | Lip loss: 0.0020558 | Grad norm: 0.346197 | Time: 20s889ms
Epoch: 009 | Test Loss: 0.0033917 | Time: 536ms
==> Save the model at epoch 009 with test loss 0.0033917
Epoch: 010 | Loss: 0.0054506 | L2 loss: 0.0033892 | Lip loss: 0.0020614 | Grad norm: 0.368370 | Time: 20s465ms
Epoch: 010 | Test Loss: 0.0033673 | Time: 556ms
==> Save the model at epoch 010 with test loss 0.0033673
Epoch: 011 | Loss: 0.0054049 | L2 loss: 0.0033495 | Lip loss: 0.0020554 | Grad norm: 0.256480 | Time: 20s738ms
Epoch: 011 | Test Loss: 0.0033647 | Time: 552ms
==> Save the model at epoch 011 with test loss 0.0033647
Epoch: 012 | Loss: 0.0054064 | L2 loss: 0.0033468 | Lip loss: 0.0020597 | Grad norm: 0.251238 | Time: 20s934ms
Epoch: 012 | Test Loss: 0.0033640 | Time: 537ms
==> Save the model at epoch 012 with test loss 0.0033640
Epoch: 013 | Loss: 0.0054014 | L2 loss: 0.0033448 | Lip loss: 0.0020566 | Grad norm: 0.245940 | Time: 21s11ms
Epoch: 013 | Test Loss: 0.0033605 | Time: 540ms
==> Save the model at epoch 013 with test loss 0.0033605
Epoch: 014 | Loss: 0.0053989 | L2 loss: 0.0033431 | Lip loss: 0.0020559 | Grad norm: 0.255000 | Time: 21s80ms
Epoch: 014 | Test Loss: 0.0033581 | Time: 629ms
==> Save the model at epoch 014 with test loss 0.0033581
Epoch: 015 | Loss: 0.0053968 | L2 loss: 0.0033416 | Lip loss: 0.0020551 | Grad norm: 0.256222 | Time: 20s496ms
Epoch: 015 | Test Loss: 0.0033657 | Time: 567ms
Epoch: 016 | Loss: 0.0053946 | L2 loss: 0.0033372 | Lip loss: 0.0020574 | Grad norm: 0.253559 | Time: 21s57ms
Epoch: 016 | Test Loss: 0.0033533 | Time: 543ms
==> Save the model at epoch 016 with test loss 0.0033533
Epoch: 017 | Loss: 0.0053926 | L2 loss: 0.0033356 | Lip loss: 0.0020570 | Grad norm: 0.235141 | Time: 20s268ms
Epoch: 017 | Test Loss: 0.0033536 | Time: 618ms
Epoch: 018 | Loss: 0.0053904 | L2 loss: 0.0033356 | Lip loss: 0.0020548 | Grad norm: 0.234303 | Time: 20s349ms
Epoch: 018 | Test Loss: 0.0033531 | Time: 541ms
==> Save the model at epoch 018 with test loss 0.0033531
Epoch: 019 | Loss: 0.0053894 | L2 loss: 0.0033354 | Lip loss: 0.0020540 | Grad norm: 0.244133 | Time: 20s500ms
Epoch: 019 | Test Loss: 0.0033530 | Time: 544ms
==> Save the model at epoch 019 with test loss 0.0033530
Epoch: 020 | Loss: 0.0053873 | L2 loss: 0.0033348 | Lip loss: 0.0020525 | Grad norm: 0.236592 | Time: 21s289ms
Epoch: 020 | Test Loss: 0.0033530 | Time: 545ms
==> Save the model at epoch 020 with test loss 0.0033530
Epoch: 021 | Loss: 0.0053936 | L2 loss: 0.0033345 | Lip loss: 0.0020591 | Grad norm: 0.233125 | Time: 20s511ms
Epoch: 021 | Test Loss: 0.0033524 | Time: 546ms
==> Save the model at epoch 021 with test loss 0.0033524
Epoch: 022 | Loss: 0.0053883 | L2 loss: 0.0033344 | Lip loss: 0.0020539 | Grad norm: 0.236179 | Time: 20s816ms
Epoch: 022 | Test Loss: 0.0033523 | Time: 548ms
==> Save the model at epoch 022 with test loss 0.0033523
Epoch: 023 | Loss: 0.0053911 | L2 loss: 0.0033341 | Lip loss: 0.0020570 | Grad norm: 0.242259 | Time: 20s149ms
Epoch: 023 | Test Loss: 0.0033523 | Time: 546ms
==> Save the model at epoch 023 with test loss 0.0033523
Epoch: 024 | Loss: 0.0053891 | L2 loss: 0.0033342 | Lip loss: 0.0020550 | Grad norm: 0.244690 | Time: 19s848ms
Epoch: 024 | Test Loss: 0.0033523 | Time: 549ms
Epoch: 025 | Loss: 0.0053921 | L2 loss: 0.0033342 | Lip loss: 0.0020579 | Grad norm: 0.228756 | Time: 20s596ms
Epoch: 025 | Test Loss: 0.0033523 | Time: 537ms
==> Save the model at epoch 025 with test loss 0.0033523
Epoch: 026 | Loss: 0.0053967 | L2 loss: 0.0033340 | Lip loss: 0.0020627 | Grad norm: 0.232293 | Time: 20s726ms
Epoch: 026 | Test Loss: 0.0033523 | Time: 545ms
==> Save the model at epoch 026 with test loss 0.0033523
Epoch: 027 | Loss: 0.0053936 | L2 loss: 0.0033343 | Lip loss: 0.0020593 | Grad norm: 0.233241 | Time: 19s685ms
Epoch: 027 | Test Loss: 0.0033523 | Time: 606ms
==> Save the model at epoch 027 with test loss 0.0033523
Epoch: 028 | Loss: 0.0053916 | L2 loss: 0.0033340 | Lip loss: 0.0020577 | Grad norm: 0.248635 | Time: 20s355ms
Epoch: 028 | Test Loss: 0.0033523 | Time: 563ms
==> Save the model at epoch 028 with test loss 0.0033523
Epoch: 029 | Loss: 0.0053897 | L2 loss: 0.0033340 | Lip loss: 0.0020557 | Grad norm: 0.228403 | Time: 20s579ms
Epoch: 029 | Test Loss: 0.0033523 | Time: 559ms
Epoch: 030 | Loss: 0.0053942 | L2 loss: 0.0033342 | Lip loss: 0.0020601 | Grad norm: 0.226474 | Time: 20s576ms
Epoch: 030 | Test Loss: 0.0033523 | Time: 613ms
==> Save the model at epoch 030 with test loss 0.0033523
Epoch: 031 | Loss: 0.0053901 | L2 loss: 0.0033338 | Lip loss: 0.0020562 | Grad norm: 0.236039 | Time: 20s273ms
Epoch: 031 | Test Loss: 0.0033523 | Time: 540ms
==> Save the model at epoch 031 with test loss 0.0033523
Epoch: 032 | Loss: 0.0053895 | L2 loss: 0.0033341 | Lip loss: 0.0020553 | Grad norm: 0.219246 | Time: 20s477ms
Epoch: 032 | Test Loss: 0.0033523 | Time: 533ms
Epoch: 033 | Loss: 0.0053859 | L2 loss: 0.0033341 | Lip loss: 0.0020518 | Grad norm: 0.229147 | Time: 20s804ms
Epoch: 033 | Test Loss: 0.0033523 | Time: 536ms
Epoch: 034 | Loss: 0.0053921 | L2 loss: 0.0033341 | Lip loss: 0.0020580 | Grad norm: 0.232917 | Time: 20s698ms
Epoch: 034 | Test Loss: 0.0033523 | Time: 564ms
Epoch: 035 | Loss: 0.0053886 | L2 loss: 0.0033343 | Lip loss: 0.0020543 | Grad norm: 0.232737 | Time: 20s605ms
Epoch: 035 | Test Loss: 0.0033523 | Time: 536ms
Epoch: 036 | Loss: 0.0053940 | L2 loss: 0.0033341 | Lip loss: 0.0020600 | Grad norm: 0.223803 | Time: 20s748ms
Epoch: 036 | Test Loss: 0.0033523 | Time: 533ms
Epoch: 037 | Loss: 0.0053897 | L2 loss: 0.0033339 | Lip loss: 0.0020558 | Grad norm: 0.233007 | Time: 20s950ms
Epoch: 037 | Test Loss: 0.0033523 | Time: 565ms
Epoch: 038 | Loss: 0.0053927 | L2 loss: 0.0033341 | Lip loss: 0.0020586 | Grad norm: 0.240443 | Time: 20s637ms
Epoch: 038 | Test Loss: 0.0033523 | Time: 547ms
Epoch: 039 | Loss: 0.0053915 | L2 loss: 0.0033340 | Lip loss: 0.0020574 | Grad norm: 0.232794 | Time: 20s997ms
Epoch: 039 | Test Loss: 0.0033523 | Time: 548ms
Epoch: 040 | Loss: 0.0053923 | L2 loss: 0.0033339 | Lip loss: 0.0020585 | Grad norm: 0.229556 | Time: 20s808ms
Epoch: 040 | Test Loss: 0.0033523 | Time: 565ms
Total time: 14m8s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
