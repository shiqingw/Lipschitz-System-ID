==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 18.3992413 | L2 loss: 18.3981757 | Lip loss: 0.0010655 | Grad norm: 2.731216 | Time: 14s14ms
Epoch: 001 | Test Loss: 14.8089539 | Time: 369ms
==> Save the model at epoch 001 with test loss 14.8089539
Epoch: 002 | Loss: 4.7088366 | L2 loss: 4.6889927 | Lip loss: 0.0198440 | Grad norm: 8.932266 | Time: 14s639ms
Epoch: 002 | Test Loss: 0.0378772 | Time: 365ms
==> Save the model at epoch 002 with test loss 0.0378772
Epoch: 003 | Loss: 0.0335175 | L2 loss: 0.0118718 | Lip loss: 0.0216457 | Grad norm: 1.582241 | Time: 14s68ms
Epoch: 003 | Test Loss: 0.0040184 | Time: 373ms
==> Save the model at epoch 003 with test loss 0.0040184
Epoch: 004 | Loss: 0.0239509 | L2 loss: 0.0030930 | Lip loss: 0.0208579 | Grad norm: 0.883361 | Time: 14s395ms
Epoch: 004 | Test Loss: 0.0022246 | Time: 362ms
==> Save the model at epoch 004 with test loss 0.0022246
Epoch: 005 | Loss: 0.0228221 | L2 loss: 0.0021062 | Lip loss: 0.0207158 | Grad norm: 0.783596 | Time: 14s672ms
Epoch: 005 | Test Loss: 0.0020686 | Time: 368ms
==> Save the model at epoch 005 with test loss 0.0020686
Epoch: 006 | Loss: 0.0222697 | L2 loss: 0.0016346 | Lip loss: 0.0206352 | Grad norm: 0.291793 | Time: 14s725ms
Epoch: 006 | Test Loss: 0.0015823 | Time: 372ms
==> Save the model at epoch 006 with test loss 0.0015823
Epoch: 007 | Loss: 0.0222030 | L2 loss: 0.0015829 | Lip loss: 0.0206202 | Grad norm: 0.250275 | Time: 14s497ms
Epoch: 007 | Test Loss: 0.0015532 | Time: 364ms
==> Save the model at epoch 007 with test loss 0.0015532
Epoch: 008 | Loss: 0.0221682 | L2 loss: 0.0015504 | Lip loss: 0.0206179 | Grad norm: 0.271065 | Time: 14s662ms
Epoch: 008 | Test Loss: 0.0015146 | Time: 375ms
==> Save the model at epoch 008 with test loss 0.0015146
Epoch: 009 | Loss: 0.0220855 | L2 loss: 0.0015121 | Lip loss: 0.0205734 | Grad norm: 0.260727 | Time: 13s926ms
Epoch: 009 | Test Loss: 0.0015070 | Time: 462ms
==> Save the model at epoch 009 with test loss 0.0015070
Epoch: 010 | Loss: 0.0221109 | L2 loss: 0.0014915 | Lip loss: 0.0206194 | Grad norm: 0.292026 | Time: 14s66ms
Epoch: 010 | Test Loss: 0.0015014 | Time: 390ms
==> Save the model at epoch 010 with test loss 0.0015014
Epoch: 011 | Loss: 0.0220342 | L2 loss: 0.0014531 | Lip loss: 0.0205811 | Grad norm: 0.212637 | Time: 14s53ms
Epoch: 011 | Test Loss: 0.0014390 | Time: 379ms
==> Save the model at epoch 011 with test loss 0.0014390
Epoch: 012 | Loss: 0.0220921 | L2 loss: 0.0014461 | Lip loss: 0.0206459 | Grad norm: 0.190654 | Time: 14s516ms
Epoch: 012 | Test Loss: 0.0014384 | Time: 443ms
==> Save the model at epoch 012 with test loss 0.0014384
Epoch: 013 | Loss: 0.0220213 | L2 loss: 0.0014404 | Lip loss: 0.0205809 | Grad norm: 0.199296 | Time: 13s596ms
Epoch: 013 | Test Loss: 0.0014347 | Time: 366ms
==> Save the model at epoch 013 with test loss 0.0014347
Epoch: 014 | Loss: 0.0220578 | L2 loss: 0.0014371 | Lip loss: 0.0206207 | Grad norm: 0.202102 | Time: 14s395ms
Epoch: 014 | Test Loss: 0.0014269 | Time: 439ms
==> Save the model at epoch 014 with test loss 0.0014269
Epoch: 015 | Loss: 0.0220221 | L2 loss: 0.0014327 | Lip loss: 0.0205894 | Grad norm: 0.192273 | Time: 14s464ms
Epoch: 015 | Test Loss: 0.0014223 | Time: 367ms
==> Save the model at epoch 015 with test loss 0.0014223
Epoch: 016 | Loss: 0.0220472 | L2 loss: 0.0014324 | Lip loss: 0.0206147 | Grad norm: 0.192406 | Time: 13s890ms
Epoch: 016 | Test Loss: 0.0014212 | Time: 431ms
==> Save the model at epoch 016 with test loss 0.0014212
Epoch: 017 | Loss: 0.0220145 | L2 loss: 0.0014285 | Lip loss: 0.0205860 | Grad norm: 0.189368 | Time: 14s54ms
Epoch: 017 | Test Loss: 0.0014212 | Time: 360ms
Epoch: 018 | Loss: 0.0220582 | L2 loss: 0.0014295 | Lip loss: 0.0206286 | Grad norm: 0.184420 | Time: 13s869ms
Epoch: 018 | Test Loss: 0.0014199 | Time: 435ms
==> Save the model at epoch 018 with test loss 0.0014199
Epoch: 019 | Loss: 0.0220105 | L2 loss: 0.0014293 | Lip loss: 0.0205812 | Grad norm: 0.203710 | Time: 14s145ms
Epoch: 019 | Test Loss: 0.0014196 | Time: 365ms
==> Save the model at epoch 019 with test loss 0.0014196
Epoch: 020 | Loss: 0.0220358 | L2 loss: 0.0014263 | Lip loss: 0.0206095 | Grad norm: 0.180488 | Time: 14s67ms
Epoch: 020 | Test Loss: 0.0014192 | Time: 366ms
==> Save the model at epoch 020 with test loss 0.0014192
Epoch: 021 | Loss: 0.0220206 | L2 loss: 0.0014264 | Lip loss: 0.0205943 | Grad norm: 0.188075 | Time: 13s783ms
Epoch: 021 | Test Loss: 0.0014190 | Time: 367ms
==> Save the model at epoch 021 with test loss 0.0014190
Epoch: 022 | Loss: 0.0219864 | L2 loss: 0.0014257 | Lip loss: 0.0205608 | Grad norm: 0.190439 | Time: 14s129ms
Epoch: 022 | Test Loss: 0.0014190 | Time: 363ms
==> Save the model at epoch 022 with test loss 0.0014190
Epoch: 023 | Loss: 0.0219770 | L2 loss: 0.0014269 | Lip loss: 0.0205501 | Grad norm: 0.179980 | Time: 14s162ms
Epoch: 023 | Test Loss: 0.0014189 | Time: 371ms
==> Save the model at epoch 023 with test loss 0.0014189
Epoch: 024 | Loss: 0.0220416 | L2 loss: 0.0014251 | Lip loss: 0.0206165 | Grad norm: 0.183707 | Time: 13s624ms
Epoch: 024 | Test Loss: 0.0014189 | Time: 481ms
==> Save the model at epoch 024 with test loss 0.0014189
Epoch: 025 | Loss: 0.0221071 | L2 loss: 0.0014258 | Lip loss: 0.0206813 | Grad norm: 0.175342 | Time: 14s262ms
Epoch: 025 | Test Loss: 0.0014190 | Time: 365ms
Epoch: 026 | Loss: 0.0220066 | L2 loss: 0.0014261 | Lip loss: 0.0205805 | Grad norm: 0.192298 | Time: 13s761ms
Epoch: 026 | Test Loss: 0.0014190 | Time: 369ms
Epoch: 027 | Loss: 0.0220332 | L2 loss: 0.0014257 | Lip loss: 0.0206075 | Grad norm: 0.176803 | Time: 14s488ms
Epoch: 027 | Test Loss: 0.0014190 | Time: 363ms
Epoch: 028 | Loss: 0.0220559 | L2 loss: 0.0014267 | Lip loss: 0.0206293 | Grad norm: 0.190351 | Time: 14s199ms
Epoch: 028 | Test Loss: 0.0014190 | Time: 384ms
Epoch: 029 | Loss: 0.0220070 | L2 loss: 0.0014244 | Lip loss: 0.0205825 | Grad norm: 0.189467 | Time: 14s151ms
Epoch: 029 | Test Loss: 0.0014190 | Time: 369ms
Epoch: 030 | Loss: 0.0220699 | L2 loss: 0.0014276 | Lip loss: 0.0206423 | Grad norm: 0.185090 | Time: 14s455ms
Epoch: 030 | Test Loss: 0.0014190 | Time: 383ms
Epoch: 031 | Loss: 0.0220275 | L2 loss: 0.0014261 | Lip loss: 0.0206015 | Grad norm: 0.191127 | Time: 14s193ms
Epoch: 031 | Test Loss: 0.0014190 | Time: 371ms
Epoch: 032 | Loss: 0.0220397 | L2 loss: 0.0014277 | Lip loss: 0.0206120 | Grad norm: 0.178703 | Time: 14s200ms
Epoch: 032 | Test Loss: 0.0014190 | Time: 387ms
Epoch: 033 | Loss: 0.0220483 | L2 loss: 0.0014275 | Lip loss: 0.0206208 | Grad norm: 0.182578 | Time: 13s721ms
Epoch: 033 | Test Loss: 0.0014190 | Time: 380ms
Epoch: 034 | Loss: 0.0220488 | L2 loss: 0.0014256 | Lip loss: 0.0206232 | Grad norm: 0.189354 | Time: 13s837ms
Epoch: 034 | Test Loss: 0.0014190 | Time: 362ms
Epoch: 035 | Loss: 0.0220215 | L2 loss: 0.0014265 | Lip loss: 0.0205949 | Grad norm: 0.193658 | Time: 14s509ms
Epoch: 035 | Test Loss: 0.0014190 | Time: 367ms
Epoch: 036 | Loss: 0.0220314 | L2 loss: 0.0014257 | Lip loss: 0.0206057 | Grad norm: 0.185349 | Time: 13s976ms
Epoch: 036 | Test Loss: 0.0014190 | Time: 379ms
Epoch: 037 | Loss: 0.0220623 | L2 loss: 0.0014253 | Lip loss: 0.0206369 | Grad norm: 0.185067 | Time: 13s861ms
Epoch: 037 | Test Loss: 0.0014190 | Time: 376ms
Epoch: 038 | Loss: 0.0220042 | L2 loss: 0.0014276 | Lip loss: 0.0205767 | Grad norm: 0.176293 | Time: 13s922ms
Epoch: 038 | Test Loss: 0.0014190 | Time: 373ms
Epoch: 039 | Loss: 0.0220053 | L2 loss: 0.0014262 | Lip loss: 0.0205790 | Grad norm: 0.185266 | Time: 14s205ms
Epoch: 039 | Test Loss: 0.0014190 | Time: 369ms
Epoch: 040 | Loss: 0.0219935 | L2 loss: 0.0014265 | Lip loss: 0.0205670 | Grad norm: 0.179471 | Time: 14s363ms
Epoch: 040 | Test Loss: 0.0014190 | Time: 363ms
Total time: 9m41s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
