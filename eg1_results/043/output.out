==> torch device:  cuda:2
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (M): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.25
==> Start training...
Epoch: 001 | Loss: 10.2406092 | L2 loss: 10.2236621 | Lip loss: 0.0169472 | Grad norm: 21.939908 | Time: 12s446ms
Epoch: 001 | Test Loss: 0.1810373 | Time: 538ms
==> Save the model at epoch 001 with test loss 0.1810373
Epoch: 002 | Loss: 0.1033204 | L2 loss: 0.0819798 | Lip loss: 0.0213406 | Grad norm: 16.770180 | Time: 11s801ms
Epoch: 002 | Test Loss: 0.0412075 | Time: 531ms
==> Save the model at epoch 002 with test loss 0.0412075
Epoch: 003 | Loss: 0.0586405 | L2 loss: 0.0378474 | Lip loss: 0.0207931 | Grad norm: 11.115673 | Time: 11s601ms
Epoch: 003 | Test Loss: 0.0294734 | Time: 530ms
==> Save the model at epoch 003 with test loss 0.0294734
Epoch: 004 | Loss: 0.0618599 | L2 loss: 0.0411141 | Lip loss: 0.0207458 | Grad norm: 9.090471 | Time: 11s542ms
Epoch: 004 | Test Loss: 0.0609527 | Time: 531ms
Epoch: 005 | Loss: 0.0815401 | L2 loss: 0.0607553 | Lip loss: 0.0207848 | Grad norm: 10.240034 | Time: 12s554ms
Epoch: 005 | Test Loss: 0.0433947 | Time: 530ms
Epoch: 006 | Loss: 0.0323283 | L2 loss: 0.0117455 | Lip loss: 0.0205827 | Grad norm: 1.778165 | Time: 12s623ms
Epoch: 006 | Test Loss: 0.0087055 | Time: 536ms
==> Save the model at epoch 006 with test loss 0.0087055
Epoch: 007 | Loss: 0.0288960 | L2 loss: 0.0084162 | Lip loss: 0.0204798 | Grad norm: 0.715073 | Time: 13s116ms
Epoch: 007 | Test Loss: 0.0085088 | Time: 544ms
==> Save the model at epoch 007 with test loss 0.0085088
Epoch: 008 | Loss: 0.0287840 | L2 loss: 0.0083561 | Lip loss: 0.0204279 | Grad norm: 0.890058 | Time: 11s559ms
Epoch: 008 | Test Loss: 0.0088707 | Time: 596ms
Epoch: 009 | Loss: 0.0287019 | L2 loss: 0.0082649 | Lip loss: 0.0204371 | Grad norm: 0.972567 | Time: 11s575ms
Epoch: 009 | Test Loss: 0.0091743 | Time: 536ms
Epoch: 010 | Loss: 0.0285432 | L2 loss: 0.0081647 | Lip loss: 0.0203785 | Grad norm: 0.928121 | Time: 11s568ms
Epoch: 010 | Test Loss: 0.0080959 | Time: 532ms
==> Save the model at epoch 010 with test loss 0.0080959
Epoch: 011 | Loss: 0.0280525 | L2 loss: 0.0076569 | Lip loss: 0.0203956 | Grad norm: 0.424084 | Time: 11s611ms
Epoch: 011 | Test Loss: 0.0078582 | Time: 532ms
==> Save the model at epoch 011 with test loss 0.0078582
Epoch: 012 | Loss: 0.0279852 | L2 loss: 0.0076200 | Lip loss: 0.0203652 | Grad norm: 0.394718 | Time: 13s13ms
Epoch: 012 | Test Loss: 0.0078800 | Time: 559ms
Epoch: 013 | Loss: 0.0279398 | L2 loss: 0.0075888 | Lip loss: 0.0203511 | Grad norm: 0.365907 | Time: 12s710ms
Epoch: 013 | Test Loss: 0.0078563 | Time: 542ms
==> Save the model at epoch 013 with test loss 0.0078563
Epoch: 014 | Loss: 0.0279880 | L2 loss: 0.0076096 | Lip loss: 0.0203784 | Grad norm: 0.409069 | Time: 12s636ms
Epoch: 014 | Test Loss: 0.0078253 | Time: 543ms
==> Save the model at epoch 014 with test loss 0.0078253
Epoch: 015 | Loss: 0.0279351 | L2 loss: 0.0075865 | Lip loss: 0.0203487 | Grad norm: 0.408474 | Time: 12s474ms
Epoch: 015 | Test Loss: 0.0078205 | Time: 552ms
==> Save the model at epoch 015 with test loss 0.0078205
Epoch: 016 | Loss: 0.0279086 | L2 loss: 0.0075454 | Lip loss: 0.0203632 | Grad norm: 0.365922 | Time: 11s928ms
Epoch: 016 | Test Loss: 0.0078104 | Time: 597ms
==> Save the model at epoch 016 with test loss 0.0078104
Epoch: 017 | Loss: 0.0278885 | L2 loss: 0.0075428 | Lip loss: 0.0203457 | Grad norm: 0.319698 | Time: 11s530ms
Epoch: 017 | Test Loss: 0.0078041 | Time: 537ms
==> Save the model at epoch 017 with test loss 0.0078041
Epoch: 018 | Loss: 0.0278875 | L2 loss: 0.0075431 | Lip loss: 0.0203444 | Grad norm: 0.341268 | Time: 11s542ms
Epoch: 018 | Test Loss: 0.0078021 | Time: 537ms
==> Save the model at epoch 018 with test loss 0.0078021
Epoch: 019 | Loss: 0.0279465 | L2 loss: 0.0075472 | Lip loss: 0.0203993 | Grad norm: 0.376083 | Time: 12s263ms
Epoch: 019 | Test Loss: 0.0078013 | Time: 597ms
==> Save the model at epoch 019 with test loss 0.0078013
Epoch: 020 | Loss: 0.0279420 | L2 loss: 0.0075431 | Lip loss: 0.0203989 | Grad norm: 0.338758 | Time: 11s495ms
Epoch: 020 | Test Loss: 0.0077989 | Time: 535ms
==> Save the model at epoch 020 with test loss 0.0077989
Epoch: 021 | Loss: 0.0279465 | L2 loss: 0.0075379 | Lip loss: 0.0204087 | Grad norm: 0.341785 | Time: 11s499ms
Epoch: 021 | Test Loss: 0.0077976 | Time: 535ms
==> Save the model at epoch 021 with test loss 0.0077976
Epoch: 022 | Loss: 0.0278922 | L2 loss: 0.0075306 | Lip loss: 0.0203616 | Grad norm: 0.332143 | Time: 11s557ms
Epoch: 022 | Test Loss: 0.0077975 | Time: 535ms
==> Save the model at epoch 022 with test loss 0.0077975
Epoch: 023 | Loss: 0.0278873 | L2 loss: 0.0075353 | Lip loss: 0.0203520 | Grad norm: 0.354485 | Time: 11s495ms
Epoch: 023 | Test Loss: 0.0077979 | Time: 537ms
Epoch: 024 | Loss: 0.0279402 | L2 loss: 0.0075336 | Lip loss: 0.0204067 | Grad norm: 0.330981 | Time: 11s498ms
Epoch: 024 | Test Loss: 0.0077977 | Time: 538ms
Epoch: 025 | Loss: 0.0278999 | L2 loss: 0.0075341 | Lip loss: 0.0203658 | Grad norm: 0.331933 | Time: 11s558ms
Epoch: 025 | Test Loss: 0.0077981 | Time: 536ms
Epoch: 026 | Loss: 0.0278960 | L2 loss: 0.0075343 | Lip loss: 0.0203617 | Grad norm: 0.344030 | Time: 11s729ms
Epoch: 026 | Test Loss: 0.0077981 | Time: 571ms
Epoch: 027 | Loss: 0.0279076 | L2 loss: 0.0075319 | Lip loss: 0.0203757 | Grad norm: 0.350353 | Time: 12s402ms
Epoch: 027 | Test Loss: 0.0077981 | Time: 598ms
Epoch: 028 | Loss: 0.0278780 | L2 loss: 0.0075334 | Lip loss: 0.0203446 | Grad norm: 0.307031 | Time: 12s467ms
Epoch: 028 | Test Loss: 0.0077980 | Time: 543ms
Epoch: 029 | Loss: 0.0278735 | L2 loss: 0.0075348 | Lip loss: 0.0203387 | Grad norm: 0.337042 | Time: 12s473ms
Epoch: 029 | Test Loss: 0.0077980 | Time: 530ms
Epoch: 030 | Loss: 0.0279063 | L2 loss: 0.0075469 | Lip loss: 0.0203594 | Grad norm: 0.338241 | Time: 12s486ms
Epoch: 030 | Test Loss: 0.0077980 | Time: 591ms
Epoch: 031 | Loss: 0.0279131 | L2 loss: 0.0075451 | Lip loss: 0.0203680 | Grad norm: 0.344564 | Time: 11s511ms
Epoch: 031 | Test Loss: 0.0077980 | Time: 537ms
Epoch: 032 | Loss: 0.0279433 | L2 loss: 0.0075309 | Lip loss: 0.0204124 | Grad norm: 0.327334 | Time: 11s517ms
Epoch: 032 | Test Loss: 0.0077980 | Time: 537ms
Epoch: 033 | Loss: 0.0279404 | L2 loss: 0.0075354 | Lip loss: 0.0204050 | Grad norm: 0.323088 | Time: 11s581ms
Epoch: 033 | Test Loss: 0.0077980 | Time: 539ms
Epoch: 034 | Loss: 0.0278531 | L2 loss: 0.0075364 | Lip loss: 0.0203167 | Grad norm: 0.343682 | Time: 11s517ms
Epoch: 034 | Test Loss: 0.0077980 | Time: 534ms
Epoch: 035 | Loss: 0.0278632 | L2 loss: 0.0075333 | Lip loss: 0.0203300 | Grad norm: 0.318818 | Time: 11s525ms
Epoch: 035 | Test Loss: 0.0077980 | Time: 594ms
Epoch: 036 | Loss: 0.0279745 | L2 loss: 0.0075394 | Lip loss: 0.0204351 | Grad norm: 0.325183 | Time: 11s537ms
Epoch: 036 | Test Loss: 0.0077980 | Time: 529ms
Epoch: 037 | Loss: 0.0279067 | L2 loss: 0.0075419 | Lip loss: 0.0203647 | Grad norm: 0.349888 | Time: 11s525ms
Epoch: 037 | Test Loss: 0.0077980 | Time: 533ms
Epoch: 038 | Loss: 0.0278726 | L2 loss: 0.0075335 | Lip loss: 0.0203391 | Grad norm: 0.318503 | Time: 11s586ms
Epoch: 038 | Test Loss: 0.0077980 | Time: 532ms
Epoch: 039 | Loss: 0.0279197 | L2 loss: 0.0075353 | Lip loss: 0.0203844 | Grad norm: 0.329755 | Time: 11s543ms
Epoch: 039 | Test Loss: 0.0077980 | Time: 533ms
Epoch: 040 | Loss: 0.0279021 | L2 loss: 0.0075332 | Lip loss: 0.0203689 | Grad norm: 0.340250 | Time: 12s818ms
Epoch: 040 | Test Loss: 0.0077980 | Time: 667ms
Total time: 8m19s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
