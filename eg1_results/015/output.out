==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Lipschitz constant: 4.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.6513385 0.6513385]
==> Ouput transform to be applied to the neural network:
[3.0861 3.0861]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 19.5698963 | Grad norm: 19.146498 | Time: 3s889ms
Epoch: 001 | Test Loss: 18.8103695 | Time: 470ms
==> Save the model at epoch 001 with test loss 18.8103695
Epoch: 002 | Train Loss: 5.7510764 | Grad norm: 16.134131 | Time: 3s449ms
Epoch: 002 | Test Loss: 0.0129974 | Time: 472ms
==> Save the model at epoch 002 with test loss 0.0129974
Epoch: 003 | Train Loss: 0.0133536 | Grad norm: 5.636430 | Time: 3s420ms
Epoch: 003 | Test Loss: 0.0107036 | Time: 471ms
==> Save the model at epoch 003 with test loss 0.0107036
Epoch: 004 | Train Loss: 0.0333559 | Grad norm: 9.070878 | Time: 3s579ms
Epoch: 004 | Test Loss: 0.0055335 | Time: 538ms
==> Save the model at epoch 004 with test loss 0.0055335
Epoch: 005 | Train Loss: 0.0187071 | Grad norm: 6.712463 | Time: 3s433ms
Epoch: 005 | Test Loss: 0.0140864 | Time: 478ms
Epoch: 006 | Train Loss: 0.0353000 | Grad norm: 8.062067 | Time: 3s409ms
Epoch: 006 | Test Loss: 0.0161765 | Time: 503ms
Epoch: 007 | Train Loss: 0.0355955 | Grad norm: 8.580788 | Time: 3s406ms
Epoch: 007 | Test Loss: 0.0418387 | Time: 468ms
Epoch: 008 | Train Loss: 0.0505853 | Grad norm: 9.016079 | Time: 3s574ms
Epoch: 008 | Test Loss: 0.0205275 | Time: 489ms
Epoch: 009 | Train Loss: 0.0518762 | Grad norm: 8.685911 | Time: 3s510ms
Epoch: 009 | Test Loss: 0.0893390 | Time: 490ms
Epoch: 010 | Train Loss: 0.0515824 | Grad norm: 9.628510 | Time: 3s454ms
Epoch: 010 | Test Loss: 0.0598184 | Time: 487ms
Epoch: 011 | Train Loss: 0.1070283 | Grad norm: 13.121506 | Time: 3s299ms
Epoch: 011 | Test Loss: 0.1074494 | Time: 503ms
Epoch: 012 | Train Loss: 0.0984031 | Grad norm: 14.191903 | Time: 3s427ms
Epoch: 012 | Test Loss: 0.1295464 | Time: 471ms
Epoch: 013 | Train Loss: 0.0828265 | Grad norm: 12.523236 | Time: 3s286ms
Epoch: 013 | Test Loss: 0.0405484 | Time: 533ms
Epoch: 014 | Train Loss: 0.0710223 | Grad norm: 11.230515 | Time: 3s505ms
Epoch: 014 | Test Loss: 0.0538768 | Time: 476ms
Epoch: 015 | Train Loss: 0.0633369 | Grad norm: 10.262108 | Time: 3s416ms
Epoch: 015 | Test Loss: 0.1122792 | Time: 482ms
Epoch: 016 | Train Loss: 0.0444816 | Grad norm: 7.974115 | Time: 3s542ms
Epoch: 016 | Test Loss: 0.0363932 | Time: 470ms
Epoch: 017 | Train Loss: 0.0173158 | Grad norm: 4.974423 | Time: 3s474ms
Epoch: 017 | Test Loss: 0.0171542 | Time: 473ms
Epoch: 018 | Train Loss: 0.0159769 | Grad norm: 4.599069 | Time: 3s527ms
Epoch: 018 | Test Loss: 0.0150414 | Time: 537ms
Epoch: 019 | Train Loss: 0.0140138 | Grad norm: 4.332779 | Time: 3s448ms
Epoch: 019 | Test Loss: 0.0112543 | Time: 474ms
Epoch: 020 | Train Loss: 0.0148309 | Grad norm: 4.047736 | Time: 3s356ms
Epoch: 020 | Test Loss: 0.0069227 | Time: 478ms
Epoch: 021 | Train Loss: 0.0127309 | Grad norm: 3.802158 | Time: 3s299ms
Epoch: 021 | Test Loss: 0.0126059 | Time: 487ms
Epoch: 022 | Train Loss: 0.0106726 | Grad norm: 3.203091 | Time: 3s440ms
Epoch: 022 | Test Loss: 0.0078232 | Time: 491ms
Epoch: 023 | Train Loss: 0.0083715 | Grad norm: 3.220136 | Time: 3s319ms
Epoch: 023 | Test Loss: 0.0083310 | Time: 486ms
Epoch: 024 | Train Loss: 0.0064015 | Grad norm: 2.967406 | Time: 3s339ms
Epoch: 024 | Test Loss: 0.0073605 | Time: 466ms
Epoch: 025 | Train Loss: 0.0054666 | Grad norm: 2.612454 | Time: 3s415ms
Epoch: 025 | Test Loss: 0.0038270 | Time: 477ms
==> Save the model at epoch 025 with test loss 0.0038270
Epoch: 026 | Train Loss: 0.0063700 | Grad norm: 2.042918 | Time: 3s379ms
Epoch: 026 | Test Loss: 0.0053879 | Time: 474ms
Epoch: 027 | Train Loss: 0.0032852 | Grad norm: 1.368149 | Time: 3s262ms
Epoch: 027 | Test Loss: 0.0035254 | Time: 473ms
==> Save the model at epoch 027 with test loss 0.0035254
Epoch: 028 | Train Loss: 0.0026922 | Grad norm: 1.393399 | Time: 3s396ms
Epoch: 028 | Test Loss: 0.0012718 | Time: 471ms
==> Save the model at epoch 028 with test loss 0.0012718
Epoch: 029 | Train Loss: 0.0016341 | Grad norm: 0.895860 | Time: 3s294ms
Epoch: 029 | Test Loss: 0.0009010 | Time: 472ms
==> Save the model at epoch 029 with test loss 0.0009010
Epoch: 030 | Train Loss: 0.0009504 | Grad norm: 0.547537 | Time: 3s243ms
Epoch: 030 | Test Loss: 0.0011560 | Time: 479ms
Epoch: 031 | Train Loss: 0.0009475 | Grad norm: 0.603890 | Time: 3s323ms
Epoch: 031 | Test Loss: 0.0014116 | Time: 469ms
Epoch: 032 | Train Loss: 0.0007720 | Grad norm: 0.348380 | Time: 3s444ms
Epoch: 032 | Test Loss: 0.0007208 | Time: 544ms
==> Save the model at epoch 032 with test loss 0.0007208
Epoch: 033 | Train Loss: 0.0007180 | Grad norm: 0.357630 | Time: 3s344ms
Epoch: 033 | Test Loss: 0.0007079 | Time: 471ms
==> Save the model at epoch 033 with test loss 0.0007079
Epoch: 034 | Train Loss: 0.0006599 | Grad norm: 0.252102 | Time: 3s422ms
Epoch: 034 | Test Loss: 0.0006545 | Time: 475ms
==> Save the model at epoch 034 with test loss 0.0006545
Epoch: 035 | Train Loss: 0.0006339 | Grad norm: 0.205482 | Time: 3s608ms
Epoch: 035 | Test Loss: 0.0006223 | Time: 474ms
==> Save the model at epoch 035 with test loss 0.0006223
Epoch: 036 | Train Loss: 0.0006049 | Grad norm: 0.149854 | Time: 3s530ms
Epoch: 036 | Test Loss: 0.0006214 | Time: 479ms
==> Save the model at epoch 036 with test loss 0.0006214
Epoch: 037 | Train Loss: 0.0005932 | Grad norm: 0.122772 | Time: 3s582ms
Epoch: 037 | Test Loss: 0.0006060 | Time: 484ms
==> Save the model at epoch 037 with test loss 0.0006060
Epoch: 038 | Train Loss: 0.0005935 | Grad norm: 0.150982 | Time: 3s530ms
Epoch: 038 | Test Loss: 0.0006091 | Time: 515ms
Epoch: 039 | Train Loss: 0.0005813 | Grad norm: 0.095607 | Time: 3s452ms
Epoch: 039 | Test Loss: 0.0006048 | Time: 475ms
==> Save the model at epoch 039 with test loss 0.0006048
Epoch: 040 | Train Loss: 0.0005796 | Grad norm: 0.089801 | Time: 3s851ms
Epoch: 040 | Test Loss: 0.0006026 | Time: 537ms
==> Save the model at epoch 040 with test loss 0.0006026
Total time: 2m37s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.6513385 0.6513385]
==> Output transform to be applied to the neural network (trained):
[3.0861 3.0861]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
