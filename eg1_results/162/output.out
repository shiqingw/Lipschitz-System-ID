==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.6219679 0.6219679]
==> Ouput transform to be applied to the neural network:
[3.225  3.2249]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─ReLU: 2-2                         [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─ReLU: 2-4                         [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─ReLU: 2-6                         [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─ReLU: 2-8                         [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─ReLU: 2-10                        [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─ReLU: 2-12                        [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─ReLU: 2-14                        [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─Linear: 2-16                      [1, 64]                   (recursive)
│    └─ReLU: 2-17                        [1, 64]                   --
│    └─Linear: 2-18                      [1, 64]                   (recursive)
│    └─ReLU: 2-19                        [1, 64]                   --
│    └─Linear: 2-20                      [1, 64]                   (recursive)
│    └─ReLU: 2-21                        [1, 64]                   --
│    └─Linear: 2-22                      [1, 64]                   (recursive)
│    └─ReLU: 2-23                        [1, 64]                   --
│    └─Linear: 2-24                      [1, 64]                   (recursive)
│    └─ReLU: 2-25                        [1, 64]                   --
│    └─Linear: 2-26                      [1, 64]                   (recursive)
│    └─ReLU: 2-27                        [1, 64]                   --
│    └─Linear: 2-28                      [1, 64]                   (recursive)
│    └─ReLU: 2-29                        [1, 64]                   --
│    └─Linear: 2-30                      [1, 2]                    (recursive)
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.05
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.10
Estimated Total Size (MB): 0.11
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 20.8414497 | Grad norm: 1.132178 | Time: 4s991ms
Epoch: 001 | Test Loss: 20.6262419 | Time: 607ms
==> Save the model at epoch 001 with test loss 20.6262419
Epoch: 002 | Train Loss: 0.9454041 | Grad norm: 3.421766 | Time: 4s851ms
Epoch: 002 | Test Loss: 0.0085128 | Time: 573ms
==> Save the model at epoch 002 with test loss 0.0085128
Epoch: 003 | Train Loss: 0.0210451 | Grad norm: 2.004585 | Time: 4s768ms
Epoch: 003 | Test Loss: 0.0135033 | Time: 559ms
Epoch: 004 | Train Loss: 0.0281993 | Grad norm: 2.458083 | Time: 4s900ms
Epoch: 004 | Test Loss: 0.0192184 | Time: 538ms
Epoch: 005 | Train Loss: 0.0455024 | Grad norm: 3.363396 | Time: 4s843ms
Epoch: 005 | Test Loss: 0.0395511 | Time: 600ms
Epoch: 006 | Train Loss: 0.0529549 | Grad norm: 3.722034 | Time: 4s937ms
Epoch: 006 | Test Loss: 0.0276966 | Time: 552ms
Epoch: 007 | Train Loss: 0.0423814 | Grad norm: 3.462238 | Time: 4s981ms
Epoch: 007 | Test Loss: 0.0264279 | Time: 565ms
Epoch: 008 | Train Loss: 0.0542888 | Grad norm: 3.929432 | Time: 4s792ms
Epoch: 008 | Test Loss: 0.0461098 | Time: 580ms
Epoch: 009 | Train Loss: 0.0449270 | Grad norm: 3.691074 | Time: 4s841ms
Epoch: 009 | Test Loss: 0.0531154 | Time: 610ms
Epoch: 010 | Train Loss: 0.0675995 | Grad norm: 4.318752 | Time: 4s998ms
Epoch: 010 | Test Loss: 0.0121508 | Time: 551ms
Epoch: 011 | Train Loss: 0.0482897 | Grad norm: 3.769740 | Time: 5s93ms
Epoch: 011 | Test Loss: 0.0213348 | Time: 550ms
Epoch: 012 | Train Loss: 0.0347421 | Grad norm: 3.165136 | Time: 4s909ms
Epoch: 012 | Test Loss: 0.0467053 | Time: 655ms
Epoch: 013 | Train Loss: 0.0263212 | Grad norm: 2.792587 | Time: 4s897ms
Epoch: 013 | Test Loss: 0.0166260 | Time: 616ms
Epoch: 014 | Train Loss: 0.0259271 | Grad norm: 2.742361 | Time: 4s849ms
Epoch: 014 | Test Loss: 0.0080455 | Time: 534ms
==> Save the model at epoch 014 with test loss 0.0080455
Epoch: 015 | Train Loss: 0.0221175 | Grad norm: 2.478969 | Time: 5s19ms
Epoch: 015 | Test Loss: 0.0133504 | Time: 541ms
Epoch: 016 | Train Loss: 0.0181937 | Grad norm: 2.192557 | Time: 4s774ms
Epoch: 016 | Test Loss: 0.0143193 | Time: 536ms
Epoch: 017 | Train Loss: 0.0176255 | Grad norm: 2.122558 | Time: 4s721ms
Epoch: 017 | Test Loss: 0.0118298 | Time: 536ms
Epoch: 018 | Train Loss: 0.0125256 | Grad norm: 1.729781 | Time: 4s852ms
Epoch: 018 | Test Loss: 0.0076043 | Time: 548ms
==> Save the model at epoch 018 with test loss 0.0076043
Epoch: 019 | Train Loss: 0.0127122 | Grad norm: 1.663508 | Time: 5s88ms
Epoch: 019 | Test Loss: 0.0155962 | Time: 530ms
Epoch: 020 | Train Loss: 0.0102297 | Grad norm: 1.460389 | Time: 4s920ms
Epoch: 020 | Test Loss: 0.0050032 | Time: 540ms
==> Save the model at epoch 020 with test loss 0.0050032
Epoch: 021 | Train Loss: 0.0098181 | Grad norm: 1.445786 | Time: 4s927ms
Epoch: 021 | Test Loss: 0.0078184 | Time: 552ms
Epoch: 022 | Train Loss: 0.0066586 | Grad norm: 1.019597 | Time: 5s136ms
Epoch: 022 | Test Loss: 0.0127043 | Time: 537ms
Epoch: 023 | Train Loss: 0.0070740 | Grad norm: 1.118253 | Time: 4s847ms
Epoch: 023 | Test Loss: 0.0047359 | Time: 555ms
==> Save the model at epoch 023 with test loss 0.0047359
Epoch: 024 | Train Loss: 0.0060247 | Grad norm: 0.970745 | Time: 4s558ms
Epoch: 024 | Test Loss: 0.0054471 | Time: 547ms
Epoch: 025 | Train Loss: 0.0051319 | Grad norm: 0.821619 | Time: 4s914ms
Epoch: 025 | Test Loss: 0.0050243 | Time: 552ms
Epoch: 026 | Train Loss: 0.0049848 | Grad norm: 0.752308 | Time: 4s988ms
Epoch: 026 | Test Loss: 0.0062774 | Time: 636ms
Epoch: 027 | Train Loss: 0.0046224 | Grad norm: 0.670830 | Time: 4s774ms
Epoch: 027 | Test Loss: 0.0041788 | Time: 569ms
==> Save the model at epoch 027 with test loss 0.0041788
Epoch: 028 | Train Loss: 0.0044428 | Grad norm: 0.651662 | Time: 5s198ms
Epoch: 028 | Test Loss: 0.0047060 | Time: 536ms
Epoch: 029 | Train Loss: 0.0041990 | Grad norm: 0.563232 | Time: 4s956ms
Epoch: 029 | Test Loss: 0.0034804 | Time: 535ms
==> Save the model at epoch 029 with test loss 0.0034804
Epoch: 030 | Train Loss: 0.0041086 | Grad norm: 0.528244 | Time: 4s961ms
Epoch: 030 | Test Loss: 0.0036428 | Time: 604ms
Epoch: 031 | Train Loss: 0.0038954 | Grad norm: 0.449986 | Time: 4s613ms
Epoch: 031 | Test Loss: 0.0041967 | Time: 540ms
Epoch: 032 | Train Loss: 0.0038873 | Grad norm: 0.457064 | Time: 4s957ms
Epoch: 032 | Test Loss: 0.0037039 | Time: 534ms
Epoch: 033 | Train Loss: 0.0037714 | Grad norm: 0.390559 | Time: 5s8ms
Epoch: 033 | Test Loss: 0.0032920 | Time: 547ms
==> Save the model at epoch 033 with test loss 0.0032920
Epoch: 034 | Train Loss: 0.0036628 | Grad norm: 0.358364 | Time: 4s730ms
Epoch: 034 | Test Loss: 0.0036118 | Time: 605ms
Epoch: 035 | Train Loss: 0.0036029 | Grad norm: 0.315889 | Time: 4s905ms
Epoch: 035 | Test Loss: 0.0035681 | Time: 543ms
Epoch: 036 | Train Loss: 0.0035369 | Grad norm: 0.265562 | Time: 5s169ms
Epoch: 036 | Test Loss: 0.0034032 | Time: 549ms
Epoch: 037 | Train Loss: 0.0034777 | Grad norm: 0.240727 | Time: 4s889ms
Epoch: 037 | Test Loss: 0.0035234 | Time: 548ms
Epoch: 038 | Train Loss: 0.0034902 | Grad norm: 0.208091 | Time: 4s906ms
Epoch: 038 | Test Loss: 0.0033874 | Time: 538ms
Epoch: 039 | Train Loss: 0.0034171 | Grad norm: 0.182340 | Time: 4s938ms
Epoch: 039 | Test Loss: 0.0033839 | Time: 536ms
Epoch: 040 | Train Loss: 0.0034128 | Grad norm: 0.147233 | Time: 5s142ms
Epoch: 040 | Test Loss: 0.0034086 | Time: 536ms
Total time: 3m39s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.6219679 0.6219679]
==> Output transform to be applied to the neural network (trained):
[3.225  3.2249]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
