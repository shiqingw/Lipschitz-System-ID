==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 19.3949580 | L2 loss: 19.3949571 | Lip loss: 0.0000010 | Grad norm: 0.887028 | Time: 7s360ms
Epoch: 001 | Test Loss: 18.4881163 | Time: 379ms
==> Save the model at epoch 001 with test loss 18.4881163
Epoch: 002 | Loss: 18.2525617 | L2 loss: 18.2525424 | Lip loss: 0.0000193 | Grad norm: 4.078650 | Time: 7s72ms
Epoch: 002 | Test Loss: 15.4279318 | Time: 365ms
==> Save the model at epoch 002 with test loss 15.4279318
Epoch: 003 | Loss: 9.8223874 | L2 loss: 9.8222507 | Lip loss: 0.0001367 | Grad norm: 14.828043 | Time: 7s54ms
Epoch: 003 | Test Loss: 1.9617731 | Time: 370ms
==> Save the model at epoch 003 with test loss 1.9617731
Epoch: 004 | Loss: 0.3588729 | L2 loss: 0.3586055 | Lip loss: 0.0002674 | Grad norm: 4.854398 | Time: 7s175ms
Epoch: 004 | Test Loss: 0.0712783 | Time: 373ms
==> Save the model at epoch 004 with test loss 0.0712783
Epoch: 005 | Loss: 0.0369953 | L2 loss: 0.0367633 | Lip loss: 0.0002320 | Grad norm: 2.091481 | Time: 7s146ms
Epoch: 005 | Test Loss: 0.0143104 | Time: 381ms
==> Save the model at epoch 005 with test loss 0.0143104
Epoch: 006 | Loss: 0.0135084 | L2 loss: 0.0132867 | Lip loss: 0.0002217 | Grad norm: 0.639514 | Time: 6s845ms
Epoch: 006 | Test Loss: 0.0127554 | Time: 441ms
==> Save the model at epoch 006 with test loss 0.0127554
Epoch: 007 | Loss: 0.0121481 | L2 loss: 0.0119282 | Lip loss: 0.0002199 | Grad norm: 0.569353 | Time: 6s901ms
Epoch: 007 | Test Loss: 0.0115535 | Time: 364ms
==> Save the model at epoch 007 with test loss 0.0115535
Epoch: 008 | Loss: 0.0111378 | L2 loss: 0.0109148 | Lip loss: 0.0002230 | Grad norm: 0.549058 | Time: 6s786ms
Epoch: 008 | Test Loss: 0.0105511 | Time: 375ms
==> Save the model at epoch 008 with test loss 0.0105511
Epoch: 009 | Loss: 0.0103444 | L2 loss: 0.0101242 | Lip loss: 0.0002202 | Grad norm: 0.528128 | Time: 6s798ms
Epoch: 009 | Test Loss: 0.0097759 | Time: 372ms
==> Save the model at epoch 009 with test loss 0.0097759
Epoch: 010 | Loss: 0.0095817 | L2 loss: 0.0093643 | Lip loss: 0.0002175 | Grad norm: 0.484204 | Time: 7s74ms
Epoch: 010 | Test Loss: 0.0091052 | Time: 376ms
==> Save the model at epoch 010 with test loss 0.0091052
Epoch: 011 | Loss: 0.0091992 | L2 loss: 0.0089831 | Lip loss: 0.0002161 | Grad norm: 0.435682 | Time: 6s843ms
Epoch: 011 | Test Loss: 0.0090471 | Time: 364ms
==> Save the model at epoch 011 with test loss 0.0090471
Epoch: 012 | Loss: 0.0091549 | L2 loss: 0.0089373 | Lip loss: 0.0002177 | Grad norm: 0.422112 | Time: 7s239ms
Epoch: 012 | Test Loss: 0.0089851 | Time: 370ms
==> Save the model at epoch 012 with test loss 0.0089851
Epoch: 013 | Loss: 0.0091115 | L2 loss: 0.0088944 | Lip loss: 0.0002171 | Grad norm: 0.399552 | Time: 7s133ms
Epoch: 013 | Test Loss: 0.0089165 | Time: 433ms
==> Save the model at epoch 013 with test loss 0.0089165
Epoch: 014 | Loss: 0.0090867 | L2 loss: 0.0088686 | Lip loss: 0.0002181 | Grad norm: 0.434652 | Time: 7s70ms
Epoch: 014 | Test Loss: 0.0088605 | Time: 365ms
==> Save the model at epoch 014 with test loss 0.0088605
Epoch: 015 | Loss: 0.0089839 | L2 loss: 0.0087649 | Lip loss: 0.0002191 | Grad norm: 0.406804 | Time: 7s8ms
Epoch: 015 | Test Loss: 0.0087853 | Time: 374ms
==> Save the model at epoch 015 with test loss 0.0087853
Epoch: 016 | Loss: 0.0089314 | L2 loss: 0.0087137 | Lip loss: 0.0002178 | Grad norm: 0.380906 | Time: 7s240ms
Epoch: 016 | Test Loss: 0.0087794 | Time: 361ms
==> Save the model at epoch 016 with test loss 0.0087794
Epoch: 017 | Loss: 0.0089240 | L2 loss: 0.0087038 | Lip loss: 0.0002202 | Grad norm: 0.363900 | Time: 7s21ms
Epoch: 017 | Test Loss: 0.0087723 | Time: 374ms
==> Save the model at epoch 017 with test loss 0.0087723
Epoch: 018 | Loss: 0.0089405 | L2 loss: 0.0087204 | Lip loss: 0.0002201 | Grad norm: 0.372122 | Time: 7s6ms
Epoch: 018 | Test Loss: 0.0087669 | Time: 375ms
==> Save the model at epoch 018 with test loss 0.0087669
Epoch: 019 | Loss: 0.0089244 | L2 loss: 0.0087060 | Lip loss: 0.0002184 | Grad norm: 0.381664 | Time: 7s87ms
Epoch: 019 | Test Loss: 0.0087593 | Time: 375ms
==> Save the model at epoch 019 with test loss 0.0087593
Epoch: 020 | Loss: 0.0089470 | L2 loss: 0.0087285 | Lip loss: 0.0002185 | Grad norm: 0.363951 | Time: 6s942ms
Epoch: 020 | Test Loss: 0.0087534 | Time: 363ms
==> Save the model at epoch 020 with test loss 0.0087534
Epoch: 021 | Loss: 0.0089044 | L2 loss: 0.0086873 | Lip loss: 0.0002171 | Grad norm: 0.339847 | Time: 6s835ms
Epoch: 021 | Test Loss: 0.0087530 | Time: 363ms
==> Save the model at epoch 021 with test loss 0.0087530
Epoch: 022 | Loss: 0.0089132 | L2 loss: 0.0086953 | Lip loss: 0.0002180 | Grad norm: 0.359017 | Time: 6s911ms
Epoch: 022 | Test Loss: 0.0087524 | Time: 369ms
==> Save the model at epoch 022 with test loss 0.0087524
Epoch: 023 | Loss: 0.0089265 | L2 loss: 0.0087079 | Lip loss: 0.0002186 | Grad norm: 0.380254 | Time: 6s853ms
Epoch: 023 | Test Loss: 0.0087520 | Time: 443ms
==> Save the model at epoch 023 with test loss 0.0087520
Epoch: 024 | Loss: 0.0088876 | L2 loss: 0.0086706 | Lip loss: 0.0002170 | Grad norm: 0.384107 | Time: 7s72ms
Epoch: 024 | Test Loss: 0.0087514 | Time: 374ms
==> Save the model at epoch 024 with test loss 0.0087514
Epoch: 025 | Loss: 0.0088957 | L2 loss: 0.0086765 | Lip loss: 0.0002192 | Grad norm: 0.406432 | Time: 7s103ms
Epoch: 025 | Test Loss: 0.0087508 | Time: 375ms
==> Save the model at epoch 025 with test loss 0.0087508
Epoch: 026 | Loss: 0.0088942 | L2 loss: 0.0086746 | Lip loss: 0.0002196 | Grad norm: 0.384655 | Time: 7s138ms
Epoch: 026 | Test Loss: 0.0087508 | Time: 373ms
==> Save the model at epoch 026 with test loss 0.0087508
Epoch: 027 | Loss: 0.0089114 | L2 loss: 0.0086927 | Lip loss: 0.0002187 | Grad norm: 0.362561 | Time: 7s50ms
Epoch: 027 | Test Loss: 0.0087508 | Time: 368ms
==> Save the model at epoch 027 with test loss 0.0087508
Epoch: 028 | Loss: 0.0088788 | L2 loss: 0.0086613 | Lip loss: 0.0002175 | Grad norm: 0.368124 | Time: 7s1ms
Epoch: 028 | Test Loss: 0.0087508 | Time: 371ms
==> Save the model at epoch 028 with test loss 0.0087508
Epoch: 029 | Loss: 0.0088879 | L2 loss: 0.0086688 | Lip loss: 0.0002191 | Grad norm: 0.367830 | Time: 6s917ms
Epoch: 029 | Test Loss: 0.0087508 | Time: 366ms
==> Save the model at epoch 029 with test loss 0.0087508
Epoch: 030 | Loss: 0.0088986 | L2 loss: 0.0086807 | Lip loss: 0.0002179 | Grad norm: 0.382228 | Time: 6s882ms
Epoch: 030 | Test Loss: 0.0087508 | Time: 433ms
==> Save the model at epoch 030 with test loss 0.0087508
Epoch: 031 | Loss: 0.0089509 | L2 loss: 0.0087326 | Lip loss: 0.0002183 | Grad norm: 0.364603 | Time: 6s888ms
Epoch: 031 | Test Loss: 0.0087508 | Time: 373ms
Epoch: 032 | Loss: 0.0089248 | L2 loss: 0.0087079 | Lip loss: 0.0002170 | Grad norm: 0.384119 | Time: 6s886ms
Epoch: 032 | Test Loss: 0.0087508 | Time: 375ms
Epoch: 033 | Loss: 0.0089443 | L2 loss: 0.0087241 | Lip loss: 0.0002202 | Grad norm: 0.380984 | Time: 6s676ms
Epoch: 033 | Test Loss: 0.0087508 | Time: 362ms
==> Save the model at epoch 033 with test loss 0.0087508
Epoch: 034 | Loss: 0.0088976 | L2 loss: 0.0086790 | Lip loss: 0.0002186 | Grad norm: 0.372427 | Time: 6s927ms
Epoch: 034 | Test Loss: 0.0087508 | Time: 365ms
==> Save the model at epoch 034 with test loss 0.0087508
Epoch: 035 | Loss: 0.0089100 | L2 loss: 0.0086927 | Lip loss: 0.0002173 | Grad norm: 0.358301 | Time: 7s242ms
Epoch: 035 | Test Loss: 0.0087508 | Time: 366ms
==> Save the model at epoch 035 with test loss 0.0087508
Epoch: 036 | Loss: 0.0089977 | L2 loss: 0.0087782 | Lip loss: 0.0002195 | Grad norm: 0.395130 | Time: 6s954ms
Epoch: 036 | Test Loss: 0.0087508 | Time: 371ms
==> Save the model at epoch 036 with test loss 0.0087508
Epoch: 037 | Loss: 0.0089084 | L2 loss: 0.0086894 | Lip loss: 0.0002189 | Grad norm: 0.395614 | Time: 6s981ms
Epoch: 037 | Test Loss: 0.0087508 | Time: 366ms
Epoch: 038 | Loss: 0.0089208 | L2 loss: 0.0086998 | Lip loss: 0.0002211 | Grad norm: 0.362737 | Time: 6s809ms
Epoch: 038 | Test Loss: 0.0087508 | Time: 362ms
Epoch: 039 | Loss: 0.0089451 | L2 loss: 0.0087276 | Lip loss: 0.0002175 | Grad norm: 0.402259 | Time: 7s1ms
Epoch: 039 | Test Loss: 0.0087508 | Time: 371ms
Epoch: 040 | Loss: 0.0089087 | L2 loss: 0.0086890 | Lip loss: 0.0002197 | Grad norm: 0.398188 | Time: 7s127ms
Epoch: 040 | Test Loss: 0.0087508 | Time: 372ms
Total time: 4m55s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
