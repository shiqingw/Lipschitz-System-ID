==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (M): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 5.1249204 | L2 loss: 5.1055632 | Lip loss: 0.0193573 | Grad norm: 14.433989 | Time: 25s308ms
Epoch: 001 | Test Loss: 0.0249752 | Time: 546ms
==> Save the model at epoch 001 with test loss 0.0249752
Epoch: 002 | Loss: 0.0526958 | L2 loss: 0.0320410 | Lip loss: 0.0206548 | Grad norm: 7.658188 | Time: 24s917ms
Epoch: 002 | Test Loss: 0.3167954 | Time: 555ms
Epoch: 003 | Loss: 0.0555140 | L2 loss: 0.0349440 | Lip loss: 0.0205700 | Grad norm: 5.956758 | Time: 22s885ms
Epoch: 003 | Test Loss: 0.0336091 | Time: 538ms
Epoch: 004 | Loss: 0.0466994 | L2 loss: 0.0261625 | Lip loss: 0.0205369 | Grad norm: 4.440024 | Time: 23s214ms
Epoch: 004 | Test Loss: 0.0149577 | Time: 532ms
==> Save the model at epoch 004 with test loss 0.0149577
Epoch: 005 | Loss: 0.0371642 | L2 loss: 0.0167087 | Lip loss: 0.0204556 | Grad norm: 3.400119 | Time: 22s637ms
Epoch: 005 | Test Loss: 0.0152323 | Time: 525ms
Epoch: 006 | Loss: 0.0284640 | L2 loss: 0.0081273 | Lip loss: 0.0203367 | Grad norm: 0.686951 | Time: 22s577ms
Epoch: 006 | Test Loss: 0.0078800 | Time: 525ms
==> Save the model at epoch 006 with test loss 0.0078800
Epoch: 007 | Loss: 0.0281138 | L2 loss: 0.0078003 | Lip loss: 0.0203135 | Grad norm: 0.568860 | Time: 22s643ms
Epoch: 007 | Test Loss: 0.0076573 | Time: 524ms
==> Save the model at epoch 007 with test loss 0.0076573
Epoch: 008 | Loss: 0.0280898 | L2 loss: 0.0077769 | Lip loss: 0.0203130 | Grad norm: 0.611286 | Time: 22s563ms
Epoch: 008 | Test Loss: 0.0078447 | Time: 584ms
Epoch: 009 | Loss: 0.0279761 | L2 loss: 0.0076872 | Lip loss: 0.0202889 | Grad norm: 0.563373 | Time: 22s764ms
Epoch: 009 | Test Loss: 0.0077487 | Time: 537ms
Epoch: 010 | Loss: 0.0279333 | L2 loss: 0.0076531 | Lip loss: 0.0202801 | Grad norm: 0.533784 | Time: 24s648ms
Epoch: 010 | Test Loss: 0.0076272 | Time: 556ms
==> Save the model at epoch 010 with test loss 0.0076272
Epoch: 011 | Loss: 0.0276304 | L2 loss: 0.0073699 | Lip loss: 0.0202605 | Grad norm: 0.323284 | Time: 24s601ms
Epoch: 011 | Test Loss: 0.0073512 | Time: 542ms
==> Save the model at epoch 011 with test loss 0.0073512
Epoch: 012 | Loss: 0.0276233 | L2 loss: 0.0073656 | Lip loss: 0.0202577 | Grad norm: 0.320072 | Time: 24s722ms
Epoch: 012 | Test Loss: 0.0073610 | Time: 536ms
Epoch: 013 | Loss: 0.0276208 | L2 loss: 0.0073624 | Lip loss: 0.0202584 | Grad norm: 0.333143 | Time: 24s530ms
Epoch: 013 | Test Loss: 0.0073854 | Time: 531ms
Epoch: 014 | Loss: 0.0276136 | L2 loss: 0.0073561 | Lip loss: 0.0202575 | Grad norm: 0.329040 | Time: 24s761ms
Epoch: 014 | Test Loss: 0.0073399 | Time: 547ms
==> Save the model at epoch 014 with test loss 0.0073399
Epoch: 015 | Loss: 0.0276286 | L2 loss: 0.0073700 | Lip loss: 0.0202586 | Grad norm: 0.350611 | Time: 24s405ms
Epoch: 015 | Test Loss: 0.0074239 | Time: 530ms
Epoch: 016 | Loss: 0.0275930 | L2 loss: 0.0073310 | Lip loss: 0.0202620 | Grad norm: 0.299292 | Time: 23s712ms
Epoch: 016 | Test Loss: 0.0073294 | Time: 571ms
==> Save the model at epoch 016 with test loss 0.0073294
Epoch: 017 | Loss: 0.0275888 | L2 loss: 0.0073164 | Lip loss: 0.0202724 | Grad norm: 0.284290 | Time: 22s638ms
Epoch: 017 | Test Loss: 0.0073282 | Time: 526ms
==> Save the model at epoch 017 with test loss 0.0073282
Epoch: 018 | Loss: 0.0276032 | L2 loss: 0.0073206 | Lip loss: 0.0202826 | Grad norm: 0.292779 | Time: 22s653ms
Epoch: 018 | Test Loss: 0.0073268 | Time: 526ms
==> Save the model at epoch 018 with test loss 0.0073268
Epoch: 019 | Loss: 0.0276079 | L2 loss: 0.0073182 | Lip loss: 0.0202897 | Grad norm: 0.289581 | Time: 23s82ms
Epoch: 019 | Test Loss: 0.0073322 | Time: 584ms
Epoch: 020 | Loss: 0.0275893 | L2 loss: 0.0073231 | Lip loss: 0.0202662 | Grad norm: 0.288873 | Time: 23s209ms
Epoch: 020 | Test Loss: 0.0073360 | Time: 537ms
Epoch: 021 | Loss: 0.0275730 | L2 loss: 0.0073134 | Lip loss: 0.0202596 | Grad norm: 0.291312 | Time: 22s637ms
Epoch: 021 | Test Loss: 0.0073304 | Time: 528ms
Epoch: 022 | Loss: 0.0275868 | L2 loss: 0.0073415 | Lip loss: 0.0202453 | Grad norm: 0.295712 | Time: 22s588ms
Epoch: 022 | Test Loss: 0.0073289 | Time: 523ms
Epoch: 023 | Loss: 0.0275627 | L2 loss: 0.0073171 | Lip loss: 0.0202456 | Grad norm: 0.286580 | Time: 22s620ms
Epoch: 023 | Test Loss: 0.0073284 | Time: 525ms
Epoch: 024 | Loss: 0.0275752 | L2 loss: 0.0073162 | Lip loss: 0.0202591 | Grad norm: 0.281994 | Time: 22s537ms
Epoch: 024 | Test Loss: 0.0073289 | Time: 530ms
Epoch: 025 | Loss: 0.0275804 | L2 loss: 0.0073149 | Lip loss: 0.0202655 | Grad norm: 0.284162 | Time: 22s590ms
Epoch: 025 | Test Loss: 0.0073285 | Time: 527ms
Epoch: 026 | Loss: 0.0275769 | L2 loss: 0.0073199 | Lip loss: 0.0202570 | Grad norm: 0.289151 | Time: 22s590ms
Epoch: 026 | Test Loss: 0.0073285 | Time: 525ms
Epoch: 027 | Loss: 0.0275701 | L2 loss: 0.0073115 | Lip loss: 0.0202585 | Grad norm: 0.284172 | Time: 22s539ms
Epoch: 027 | Test Loss: 0.0073285 | Time: 527ms
Epoch: 028 | Loss: 0.0275605 | L2 loss: 0.0073096 | Lip loss: 0.0202509 | Grad norm: 0.287181 | Time: 22s602ms
Epoch: 028 | Test Loss: 0.0073285 | Time: 531ms
Epoch: 029 | Loss: 0.0275714 | L2 loss: 0.0073113 | Lip loss: 0.0202602 | Grad norm: 0.286248 | Time: 22s550ms
Epoch: 029 | Test Loss: 0.0073284 | Time: 586ms
Epoch: 030 | Loss: 0.0275695 | L2 loss: 0.0073119 | Lip loss: 0.0202577 | Grad norm: 0.287905 | Time: 23s6ms
Epoch: 030 | Test Loss: 0.0073284 | Time: 537ms
Epoch: 031 | Loss: 0.0275634 | L2 loss: 0.0073155 | Lip loss: 0.0202480 | Grad norm: 0.295287 | Time: 22s687ms
Epoch: 031 | Test Loss: 0.0073284 | Time: 527ms
Epoch: 032 | Loss: 0.0275733 | L2 loss: 0.0073096 | Lip loss: 0.0202637 | Grad norm: 0.286611 | Time: 22s619ms
Epoch: 032 | Test Loss: 0.0073284 | Time: 527ms
Epoch: 033 | Loss: 0.0275658 | L2 loss: 0.0073131 | Lip loss: 0.0202527 | Grad norm: 0.285138 | Time: 23s910ms
Epoch: 033 | Test Loss: 0.0073284 | Time: 537ms
Epoch: 034 | Loss: 0.0275755 | L2 loss: 0.0073107 | Lip loss: 0.0202648 | Grad norm: 0.281160 | Time: 22s680ms
Epoch: 034 | Test Loss: 0.0073284 | Time: 591ms
Epoch: 035 | Loss: 0.0275640 | L2 loss: 0.0073131 | Lip loss: 0.0202508 | Grad norm: 0.277626 | Time: 23s285ms
Epoch: 035 | Test Loss: 0.0073284 | Time: 543ms
Epoch: 036 | Loss: 0.0275691 | L2 loss: 0.0073096 | Lip loss: 0.0202595 | Grad norm: 0.291062 | Time: 24s859ms
Epoch: 036 | Test Loss: 0.0073284 | Time: 544ms
Epoch: 037 | Loss: 0.0275872 | L2 loss: 0.0073140 | Lip loss: 0.0202732 | Grad norm: 0.281960 | Time: 24s634ms
Epoch: 037 | Test Loss: 0.0073284 | Time: 542ms
Epoch: 038 | Loss: 0.0275526 | L2 loss: 0.0073106 | Lip loss: 0.0202420 | Grad norm: 0.297071 | Time: 24s976ms
Epoch: 038 | Test Loss: 0.0073284 | Time: 541ms
Epoch: 039 | Loss: 0.0275614 | L2 loss: 0.0073121 | Lip loss: 0.0202492 | Grad norm: 0.293793 | Time: 24s612ms
Epoch: 039 | Test Loss: 0.0073284 | Time: 591ms
Epoch: 040 | Loss: 0.0275812 | L2 loss: 0.0073102 | Lip loss: 0.0202709 | Grad norm: 0.294975 | Time: 24s738ms
Epoch: 040 | Test Loss: 0.0073284 | Time: 537ms
Total time: 15m59s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
