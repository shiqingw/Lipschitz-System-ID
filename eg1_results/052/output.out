==> torch device:  cuda:3
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (M): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.50
==> Start training...
Epoch: 001 | Loss: 5.3431525 | L2 loss: 5.1542430 | Lip loss: 0.1889095 | Grad norm: 24.309989 | Time: 24s863ms
Epoch: 001 | Test Loss: 0.3908206 | Time: 526ms
==> Save the model at epoch 001 with test loss 0.3908206
Epoch: 002 | Loss: 0.2587441 | L2 loss: 0.0518969 | Lip loss: 0.2068472 | Grad norm: 10.092392 | Time: 23s375ms
Epoch: 002 | Test Loss: 0.0224034 | Time: 526ms
==> Save the model at epoch 002 with test loss 0.0224034
Epoch: 003 | Loss: 0.2328988 | L2 loss: 0.0270319 | Lip loss: 0.2058669 | Grad norm: 6.006260 | Time: 23s492ms
Epoch: 003 | Test Loss: 0.0285103 | Time: 541ms
Epoch: 004 | Loss: 0.2362178 | L2 loss: 0.0299267 | Lip loss: 0.2062911 | Grad norm: 5.210194 | Time: 25s419ms
Epoch: 004 | Test Loss: 0.0248086 | Time: 531ms
Epoch: 005 | Loss: 0.2387468 | L2 loss: 0.0327086 | Lip loss: 0.2060382 | Grad norm: 5.150256 | Time: 25s450ms
Epoch: 005 | Test Loss: 0.0415243 | Time: 530ms
Epoch: 006 | Loss: 0.2142470 | L2 loss: 0.0105489 | Lip loss: 0.2036981 | Grad norm: 1.486082 | Time: 25s390ms
Epoch: 006 | Test Loss: 0.0087587 | Time: 531ms
==> Save the model at epoch 006 with test loss 0.0087587
Epoch: 007 | Loss: 0.2123305 | L2 loss: 0.0092352 | Lip loss: 0.2030952 | Grad norm: 1.213249 | Time: 25s278ms
Epoch: 007 | Test Loss: 0.0092467 | Time: 529ms
Epoch: 008 | Loss: 0.2114342 | L2 loss: 0.0088832 | Lip loss: 0.2025510 | Grad norm: 1.111274 | Time: 25s630ms
Epoch: 008 | Test Loss: 0.0091453 | Time: 536ms
Epoch: 009 | Loss: 0.2114922 | L2 loss: 0.0089042 | Lip loss: 0.2025881 | Grad norm: 1.196717 | Time: 25s530ms
Epoch: 009 | Test Loss: 0.0089041 | Time: 525ms
Epoch: 010 | Loss: 0.2112343 | L2 loss: 0.0087490 | Lip loss: 0.2024853 | Grad norm: 1.133220 | Time: 23s378ms
Epoch: 010 | Test Loss: 0.0090687 | Time: 524ms
Epoch: 011 | Loss: 0.2098496 | L2 loss: 0.0079923 | Lip loss: 0.2018572 | Grad norm: 0.890122 | Time: 23s331ms
Epoch: 011 | Test Loss: 0.0080283 | Time: 531ms
==> Save the model at epoch 011 with test loss 0.0080283
Epoch: 012 | Loss: 0.2097034 | L2 loss: 0.0078919 | Lip loss: 0.2018115 | Grad norm: 0.859450 | Time: 23s378ms
Epoch: 012 | Test Loss: 0.0079011 | Time: 526ms
==> Save the model at epoch 012 with test loss 0.0079011
Epoch: 013 | Loss: 0.2096525 | L2 loss: 0.0079169 | Lip loss: 0.2017356 | Grad norm: 0.853625 | Time: 23s344ms
Epoch: 013 | Test Loss: 0.0079669 | Time: 584ms
Epoch: 014 | Loss: 0.2098225 | L2 loss: 0.0078850 | Lip loss: 0.2019375 | Grad norm: 0.876603 | Time: 23s348ms
Epoch: 014 | Test Loss: 0.0078386 | Time: 529ms
==> Save the model at epoch 014 with test loss 0.0078386
Epoch: 015 | Loss: 0.2094324 | L2 loss: 0.0078775 | Lip loss: 0.2015549 | Grad norm: 0.857236 | Time: 23s445ms
Epoch: 015 | Test Loss: 0.0080010 | Time: 528ms
Epoch: 016 | Loss: 0.2094668 | L2 loss: 0.0078232 | Lip loss: 0.2016436 | Grad norm: 0.831573 | Time: 24s9ms
Epoch: 016 | Test Loss: 0.0078280 | Time: 534ms
==> Save the model at epoch 016 with test loss 0.0078280
Epoch: 017 | Loss: 0.2095210 | L2 loss: 0.0078045 | Lip loss: 0.2017165 | Grad norm: 0.823891 | Time: 23s528ms
Epoch: 017 | Test Loss: 0.0078296 | Time: 525ms
Epoch: 018 | Loss: 0.2093603 | L2 loss: 0.0077950 | Lip loss: 0.2015653 | Grad norm: 0.800771 | Time: 23s525ms
Epoch: 018 | Test Loss: 0.0078411 | Time: 525ms
Epoch: 019 | Loss: 0.2095569 | L2 loss: 0.0077978 | Lip loss: 0.2017592 | Grad norm: 0.830609 | Time: 23s467ms
Epoch: 019 | Test Loss: 0.0078413 | Time: 528ms
Epoch: 020 | Loss: 0.2093606 | L2 loss: 0.0077988 | Lip loss: 0.2015619 | Grad norm: 0.809128 | Time: 23s521ms
Epoch: 020 | Test Loss: 0.0078087 | Time: 525ms
==> Save the model at epoch 020 with test loss 0.0078087
Epoch: 021 | Loss: 0.2095159 | L2 loss: 0.0077716 | Lip loss: 0.2017443 | Grad norm: 0.832451 | Time: 23s455ms
Epoch: 021 | Test Loss: 0.0078163 | Time: 584ms
Epoch: 022 | Loss: 0.2094635 | L2 loss: 0.0077822 | Lip loss: 0.2016813 | Grad norm: 0.796700 | Time: 23s452ms
Epoch: 022 | Test Loss: 0.0078181 | Time: 528ms
Epoch: 023 | Loss: 0.2095723 | L2 loss: 0.0077856 | Lip loss: 0.2017866 | Grad norm: 0.821651 | Time: 23s535ms
Epoch: 023 | Test Loss: 0.0078225 | Time: 527ms
Epoch: 024 | Loss: 0.2094068 | L2 loss: 0.0077800 | Lip loss: 0.2016268 | Grad norm: 0.817684 | Time: 23s484ms
Epoch: 024 | Test Loss: 0.0078186 | Time: 526ms
Epoch: 025 | Loss: 0.2093728 | L2 loss: 0.0077838 | Lip loss: 0.2015890 | Grad norm: 0.801820 | Time: 23s568ms
Epoch: 025 | Test Loss: 0.0078183 | Time: 529ms
Epoch: 026 | Loss: 0.2093906 | L2 loss: 0.0077982 | Lip loss: 0.2015923 | Grad norm: 0.812486 | Time: 23s569ms
Epoch: 026 | Test Loss: 0.0078177 | Time: 530ms
Epoch: 027 | Loss: 0.2092912 | L2 loss: 0.0077787 | Lip loss: 0.2015125 | Grad norm: 0.786166 | Time: 23s520ms
Epoch: 027 | Test Loss: 0.0078171 | Time: 530ms
Epoch: 028 | Loss: 0.2093760 | L2 loss: 0.0077853 | Lip loss: 0.2015907 | Grad norm: 0.790714 | Time: 23s545ms
Epoch: 028 | Test Loss: 0.0078172 | Time: 532ms
Epoch: 029 | Loss: 0.2093528 | L2 loss: 0.0077810 | Lip loss: 0.2015718 | Grad norm: 0.844091 | Time: 23s493ms
Epoch: 029 | Test Loss: 0.0078171 | Time: 584ms
Epoch: 030 | Loss: 0.2093857 | L2 loss: 0.0077761 | Lip loss: 0.2016096 | Grad norm: 0.804479 | Time: 23s478ms
Epoch: 030 | Test Loss: 0.0078170 | Time: 526ms
Epoch: 031 | Loss: 0.2094983 | L2 loss: 0.0077932 | Lip loss: 0.2017051 | Grad norm: 0.828189 | Time: 25s341ms
Epoch: 031 | Test Loss: 0.0078170 | Time: 532ms
Epoch: 032 | Loss: 0.2094077 | L2 loss: 0.0077812 | Lip loss: 0.2016264 | Grad norm: 0.817341 | Time: 25s265ms
Epoch: 032 | Test Loss: 0.0078170 | Time: 531ms
Epoch: 033 | Loss: 0.2094359 | L2 loss: 0.0077768 | Lip loss: 0.2016591 | Grad norm: 0.839245 | Time: 25s298ms
Epoch: 033 | Test Loss: 0.0078170 | Time: 533ms
Epoch: 034 | Loss: 0.2093426 | L2 loss: 0.0077844 | Lip loss: 0.2015582 | Grad norm: 0.797285 | Time: 25s258ms
Epoch: 034 | Test Loss: 0.0078170 | Time: 596ms
Epoch: 035 | Loss: 0.2095776 | L2 loss: 0.0077798 | Lip loss: 0.2017978 | Grad norm: 0.803053 | Time: 24s795ms
Epoch: 035 | Test Loss: 0.0078170 | Time: 532ms
Epoch: 036 | Loss: 0.2092875 | L2 loss: 0.0077886 | Lip loss: 0.2014989 | Grad norm: 0.788411 | Time: 26s485ms
Epoch: 036 | Test Loss: 0.0078170 | Time: 535ms
Epoch: 037 | Loss: 0.2095127 | L2 loss: 0.0077795 | Lip loss: 0.2017332 | Grad norm: 0.880987 | Time: 25s112ms
Epoch: 037 | Test Loss: 0.0078170 | Time: 528ms
Epoch: 038 | Loss: 0.2093759 | L2 loss: 0.0077782 | Lip loss: 0.2015977 | Grad norm: 0.807978 | Time: 23s360ms
Epoch: 038 | Test Loss: 0.0078170 | Time: 531ms
Epoch: 039 | Loss: 0.2095314 | L2 loss: 0.0077782 | Lip loss: 0.2017532 | Grad norm: 0.839380 | Time: 23s318ms
Epoch: 039 | Test Loss: 0.0078170 | Time: 594ms
Epoch: 040 | Loss: 0.2094453 | L2 loss: 0.0077796 | Lip loss: 0.2016657 | Grad norm: 0.847208 | Time: 23s769ms
Epoch: 040 | Test Loss: 0.0078170 | Time: 522ms
Total time: 16m27s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
