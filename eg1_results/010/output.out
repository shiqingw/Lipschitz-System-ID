==> torch device:  cuda:1
==> Lipschitz constant: 2.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 128]                  16,897
│    └─SandwichFc: 2-2                   [1, 128]                  33,025
│    └─SandwichFc: 2-3                   [1, 128]                  33,025
│    └─SandwichFc: 2-4                   [1, 128]                  33,025
│    └─SandwichFc: 2-5                   [1, 128]                  33,025
│    └─SandwichFc: 2-6                   [1, 128]                  33,025
│    └─SandwichFc: 2-7                   [1, 128]                  33,025
│    └─SandwichLin: 2-8                  [1, 2]                    263
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 128]                  (recursive)
│    └─SandwichFc: 2-10                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-11                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-12                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-13                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-14                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-15                  [1, 128]                  (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 215,316
Trainable params: 215,310
Non-trainable params: 6
Total mult-adds (M): 0.43
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 1.00
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.9603948 | Grad norm: 15.331146 | Time: 23s195ms
Epoch: 001 | Test Loss: 30.9983877 | Time: 827ms
==> Save the model at epoch 001 with test loss 30.9983877
Epoch: 002 | Train Loss: 1.6585342 | Grad norm: 7.165488 | Time: 21s153ms
Epoch: 002 | Test Loss: 0.0148616 | Time: 818ms
==> Save the model at epoch 002 with test loss 0.0148616
Epoch: 003 | Train Loss: 0.0286227 | Grad norm: 7.270381 | Time: 21s848ms
Epoch: 003 | Test Loss: 0.0201327 | Time: 803ms
Epoch: 004 | Train Loss: 0.0290654 | Grad norm: 8.551318 | Time: 23s109ms
Epoch: 004 | Test Loss: 0.0308121 | Time: 807ms
Epoch: 005 | Train Loss: 0.0653260 | Grad norm: 11.836068 | Time: 20s719ms
Epoch: 005 | Test Loss: 0.1479419 | Time: 817ms
Epoch: 006 | Train Loss: 0.1166805 | Grad norm: 18.157685 | Time: 21s72ms
Epoch: 006 | Test Loss: 0.1881987 | Time: 778ms
Epoch: 007 | Train Loss: 0.1164457 | Grad norm: 17.021537 | Time: 20s387ms
Epoch: 007 | Test Loss: 0.1779903 | Time: 805ms
Epoch: 008 | Train Loss: 0.1085345 | Grad norm: 14.855967 | Time: 22s93ms
Epoch: 008 | Test Loss: 0.0348657 | Time: 811ms
Epoch: 009 | Train Loss: 0.0409035 | Grad norm: 8.540220 | Time: 22s644ms
Epoch: 009 | Test Loss: 0.0393087 | Time: 809ms
Epoch: 010 | Train Loss: 0.0399682 | Grad norm: 8.195731 | Time: 20s672ms
Epoch: 010 | Test Loss: 0.0396959 | Time: 813ms
Epoch: 011 | Train Loss: 0.0399214 | Grad norm: 7.828946 | Time: 20s658ms
Epoch: 011 | Test Loss: 0.0306130 | Time: 776ms
Epoch: 012 | Train Loss: 0.0338068 | Grad norm: 6.776272 | Time: 22s459ms
Epoch: 012 | Test Loss: 0.0296947 | Time: 774ms
Epoch: 013 | Train Loss: 0.0289941 | Grad norm: 5.985312 | Time: 18s789ms
Epoch: 013 | Test Loss: 0.0245473 | Time: 779ms
Epoch: 014 | Train Loss: 0.0252898 | Grad norm: 5.381582 | Time: 20s575ms
Epoch: 014 | Test Loss: 0.0215251 | Time: 779ms
Epoch: 015 | Train Loss: 0.0231696 | Grad norm: 4.902153 | Time: 20s988ms
Epoch: 015 | Test Loss: 0.0213993 | Time: 771ms
Epoch: 016 | Train Loss: 0.0207686 | Grad norm: 4.455594 | Time: 21s484ms
Epoch: 016 | Test Loss: 0.0164565 | Time: 775ms
Epoch: 017 | Train Loss: 0.0187493 | Grad norm: 4.092295 | Time: 20s221ms
Epoch: 017 | Test Loss: 0.0170000 | Time: 801ms
Epoch: 018 | Train Loss: 0.0177966 | Grad norm: 3.795690 | Time: 18s405ms
Epoch: 018 | Test Loss: 0.0165314 | Time: 794ms
Epoch: 019 | Train Loss: 0.0164581 | Grad norm: 3.519596 | Time: 20s49ms
Epoch: 019 | Test Loss: 0.0168577 | Time: 790ms
Epoch: 020 | Train Loss: 0.0150574 | Grad norm: 3.236906 | Time: 19s220ms
Epoch: 020 | Test Loss: 0.0134868 | Time: 777ms
==> Save the model at epoch 020 with test loss 0.0134868
Epoch: 021 | Train Loss: 0.0140262 | Grad norm: 3.018655 | Time: 18s28ms
Epoch: 021 | Test Loss: 0.0138280 | Time: 807ms
Epoch: 022 | Train Loss: 0.0131063 | Grad norm: 2.825756 | Time: 17s343ms
Epoch: 022 | Test Loss: 0.0138338 | Time: 775ms
Epoch: 023 | Train Loss: 0.0206882 | Grad norm: 3.178683 | Time: 18s45ms
Epoch: 023 | Test Loss: 0.0122390 | Time: 821ms
==> Save the model at epoch 023 with test loss 0.0122390
Epoch: 024 | Train Loss: 0.0121023 | Grad norm: 2.538423 | Time: 17s458ms
Epoch: 024 | Test Loss: 0.0104349 | Time: 778ms
==> Save the model at epoch 024 with test loss 0.0104349
Epoch: 025 | Train Loss: 0.0110757 | Grad norm: 2.280664 | Time: 17s365ms
Epoch: 025 | Test Loss: 0.0125235 | Time: 779ms
Epoch: 026 | Train Loss: 0.0109854 | Grad norm: 2.083520 | Time: 17s549ms
Epoch: 026 | Test Loss: 0.0121234 | Time: 806ms
Epoch: 027 | Train Loss: 0.0119759 | Grad norm: 2.387937 | Time: 17s690ms
Epoch: 027 | Test Loss: 0.0128693 | Time: 776ms
Epoch: 028 | Train Loss: 0.0149093 | Grad norm: 3.133434 | Time: 17s287ms
Epoch: 028 | Test Loss: 0.0201541 | Time: 777ms
Epoch: 029 | Train Loss: 0.0097776 | Grad norm: 1.686018 | Time: 18s128ms
Epoch: 029 | Test Loss: 0.0134160 | Time: 775ms
Epoch: 030 | Train Loss: 0.0131478 | Grad norm: 2.727263 | Time: 17s865ms
Epoch: 030 | Test Loss: 0.0134707 | Time: 811ms
Epoch: 031 | Train Loss: 0.0126246 | Grad norm: 2.638290 | Time: 17s598ms
Epoch: 031 | Test Loss: 0.0128534 | Time: 869ms
Epoch: 032 | Train Loss: 0.0093485 | Grad norm: 1.332402 | Time: 17s750ms
Epoch: 032 | Test Loss: 0.0100405 | Time: 794ms
==> Save the model at epoch 032 with test loss 0.0100405
Epoch: 033 | Train Loss: 0.0086109 | Grad norm: 1.211884 | Time: 17s820ms
Epoch: 033 | Test Loss: 0.0082950 | Time: 775ms
==> Save the model at epoch 033 with test loss 0.0082950
Epoch: 034 | Train Loss: 0.0093666 | Grad norm: 1.342416 | Time: 17s734ms
Epoch: 034 | Test Loss: 0.0112478 | Time: 838ms
Epoch: 035 | Train Loss: 0.0089798 | Grad norm: 1.365803 | Time: 17s374ms
Epoch: 035 | Test Loss: 0.0072792 | Time: 774ms
==> Save the model at epoch 035 with test loss 0.0072792
Epoch: 036 | Train Loss: 0.0082657 | Grad norm: 1.017309 | Time: 18s339ms
Epoch: 036 | Test Loss: 0.0078604 | Time: 813ms
Epoch: 037 | Train Loss: 0.0077993 | Grad norm: 0.791151 | Time: 18s417ms
Epoch: 037 | Test Loss: 0.0091299 | Time: 869ms
Epoch: 038 | Train Loss: 0.0085952 | Grad norm: 1.162128 | Time: 18s77ms
Epoch: 038 | Test Loss: 0.0074334 | Time: 800ms
Epoch: 039 | Train Loss: 0.0076760 | Grad norm: 0.728677 | Time: 17s837ms
Epoch: 039 | Test Loss: 0.0081325 | Time: 813ms
Epoch: 040 | Train Loss: 0.0079244 | Grad norm: 0.849817 | Time: 21s760ms
Epoch: 040 | Test Loss: 0.0115300 | Time: 826ms
Epoch: 041 | Train Loss: 0.0076747 | Grad norm: 0.697220 | Time: 18s800ms
Epoch: 041 | Test Loss: 0.0079066 | Time: 809ms
Epoch: 042 | Train Loss: 0.0075446 | Grad norm: 0.589714 | Time: 17s812ms
Epoch: 042 | Test Loss: 0.0075070 | Time: 777ms
Epoch: 043 | Train Loss: 0.0074757 | Grad norm: 0.566543 | Time: 18s701ms
Epoch: 043 | Test Loss: 0.0073429 | Time: 847ms
Epoch: 044 | Train Loss: 0.0074845 | Grad norm: 0.594915 | Time: 18s908ms
Epoch: 044 | Test Loss: 0.0073786 | Time: 764ms
Epoch: 045 | Train Loss: 0.0074142 | Grad norm: 0.520396 | Time: 20s399ms
Epoch: 045 | Test Loss: 0.0074465 | Time: 804ms
Epoch: 046 | Train Loss: 0.0073769 | Grad norm: 0.497187 | Time: 17s982ms
Epoch: 046 | Test Loss: 0.0076043 | Time: 875ms
Epoch: 047 | Train Loss: 0.0073223 | Grad norm: 0.438953 | Time: 19s148ms
Epoch: 047 | Test Loss: 0.0075096 | Time: 803ms
Epoch: 048 | Train Loss: 0.0073864 | Grad norm: 0.513249 | Time: 19s35ms
Epoch: 048 | Test Loss: 0.0073029 | Time: 831ms
Epoch: 049 | Train Loss: 0.0073011 | Grad norm: 0.428141 | Time: 18s375ms
Epoch: 049 | Test Loss: 0.0073085 | Time: 816ms
Epoch: 050 | Train Loss: 0.0072597 | Grad norm: 0.379410 | Time: 18s561ms
Epoch: 050 | Test Loss: 0.0073363 | Time: 826ms
Epoch: 051 | Train Loss: 0.0072702 | Grad norm: 0.402993 | Time: 18s722ms
Epoch: 051 | Test Loss: 0.0075336 | Time: 822ms
Epoch: 052 | Train Loss: 0.0072170 | Grad norm: 0.339719 | Time: 20s643ms
Epoch: 052 | Test Loss: 0.0072264 | Time: 802ms
==> Save the model at epoch 052 with test loss 0.0072264
Epoch: 053 | Train Loss: 0.0071931 | Grad norm: 0.313830 | Time: 23s40ms
Epoch: 053 | Test Loss: 0.0074119 | Time: 801ms
Epoch: 054 | Train Loss: 0.0071914 | Grad norm: 0.321928 | Time: 22s396ms
Epoch: 054 | Test Loss: 0.0072459 | Time: 805ms
Epoch: 055 | Train Loss: 0.0071541 | Grad norm: 0.263406 | Time: 19s653ms
Epoch: 055 | Test Loss: 0.0072963 | Time: 778ms
Epoch: 056 | Train Loss: 0.0071452 | Grad norm: 0.261962 | Time: 19s653ms
Epoch: 056 | Test Loss: 0.0072507 | Time: 820ms
Epoch: 057 | Train Loss: 0.0071324 | Grad norm: 0.234193 | Time: 18s629ms
Epoch: 057 | Test Loss: 0.0072397 | Time: 810ms
Epoch: 058 | Train Loss: 0.0071206 | Grad norm: 0.224271 | Time: 19s605ms
Epoch: 058 | Test Loss: 0.0072190 | Time: 821ms
==> Save the model at epoch 058 with test loss 0.0072190
Epoch: 059 | Train Loss: 0.0071128 | Grad norm: 0.202973 | Time: 20s331ms
Epoch: 059 | Test Loss: 0.0072108 | Time: 807ms
==> Save the model at epoch 059 with test loss 0.0072108
Epoch: 060 | Train Loss: 0.0071070 | Grad norm: 0.193952 | Time: 20s244ms
Epoch: 060 | Test Loss: 0.0072046 | Time: 769ms
==> Save the model at epoch 060 with test loss 0.0072046
Total time: 20m18s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
