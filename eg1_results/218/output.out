==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 17.4161195 | L2 loss: 17.4160611 | Lip loss: 0.0000584 | Grad norm: 5.758156 | Time: 21s33ms
Epoch: 001 | Test Loss: 7.1702036 | Time: 622ms
==> Save the model at epoch 001 with test loss 7.1702036
Epoch: 002 | Loss: 1.5928719 | L2 loss: 1.5926259 | Lip loss: 0.0002461 | Grad norm: 5.734583 | Time: 19s861ms
Epoch: 002 | Test Loss: 0.0189065 | Time: 543ms
==> Save the model at epoch 002 with test loss 0.0189065
Epoch: 003 | Loss: 0.0111915 | L2 loss: 0.0109771 | Lip loss: 0.0002144 | Grad norm: 3.256978 | Time: 20s258ms
Epoch: 003 | Test Loss: 0.0067182 | Time: 539ms
==> Save the model at epoch 003 with test loss 0.0067182
Epoch: 004 | Loss: 0.0057224 | L2 loss: 0.0055133 | Lip loss: 0.0002091 | Grad norm: 2.293941 | Time: 20s380ms
Epoch: 004 | Test Loss: 0.0041525 | Time: 630ms
==> Save the model at epoch 004 with test loss 0.0041525
Epoch: 005 | Loss: 0.0051391 | L2 loss: 0.0049311 | Lip loss: 0.0002079 | Grad norm: 2.276274 | Time: 20s86ms
Epoch: 005 | Test Loss: 0.0039856 | Time: 537ms
==> Save the model at epoch 005 with test loss 0.0039856
Epoch: 006 | Loss: 0.0039679 | L2 loss: 0.0037608 | Lip loss: 0.0002070 | Grad norm: 0.546685 | Time: 20s228ms
Epoch: 006 | Test Loss: 0.0036883 | Time: 530ms
==> Save the model at epoch 006 with test loss 0.0036883
Epoch: 007 | Loss: 0.0039150 | L2 loss: 0.0037075 | Lip loss: 0.0002075 | Grad norm: 0.523911 | Time: 20s301ms
Epoch: 007 | Test Loss: 0.0036371 | Time: 535ms
==> Save the model at epoch 007 with test loss 0.0036371
Epoch: 008 | Loss: 0.0038745 | L2 loss: 0.0036674 | Lip loss: 0.0002072 | Grad norm: 0.545217 | Time: 19s826ms
Epoch: 008 | Test Loss: 0.0036900 | Time: 542ms
Epoch: 009 | Loss: 0.0038260 | L2 loss: 0.0036190 | Lip loss: 0.0002070 | Grad norm: 0.477473 | Time: 20s242ms
Epoch: 009 | Test Loss: 0.0035805 | Time: 535ms
==> Save the model at epoch 009 with test loss 0.0035805
Epoch: 010 | Loss: 0.0037945 | L2 loss: 0.0035884 | Lip loss: 0.0002061 | Grad norm: 0.554901 | Time: 20s172ms
Epoch: 010 | Test Loss: 0.0035494 | Time: 537ms
==> Save the model at epoch 010 with test loss 0.0035494
Epoch: 011 | Loss: 0.0037296 | L2 loss: 0.0035228 | Lip loss: 0.0002068 | Grad norm: 0.332506 | Time: 20s747ms
Epoch: 011 | Test Loss: 0.0035083 | Time: 535ms
==> Save the model at epoch 011 with test loss 0.0035083
Epoch: 012 | Loss: 0.0037233 | L2 loss: 0.0035172 | Lip loss: 0.0002061 | Grad norm: 0.318952 | Time: 20s994ms
Epoch: 012 | Test Loss: 0.0035085 | Time: 547ms
Epoch: 013 | Loss: 0.0037199 | L2 loss: 0.0035132 | Lip loss: 0.0002068 | Grad norm: 0.340880 | Time: 20s664ms
Epoch: 013 | Test Loss: 0.0035058 | Time: 548ms
==> Save the model at epoch 013 with test loss 0.0035058
Epoch: 014 | Loss: 0.0037164 | L2 loss: 0.0035103 | Lip loss: 0.0002061 | Grad norm: 0.333232 | Time: 20s831ms
Epoch: 014 | Test Loss: 0.0035001 | Time: 613ms
==> Save the model at epoch 014 with test loss 0.0035001
Epoch: 015 | Loss: 0.0037129 | L2 loss: 0.0035061 | Lip loss: 0.0002068 | Grad norm: 0.342327 | Time: 20s309ms
Epoch: 015 | Test Loss: 0.0034901 | Time: 538ms
==> Save the model at epoch 015 with test loss 0.0034901
Epoch: 016 | Loss: 0.0037035 | L2 loss: 0.0034972 | Lip loss: 0.0002063 | Grad norm: 0.296744 | Time: 20s864ms
Epoch: 016 | Test Loss: 0.0034894 | Time: 553ms
==> Save the model at epoch 016 with test loss 0.0034894
Epoch: 017 | Loss: 0.0037035 | L2 loss: 0.0034970 | Lip loss: 0.0002066 | Grad norm: 0.295775 | Time: 20s560ms
Epoch: 017 | Test Loss: 0.0034896 | Time: 606ms
Epoch: 018 | Loss: 0.0037029 | L2 loss: 0.0034965 | Lip loss: 0.0002064 | Grad norm: 0.291566 | Time: 20s675ms
Epoch: 018 | Test Loss: 0.0034873 | Time: 691ms
==> Save the model at epoch 018 with test loss 0.0034873
Epoch: 019 | Loss: 0.0037023 | L2 loss: 0.0034960 | Lip loss: 0.0002063 | Grad norm: 0.290182 | Time: 20s821ms
Epoch: 019 | Test Loss: 0.0034868 | Time: 542ms
==> Save the model at epoch 019 with test loss 0.0034868
Epoch: 020 | Loss: 0.0037021 | L2 loss: 0.0034955 | Lip loss: 0.0002066 | Grad norm: 0.298193 | Time: 20s684ms
Epoch: 020 | Test Loss: 0.0034866 | Time: 545ms
==> Save the model at epoch 020 with test loss 0.0034866
Epoch: 021 | Loss: 0.0037017 | L2 loss: 0.0034945 | Lip loss: 0.0002071 | Grad norm: 0.294175 | Time: 20s788ms
Epoch: 021 | Test Loss: 0.0034863 | Time: 534ms
==> Save the model at epoch 021 with test loss 0.0034863
Epoch: 022 | Loss: 0.0037010 | L2 loss: 0.0034945 | Lip loss: 0.0002065 | Grad norm: 0.304341 | Time: 20s795ms
Epoch: 022 | Test Loss: 0.0034863 | Time: 564ms
==> Save the model at epoch 022 with test loss 0.0034863
Epoch: 023 | Loss: 0.0037007 | L2 loss: 0.0034944 | Lip loss: 0.0002063 | Grad norm: 0.292735 | Time: 20s833ms
Epoch: 023 | Test Loss: 0.0034862 | Time: 545ms
==> Save the model at epoch 023 with test loss 0.0034862
Epoch: 024 | Loss: 0.0037005 | L2 loss: 0.0034944 | Lip loss: 0.0002062 | Grad norm: 0.297695 | Time: 20s387ms
Epoch: 024 | Test Loss: 0.0034862 | Time: 550ms
Epoch: 025 | Loss: 0.0037006 | L2 loss: 0.0034944 | Lip loss: 0.0002062 | Grad norm: 0.291956 | Time: 20s675ms
Epoch: 025 | Test Loss: 0.0034861 | Time: 530ms
==> Save the model at epoch 025 with test loss 0.0034861
Epoch: 026 | Loss: 0.0037008 | L2 loss: 0.0034942 | Lip loss: 0.0002066 | Grad norm: 0.281752 | Time: 20s385ms
Epoch: 026 | Test Loss: 0.0034861 | Time: 540ms
==> Save the model at epoch 026 with test loss 0.0034861
Epoch: 027 | Loss: 0.0037013 | L2 loss: 0.0034944 | Lip loss: 0.0002069 | Grad norm: 0.306913 | Time: 20s22ms
Epoch: 027 | Test Loss: 0.0034861 | Time: 623ms
==> Save the model at epoch 027 with test loss 0.0034861
Epoch: 028 | Loss: 0.0037010 | L2 loss: 0.0034943 | Lip loss: 0.0002067 | Grad norm: 0.301738 | Time: 20s163ms
Epoch: 028 | Test Loss: 0.0034861 | Time: 540ms
==> Save the model at epoch 028 with test loss 0.0034861
Epoch: 029 | Loss: 0.0037003 | L2 loss: 0.0034942 | Lip loss: 0.0002061 | Grad norm: 0.297957 | Time: 20s547ms
Epoch: 029 | Test Loss: 0.0034861 | Time: 537ms
==> Save the model at epoch 029 with test loss 0.0034861
Epoch: 030 | Loss: 0.0037006 | L2 loss: 0.0034942 | Lip loss: 0.0002064 | Grad norm: 0.298294 | Time: 20s792ms
Epoch: 030 | Test Loss: 0.0034861 | Time: 618ms
==> Save the model at epoch 030 with test loss 0.0034861
Epoch: 031 | Loss: 0.0037007 | L2 loss: 0.0034940 | Lip loss: 0.0002067 | Grad norm: 0.287963 | Time: 20s172ms
Epoch: 031 | Test Loss: 0.0034861 | Time: 547ms
Epoch: 032 | Loss: 0.0037009 | L2 loss: 0.0034941 | Lip loss: 0.0002068 | Grad norm: 0.294136 | Time: 20s719ms
Epoch: 032 | Test Loss: 0.0034861 | Time: 555ms
Epoch: 033 | Loss: 0.0037008 | L2 loss: 0.0034943 | Lip loss: 0.0002065 | Grad norm: 0.278393 | Time: 20s357ms
Epoch: 033 | Test Loss: 0.0034861 | Time: 560ms
Epoch: 034 | Loss: 0.0037006 | L2 loss: 0.0034942 | Lip loss: 0.0002064 | Grad norm: 0.291368 | Time: 20s613ms
Epoch: 034 | Test Loss: 0.0034861 | Time: 530ms
Epoch: 035 | Loss: 0.0037004 | L2 loss: 0.0034942 | Lip loss: 0.0002062 | Grad norm: 0.301836 | Time: 20s875ms
Epoch: 035 | Test Loss: 0.0034861 | Time: 545ms
Epoch: 036 | Loss: 0.0037001 | L2 loss: 0.0034941 | Lip loss: 0.0002060 | Grad norm: 0.285858 | Time: 21s376ms
Epoch: 036 | Test Loss: 0.0034861 | Time: 546ms
Epoch: 037 | Loss: 0.0037004 | L2 loss: 0.0034942 | Lip loss: 0.0002061 | Grad norm: 0.309951 | Time: 21s146ms
Epoch: 037 | Test Loss: 0.0034861 | Time: 531ms
Epoch: 038 | Loss: 0.0037006 | L2 loss: 0.0034942 | Lip loss: 0.0002064 | Grad norm: 0.305626 | Time: 20s701ms
Epoch: 038 | Test Loss: 0.0034861 | Time: 538ms
Epoch: 039 | Loss: 0.0037003 | L2 loss: 0.0034940 | Lip loss: 0.0002062 | Grad norm: 0.294338 | Time: 20s772ms
Epoch: 039 | Test Loss: 0.0034861 | Time: 538ms
Epoch: 040 | Loss: 0.0036999 | L2 loss: 0.0034942 | Lip loss: 0.0002057 | Grad norm: 0.277678 | Time: 20s685ms
Epoch: 040 | Test Loss: 0.0034861 | Time: 546ms
Total time: 14m4s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
