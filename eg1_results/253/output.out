==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 8.2380490 | L2 loss: 8.2368207 | Lip loss: 0.0012283 | Grad norm: 4.400655 | Time: 34s424ms
Epoch: 001 | Test Loss: 0.0129613 | Time: 448ms
==> Save the model at epoch 001 with test loss 0.0129613
Epoch: 002 | Loss: 0.0105203 | L2 loss: 0.0084121 | Lip loss: 0.0021082 | Grad norm: 1.164161 | Time: 33s678ms
Epoch: 002 | Test Loss: 0.0070167 | Time: 432ms
==> Save the model at epoch 002 with test loss 0.0070167
Epoch: 003 | Loss: 0.0089476 | L2 loss: 0.0068650 | Lip loss: 0.0020825 | Grad norm: 0.920222 | Time: 34s457ms
Epoch: 003 | Test Loss: 0.0070760 | Time: 506ms
Epoch: 004 | Loss: 0.0086178 | L2 loss: 0.0065509 | Lip loss: 0.0020669 | Grad norm: 0.989111 | Time: 33s504ms
Epoch: 004 | Test Loss: 0.0061505 | Time: 436ms
==> Save the model at epoch 004 with test loss 0.0061505
Epoch: 005 | Loss: 0.0083152 | L2 loss: 0.0062524 | Lip loss: 0.0020628 | Grad norm: 0.830997 | Time: 33s786ms
Epoch: 005 | Test Loss: 0.0059391 | Time: 447ms
==> Save the model at epoch 005 with test loss 0.0059391
Epoch: 006 | Loss: 0.0079256 | L2 loss: 0.0058632 | Lip loss: 0.0020624 | Grad norm: 0.359565 | Time: 33s796ms
Epoch: 006 | Test Loss: 0.0058913 | Time: 503ms
==> Save the model at epoch 006 with test loss 0.0058913
Epoch: 007 | Loss: 0.0079095 | L2 loss: 0.0058469 | Lip loss: 0.0020626 | Grad norm: 0.340812 | Time: 32s995ms
Epoch: 007 | Test Loss: 0.0058683 | Time: 511ms
==> Save the model at epoch 007 with test loss 0.0058683
Epoch: 008 | Loss: 0.0078955 | L2 loss: 0.0058335 | Lip loss: 0.0020620 | Grad norm: 0.336897 | Time: 34s573ms
Epoch: 008 | Test Loss: 0.0058748 | Time: 461ms
Epoch: 009 | Loss: 0.0078866 | L2 loss: 0.0058300 | Lip loss: 0.0020566 | Grad norm: 0.360893 | Time: 34s28ms
Epoch: 009 | Test Loss: 0.0058499 | Time: 433ms
==> Save the model at epoch 009 with test loss 0.0058499
Epoch: 010 | Loss: 0.0078816 | L2 loss: 0.0058247 | Lip loss: 0.0020569 | Grad norm: 0.379980 | Time: 33s756ms
Epoch: 010 | Test Loss: 0.0058641 | Time: 426ms
Epoch: 011 | Loss: 0.0078319 | L2 loss: 0.0057765 | Lip loss: 0.0020554 | Grad norm: 0.274352 | Time: 34s224ms
Epoch: 011 | Test Loss: 0.0058203 | Time: 446ms
==> Save the model at epoch 011 with test loss 0.0058203
Epoch: 012 | Loss: 0.0078294 | L2 loss: 0.0057734 | Lip loss: 0.0020560 | Grad norm: 0.269206 | Time: 33s736ms
Epoch: 012 | Test Loss: 0.0058226 | Time: 440ms
Epoch: 013 | Loss: 0.0078291 | L2 loss: 0.0057729 | Lip loss: 0.0020562 | Grad norm: 0.277277 | Time: 34s171ms
Epoch: 013 | Test Loss: 0.0058183 | Time: 436ms
==> Save the model at epoch 013 with test loss 0.0058183
Epoch: 014 | Loss: 0.0078244 | L2 loss: 0.0057710 | Lip loss: 0.0020534 | Grad norm: 0.277319 | Time: 34s429ms
Epoch: 014 | Test Loss: 0.0058240 | Time: 434ms
Epoch: 015 | Loss: 0.0078271 | L2 loss: 0.0057715 | Lip loss: 0.0020557 | Grad norm: 0.280534 | Time: 33s600ms
Epoch: 015 | Test Loss: 0.0058205 | Time: 509ms
Epoch: 016 | Loss: 0.0078212 | L2 loss: 0.0057648 | Lip loss: 0.0020564 | Grad norm: 0.259111 | Time: 33s582ms
Epoch: 016 | Test Loss: 0.0058141 | Time: 503ms
==> Save the model at epoch 016 with test loss 0.0058141
Epoch: 017 | Loss: 0.0078218 | L2 loss: 0.0057640 | Lip loss: 0.0020577 | Grad norm: 0.256541 | Time: 34s176ms
Epoch: 017 | Test Loss: 0.0058153 | Time: 430ms
Epoch: 018 | Loss: 0.0078205 | L2 loss: 0.0057639 | Lip loss: 0.0020567 | Grad norm: 0.254745 | Time: 33s799ms
Epoch: 018 | Test Loss: 0.0058148 | Time: 444ms
Epoch: 019 | Loss: 0.0078229 | L2 loss: 0.0057637 | Lip loss: 0.0020592 | Grad norm: 0.256899 | Time: 33s328ms
Epoch: 019 | Test Loss: 0.0058148 | Time: 437ms
Epoch: 020 | Loss: 0.0078211 | L2 loss: 0.0057637 | Lip loss: 0.0020574 | Grad norm: 0.261950 | Time: 33s385ms
Epoch: 020 | Test Loss: 0.0058140 | Time: 430ms
==> Save the model at epoch 020 with test loss 0.0058140
Epoch: 021 | Loss: 0.0078203 | L2 loss: 0.0057627 | Lip loss: 0.0020577 | Grad norm: 0.255334 | Time: 34s243ms
Epoch: 021 | Test Loss: 0.0058139 | Time: 451ms
==> Save the model at epoch 021 with test loss 0.0058139
Epoch: 022 | Loss: 0.0078166 | L2 loss: 0.0057626 | Lip loss: 0.0020540 | Grad norm: 0.250222 | Time: 33s547ms
Epoch: 022 | Test Loss: 0.0058144 | Time: 436ms
Epoch: 023 | Loss: 0.0078177 | L2 loss: 0.0057627 | Lip loss: 0.0020550 | Grad norm: 0.260582 | Time: 33s990ms
Epoch: 023 | Test Loss: 0.0058143 | Time: 430ms
Epoch: 024 | Loss: 0.0078165 | L2 loss: 0.0057626 | Lip loss: 0.0020538 | Grad norm: 0.255823 | Time: 32s588ms
Epoch: 024 | Test Loss: 0.0058142 | Time: 505ms
Epoch: 025 | Loss: 0.0078203 | L2 loss: 0.0057626 | Lip loss: 0.0020577 | Grad norm: 0.247671 | Time: 33s826ms
Epoch: 025 | Test Loss: 0.0058142 | Time: 499ms
Epoch: 026 | Loss: 0.0078190 | L2 loss: 0.0057625 | Lip loss: 0.0020565 | Grad norm: 0.253225 | Time: 34s137ms
Epoch: 026 | Test Loss: 0.0058142 | Time: 435ms
Epoch: 027 | Loss: 0.0078231 | L2 loss: 0.0057625 | Lip loss: 0.0020606 | Grad norm: 0.254388 | Time: 34s127ms
Epoch: 027 | Test Loss: 0.0058142 | Time: 429ms
Epoch: 028 | Loss: 0.0078198 | L2 loss: 0.0057625 | Lip loss: 0.0020573 | Grad norm: 0.250736 | Time: 34s80ms
Epoch: 028 | Test Loss: 0.0058142 | Time: 437ms
Epoch: 029 | Loss: 0.0078193 | L2 loss: 0.0057625 | Lip loss: 0.0020568 | Grad norm: 0.263052 | Time: 33s552ms
Epoch: 029 | Test Loss: 0.0058142 | Time: 441ms
Epoch: 030 | Loss: 0.0078205 | L2 loss: 0.0057625 | Lip loss: 0.0020579 | Grad norm: 0.250252 | Time: 34s34ms
Epoch: 030 | Test Loss: 0.0058142 | Time: 435ms
Epoch: 031 | Loss: 0.0078176 | L2 loss: 0.0057625 | Lip loss: 0.0020551 | Grad norm: 0.250557 | Time: 34s524ms
Epoch: 031 | Test Loss: 0.0058142 | Time: 449ms
Epoch: 032 | Loss: 0.0078199 | L2 loss: 0.0057625 | Lip loss: 0.0020573 | Grad norm: 0.251205 | Time: 34s68ms
Epoch: 032 | Test Loss: 0.0058142 | Time: 507ms
Epoch: 033 | Loss: 0.0078169 | L2 loss: 0.0057625 | Lip loss: 0.0020544 | Grad norm: 0.260082 | Time: 33s715ms
Epoch: 033 | Test Loss: 0.0058142 | Time: 439ms
Epoch: 034 | Loss: 0.0078204 | L2 loss: 0.0057625 | Lip loss: 0.0020579 | Grad norm: 0.253271 | Time: 33s589ms
Epoch: 034 | Test Loss: 0.0058142 | Time: 510ms
Epoch: 035 | Loss: 0.0078188 | L2 loss: 0.0057625 | Lip loss: 0.0020563 | Grad norm: 0.253870 | Time: 33s79ms
Epoch: 035 | Test Loss: 0.0058142 | Time: 519ms
Epoch: 036 | Loss: 0.0078193 | L2 loss: 0.0057625 | Lip loss: 0.0020567 | Grad norm: 0.247135 | Time: 34s198ms
Epoch: 036 | Test Loss: 0.0058142 | Time: 432ms
Epoch: 037 | Loss: 0.0078194 | L2 loss: 0.0057625 | Lip loss: 0.0020568 | Grad norm: 0.259357 | Time: 33s927ms
Epoch: 037 | Test Loss: 0.0058142 | Time: 432ms
Epoch: 038 | Loss: 0.0078216 | L2 loss: 0.0057625 | Lip loss: 0.0020591 | Grad norm: 0.256445 | Time: 33s953ms
Epoch: 038 | Test Loss: 0.0058142 | Time: 439ms
Epoch: 039 | Loss: 0.0078181 | L2 loss: 0.0057625 | Lip loss: 0.0020556 | Grad norm: 0.255669 | Time: 34s151ms
Epoch: 039 | Test Loss: 0.0058142 | Time: 432ms
Epoch: 040 | Loss: 0.0078208 | L2 loss: 0.0057625 | Lip loss: 0.0020582 | Grad norm: 0.249541 | Time: 34s343ms
Epoch: 040 | Test Loss: 0.0058142 | Time: 430ms
Total time: 22m53s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
