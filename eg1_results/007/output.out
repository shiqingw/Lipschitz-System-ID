==> torch device:  cuda:2
==> Lipschitz constant: 4.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 128]                  16,897
│    └─SandwichFc: 2-2                   [1, 128]                  33,025
│    └─SandwichFc: 2-3                   [1, 128]                  33,025
│    └─SandwichFc: 2-4                   [1, 128]                  33,025
│    └─SandwichFc: 2-5                   [1, 128]                  33,025
│    └─SandwichFc: 2-6                   [1, 128]                  33,025
│    └─SandwichFc: 2-7                   [1, 128]                  33,025
│    └─SandwichLin: 2-8                  [1, 2]                    263
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 128]                  (recursive)
│    └─SandwichFc: 2-10                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-11                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-12                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-13                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-14                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-15                  [1, 128]                  (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 215,316
Trainable params: 215,310
Non-trainable params: 6
Total mult-adds (M): 0.43
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.50
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.9783043 | Grad norm: 30.685075 | Time: 10s101ms
Epoch: 001 | Test Loss: 30.9740978 | Time: 803ms
==> Save the model at epoch 001 with test loss 30.9740978
Epoch: 002 | Train Loss: 2.4403335 | Grad norm: 14.500189 | Time: 8s837ms
Epoch: 002 | Test Loss: 0.0191465 | Time: 816ms
==> Save the model at epoch 002 with test loss 0.0191465
Epoch: 003 | Train Loss: 0.0447770 | Grad norm: 10.574220 | Time: 8s959ms
Epoch: 003 | Test Loss: 0.0178875 | Time: 834ms
==> Save the model at epoch 003 with test loss 0.0178875
Epoch: 004 | Train Loss: 0.0459545 | Grad norm: 11.719970 | Time: 9s795ms
Epoch: 004 | Test Loss: 0.0367913 | Time: 841ms
Epoch: 005 | Train Loss: 0.0649622 | Grad norm: 12.900147 | Time: 9s826ms
Epoch: 005 | Test Loss: 0.0412169 | Time: 791ms
Epoch: 006 | Train Loss: 0.0664833 | Grad norm: 14.171349 | Time: 11s92ms
Epoch: 006 | Test Loss: 0.0639251 | Time: 828ms
Epoch: 007 | Train Loss: 0.0743098 | Grad norm: 14.248820 | Time: 10s836ms
Epoch: 007 | Test Loss: 0.0786660 | Time: 838ms
Epoch: 008 | Train Loss: 0.0854267 | Grad norm: 14.172082 | Time: 9s788ms
Epoch: 008 | Test Loss: 0.0495235 | Time: 805ms
Epoch: 009 | Train Loss: 0.0986379 | Grad norm: 14.690433 | Time: 10s433ms
Epoch: 009 | Test Loss: 0.0938990 | Time: 822ms
Epoch: 010 | Train Loss: 0.0811191 | Grad norm: 13.784968 | Time: 11s602ms
Epoch: 010 | Test Loss: 0.0831671 | Time: 845ms
Epoch: 011 | Train Loss: 0.0796425 | Grad norm: 12.950800 | Time: 11s868ms
Epoch: 011 | Test Loss: 0.1179388 | Time: 835ms
Epoch: 012 | Train Loss: 0.0694054 | Grad norm: 11.549711 | Time: 10s565ms
Epoch: 012 | Test Loss: 0.0589105 | Time: 808ms
Epoch: 013 | Train Loss: 0.0592566 | Grad norm: 10.302143 | Time: 9s522ms
Epoch: 013 | Test Loss: 0.0382745 | Time: 842ms
Epoch: 014 | Train Loss: 0.0556846 | Grad norm: 9.328514 | Time: 10s752ms
Epoch: 014 | Test Loss: 0.0666972 | Time: 805ms
Epoch: 015 | Train Loss: 0.0540695 | Grad norm: 9.083661 | Time: 11s314ms
Epoch: 015 | Test Loss: 0.0363595 | Time: 839ms
Epoch: 016 | Train Loss: 0.0455970 | Grad norm: 8.252532 | Time: 10s350ms
Epoch: 016 | Test Loss: 0.0342442 | Time: 804ms
Epoch: 017 | Train Loss: 0.0363119 | Grad norm: 7.553756 | Time: 10s985ms
Epoch: 017 | Test Loss: 0.0302303 | Time: 850ms
Epoch: 018 | Train Loss: 0.0352936 | Grad norm: 7.096542 | Time: 11s482ms
Epoch: 018 | Test Loss: 0.0401552 | Time: 842ms
Epoch: 019 | Train Loss: 0.0319407 | Grad norm: 6.638396 | Time: 9s20ms
Epoch: 019 | Test Loss: 0.0283514 | Time: 837ms
Epoch: 020 | Train Loss: 0.0297429 | Grad norm: 6.274054 | Time: 9s180ms
Epoch: 020 | Test Loss: 0.0290812 | Time: 828ms
Epoch: 021 | Train Loss: 0.0321840 | Grad norm: 6.035964 | Time: 9s47ms
Epoch: 021 | Test Loss: 0.0167759 | Time: 810ms
==> Save the model at epoch 021 with test loss 0.0167759
Epoch: 022 | Train Loss: 0.0347993 | Grad norm: 5.734587 | Time: 9s252ms
Epoch: 022 | Test Loss: 0.0284424 | Time: 863ms
Epoch: 023 | Train Loss: 0.0284180 | Grad norm: 5.362589 | Time: 8s965ms
Epoch: 023 | Test Loss: 0.0285455 | Time: 812ms
Epoch: 024 | Train Loss: 0.0214518 | Grad norm: 5.295110 | Time: 10s5ms
Epoch: 024 | Test Loss: 0.0249505 | Time: 816ms
Epoch: 025 | Train Loss: 0.0200626 | Grad norm: 4.937249 | Time: 12s7ms
Epoch: 025 | Test Loss: 0.0220134 | Time: 794ms
Epoch: 026 | Train Loss: 0.0187863 | Grad norm: 4.664189 | Time: 10s885ms
Epoch: 026 | Test Loss: 0.0148189 | Time: 804ms
==> Save the model at epoch 026 with test loss 0.0148189
Epoch: 027 | Train Loss: 0.0188156 | Grad norm: 4.350225 | Time: 8s858ms
Epoch: 027 | Test Loss: 0.0133602 | Time: 890ms
==> Save the model at epoch 027 with test loss 0.0133602
Epoch: 028 | Train Loss: 0.0171938 | Grad norm: 4.191235 | Time: 9s285ms
Epoch: 028 | Test Loss: 0.0153029 | Time: 808ms
Epoch: 029 | Train Loss: 0.0158689 | Grad norm: 3.918583 | Time: 11s196ms
Epoch: 029 | Test Loss: 0.0137938 | Time: 844ms
Epoch: 030 | Train Loss: 0.0151182 | Grad norm: 3.715210 | Time: 10s867ms
Epoch: 030 | Test Loss: 0.0147463 | Time: 811ms
Epoch: 031 | Train Loss: 0.0151378 | Grad norm: 3.503737 | Time: 10s783ms
Epoch: 031 | Test Loss: 0.0134532 | Time: 800ms
Epoch: 032 | Train Loss: 0.0136729 | Grad norm: 3.281712 | Time: 10s321ms
Epoch: 032 | Test Loss: 0.0108889 | Time: 876ms
==> Save the model at epoch 032 with test loss 0.0108889
Epoch: 033 | Train Loss: 0.0134383 | Grad norm: 3.126135 | Time: 10s202ms
Epoch: 033 | Test Loss: 0.0150428 | Time: 802ms
Epoch: 034 | Train Loss: 0.0123995 | Grad norm: 2.907102 | Time: 10s205ms
Epoch: 034 | Test Loss: 0.0113974 | Time: 807ms
Epoch: 035 | Train Loss: 0.0140545 | Grad norm: 2.893698 | Time: 10s59ms
Epoch: 035 | Test Loss: 0.0142805 | Time: 799ms
Epoch: 036 | Train Loss: 0.0113689 | Grad norm: 2.591433 | Time: 8s844ms
Epoch: 036 | Test Loss: 0.0110007 | Time: 801ms
Epoch: 037 | Train Loss: 0.0124254 | Grad norm: 2.441440 | Time: 8s856ms
Epoch: 037 | Test Loss: 0.0086788 | Time: 866ms
==> Save the model at epoch 037 with test loss 0.0086788
Epoch: 038 | Train Loss: 0.0106715 | Grad norm: 2.273407 | Time: 8s845ms
Epoch: 038 | Test Loss: 0.0102376 | Time: 799ms
Epoch: 039 | Train Loss: 0.0104321 | Grad norm: 1.881095 | Time: 9s155ms
Epoch: 039 | Test Loss: 0.0095834 | Time: 818ms
Epoch: 040 | Train Loss: 0.0099652 | Grad norm: 1.784938 | Time: 9s91ms
Epoch: 040 | Test Loss: 0.0118576 | Time: 845ms
Epoch: 041 | Train Loss: 0.0124272 | Grad norm: 3.046449 | Time: 8s983ms
Epoch: 041 | Test Loss: 0.0115912 | Time: 832ms
Epoch: 042 | Train Loss: 0.0116984 | Grad norm: 2.915648 | Time: 9s909ms
Epoch: 042 | Test Loss: 0.0098496 | Time: 864ms
Epoch: 043 | Train Loss: 0.0095079 | Grad norm: 1.605211 | Time: 10s195ms
Epoch: 043 | Test Loss: 0.0093641 | Time: 799ms
Epoch: 044 | Train Loss: 0.0096884 | Grad norm: 1.859492 | Time: 10s844ms
Epoch: 044 | Test Loss: 0.0087909 | Time: 811ms
Epoch: 045 | Train Loss: 0.0082476 | Grad norm: 0.991901 | Time: 10s415ms
Epoch: 045 | Test Loss: 0.0092775 | Time: 804ms
Epoch: 046 | Train Loss: 0.0083400 | Grad norm: 1.015307 | Time: 8s826ms
Epoch: 046 | Test Loss: 0.0080467 | Time: 801ms
==> Save the model at epoch 046 with test loss 0.0080467
Epoch: 047 | Train Loss: 0.0081222 | Grad norm: 1.015117 | Time: 9s167ms
Epoch: 047 | Test Loss: 0.0076595 | Time: 864ms
==> Save the model at epoch 047 with test loss 0.0076595
Epoch: 048 | Train Loss: 0.0078540 | Grad norm: 0.769609 | Time: 8s940ms
Epoch: 048 | Test Loss: 0.0080319 | Time: 818ms
Epoch: 049 | Train Loss: 0.0077622 | Grad norm: 0.738325 | Time: 8s934ms
Epoch: 049 | Test Loss: 0.0077038 | Time: 802ms
Epoch: 050 | Train Loss: 0.0076854 | Grad norm: 0.720360 | Time: 9s233ms
Epoch: 050 | Test Loss: 0.0077612 | Time: 891ms
Epoch: 051 | Train Loss: 0.0075785 | Grad norm: 0.671413 | Time: 8s929ms
Epoch: 051 | Test Loss: 0.0078151 | Time: 801ms
Epoch: 052 | Train Loss: 0.0075709 | Grad norm: 0.699160 | Time: 8s942ms
Epoch: 052 | Test Loss: 0.0074200 | Time: 859ms
==> Save the model at epoch 052 with test loss 0.0074200
Epoch: 053 | Train Loss: 0.0074837 | Grad norm: 0.682607 | Time: 8s914ms
Epoch: 053 | Test Loss: 0.0074788 | Time: 832ms
Epoch: 054 | Train Loss: 0.0073484 | Grad norm: 0.532950 | Time: 8s936ms
Epoch: 054 | Test Loss: 0.0073506 | Time: 811ms
==> Save the model at epoch 054 with test loss 0.0073506
Epoch: 055 | Train Loss: 0.0073310 | Grad norm: 0.523890 | Time: 8s951ms
Epoch: 055 | Test Loss: 0.0072881 | Time: 801ms
==> Save the model at epoch 055 with test loss 0.0072881
Epoch: 056 | Train Loss: 0.0072305 | Grad norm: 0.402982 | Time: 10s62ms
Epoch: 056 | Test Loss: 0.0072158 | Time: 818ms
==> Save the model at epoch 056 with test loss 0.0072158
Epoch: 057 | Train Loss: 0.0071954 | Grad norm: 0.386491 | Time: 11s648ms
Epoch: 057 | Test Loss: 0.0072393 | Time: 807ms
Epoch: 058 | Train Loss: 0.0071629 | Grad norm: 0.368259 | Time: 9s750ms
Epoch: 058 | Test Loss: 0.0072363 | Time: 887ms
Epoch: 059 | Train Loss: 0.0071335 | Grad norm: 0.328223 | Time: 11s154ms
Epoch: 059 | Test Loss: 0.0071952 | Time: 838ms
==> Save the model at epoch 059 with test loss 0.0071952
Epoch: 060 | Train Loss: 0.0071146 | Grad norm: 0.317592 | Time: 11s619ms
Epoch: 060 | Test Loss: 0.0071875 | Time: 826ms
==> Save the model at epoch 060 with test loss 0.0071875
Total time: 10m47s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
