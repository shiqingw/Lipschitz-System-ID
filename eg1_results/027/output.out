==> torch device:  cuda:2
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.50
==> Further split seed:  None
==> Lipschitz constant: 0.50
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.70982397 0.70987433]
==> Ouput transform to be applied to the neural network:
[2.8319 2.8318]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 64]                   4,353
│    └─SandwichFc: 2-2                   [1, 64]                   8,321
│    └─SandwichFc: 2-3                   [1, 64]                   8,321
│    └─SandwichFc: 2-4                   [1, 64]                   8,321
│    └─SandwichFc: 2-5                   [1, 64]                   8,321
│    └─SandwichFc: 2-6                   [1, 64]                   8,321
│    └─SandwichFc: 2-7                   [1, 64]                   8,321
│    └─SandwichLin: 2-8                  [1, 2]                    135
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 64]                   (recursive)
│    └─SandwichFc: 2-10                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-11                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-12                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-13                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-14                  [1, 64]                   (recursive)
│    └─SandwichFc: 2-15                  [1, 64]                   (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 54,420
Trainable params: 54,414
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.11
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.22
Estimated Total Size (MB): 0.22
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 16.1560010 | Grad norm: 1.832943 | Time: 8s431ms
Epoch: 001 | Test Loss: 15.9430370 | Time: 579ms
==> Save the model at epoch 001 with test loss 15.9430370
Epoch: 002 | Train Loss: 10.2452767 | Grad norm: 6.188767 | Time: 8s263ms
Epoch: 002 | Test Loss: 8.1103720 | Time: 619ms
==> Save the model at epoch 002 with test loss 8.1103720
Epoch: 003 | Train Loss: 7.5419014 | Grad norm: 5.573513 | Time: 8s201ms
Epoch: 003 | Test Loss: 6.8243394 | Time: 564ms
==> Save the model at epoch 003 with test loss 6.8243394
Epoch: 004 | Train Loss: 6.2905970 | Grad norm: 4.793996 | Time: 8s132ms
Epoch: 004 | Test Loss: 5.5950392 | Time: 674ms
==> Save the model at epoch 004 with test loss 5.5950392
Epoch: 005 | Train Loss: 5.1220447 | Grad norm: 3.305571 | Time: 8s540ms
Epoch: 005 | Test Loss: 4.6005636 | Time: 572ms
==> Save the model at epoch 005 with test loss 4.6005636
Epoch: 006 | Train Loss: 4.4050103 | Grad norm: 2.868479 | Time: 8s497ms
Epoch: 006 | Test Loss: 4.1667112 | Time: 553ms
==> Save the model at epoch 006 with test loss 4.1667112
Epoch: 007 | Train Loss: 4.1524579 | Grad norm: 3.132587 | Time: 8s273ms
Epoch: 007 | Test Loss: 4.0574576 | Time: 555ms
==> Save the model at epoch 007 with test loss 4.0574576
Epoch: 008 | Train Loss: 4.0949976 | Grad norm: 3.815968 | Time: 8s500ms
Epoch: 008 | Test Loss: 4.0306789 | Time: 559ms
==> Save the model at epoch 008 with test loss 4.0306789
Epoch: 009 | Train Loss: 4.0710228 | Grad norm: 3.660982 | Time: 8s296ms
Epoch: 009 | Test Loss: 4.0060728 | Time: 565ms
==> Save the model at epoch 009 with test loss 4.0060728
Epoch: 010 | Train Loss: 4.0594177 | Grad norm: 3.185370 | Time: 8s204ms
Epoch: 010 | Test Loss: 3.9995994 | Time: 626ms
==> Save the model at epoch 010 with test loss 3.9995994
Epoch: 011 | Train Loss: 4.0579325 | Grad norm: 2.819876 | Time: 8s345ms
Epoch: 011 | Test Loss: 4.0083158 | Time: 557ms
Epoch: 012 | Train Loss: 4.0554309 | Grad norm: 2.540957 | Time: 8s597ms
Epoch: 012 | Test Loss: 4.0004365 | Time: 555ms
Epoch: 013 | Train Loss: 4.0501901 | Grad norm: 2.294316 | Time: 8s260ms
Epoch: 013 | Test Loss: 3.9931776 | Time: 555ms
==> Save the model at epoch 013 with test loss 3.9931776
Epoch: 014 | Train Loss: 4.0513800 | Grad norm: 2.158709 | Time: 8s486ms
Epoch: 014 | Test Loss: 3.9926649 | Time: 555ms
==> Save the model at epoch 014 with test loss 3.9926649
Epoch: 015 | Train Loss: 4.0475343 | Grad norm: 1.747806 | Time: 8s70ms
Epoch: 015 | Test Loss: 3.9935887 | Time: 622ms
Epoch: 016 | Train Loss: 4.0450555 | Grad norm: 1.754103 | Time: 8s411ms
Epoch: 016 | Test Loss: 3.9950452 | Time: 566ms
Epoch: 017 | Train Loss: 4.0461350 | Grad norm: 1.074712 | Time: 8s419ms
Epoch: 017 | Test Loss: 3.9917323 | Time: 555ms
==> Save the model at epoch 017 with test loss 3.9917323
Epoch: 018 | Train Loss: 4.0444425 | Grad norm: 0.696906 | Time: 8s186ms
Epoch: 018 | Test Loss: 3.9909139 | Time: 551ms
==> Save the model at epoch 018 with test loss 3.9909139
Epoch: 019 | Train Loss: 4.0446440 | Grad norm: 1.225946 | Time: 8s540ms
Epoch: 019 | Test Loss: 3.9873410 | Time: 554ms
==> Save the model at epoch 019 with test loss 3.9873410
Epoch: 020 | Train Loss: 4.0414222 | Grad norm: 0.329573 | Time: 8s342ms
Epoch: 020 | Test Loss: 3.9869753 | Time: 567ms
==> Save the model at epoch 020 with test loss 3.9869753
Epoch: 021 | Train Loss: 4.0433882 | Grad norm: 0.478998 | Time: 8s352ms
Epoch: 021 | Test Loss: 3.9896551 | Time: 550ms
Epoch: 022 | Train Loss: 4.0429901 | Grad norm: 0.649431 | Time: 8s446ms
Epoch: 022 | Test Loss: 3.9984315 | Time: 694ms
Epoch: 023 | Train Loss: 4.0499754 | Grad norm: 2.315221 | Time: 8s325ms
Epoch: 023 | Test Loss: 3.9903644 | Time: 624ms
Epoch: 024 | Train Loss: 4.0436181 | Grad norm: 0.639833 | Time: 7s877ms
Epoch: 024 | Test Loss: 3.9867746 | Time: 550ms
==> Save the model at epoch 024 with test loss 3.9867746
Epoch: 025 | Train Loss: 4.0429637 | Grad norm: 0.122659 | Time: 8s406ms
Epoch: 025 | Test Loss: 3.9867908 | Time: 556ms
Epoch: 026 | Train Loss: 4.0396230 | Grad norm: 0.171880 | Time: 8s239ms
Epoch: 026 | Test Loss: 3.9867638 | Time: 571ms
==> Save the model at epoch 026 with test loss 3.9867638
Epoch: 027 | Train Loss: 4.0431846 | Grad norm: 0.155097 | Time: 8s198ms
Epoch: 027 | Test Loss: 3.9867818 | Time: 555ms
Epoch: 028 | Train Loss: 4.0416184 | Grad norm: 0.157481 | Time: 8s353ms
Epoch: 028 | Test Loss: 3.9867558 | Time: 630ms
==> Save the model at epoch 028 with test loss 3.9867558
Epoch: 029 | Train Loss: 4.0423426 | Grad norm: 0.155677 | Time: 7s894ms
Epoch: 029 | Test Loss: 3.9867726 | Time: 555ms
Epoch: 030 | Train Loss: 4.0385941 | Grad norm: 0.126194 | Time: 8s413ms
Epoch: 030 | Test Loss: 3.9867646 | Time: 560ms
Epoch: 031 | Train Loss: 4.0415044 | Grad norm: 0.111831 | Time: 8s603ms
Epoch: 031 | Test Loss: 3.9867872 | Time: 558ms
Epoch: 032 | Train Loss: 4.0440058 | Grad norm: 0.110921 | Time: 8s400ms
Epoch: 032 | Test Loss: 3.9867581 | Time: 554ms
Epoch: 033 | Train Loss: 4.0440500 | Grad norm: 0.115642 | Time: 8s111ms
Epoch: 033 | Test Loss: 3.9867703 | Time: 562ms
Epoch: 034 | Train Loss: 4.0380818 | Grad norm: 0.105094 | Time: 8s397ms
Epoch: 034 | Test Loss: 3.9867730 | Time: 555ms
Epoch: 035 | Train Loss: 4.0408081 | Grad norm: 0.087880 | Time: 8s423ms
Epoch: 035 | Test Loss: 3.9868446 | Time: 556ms
Epoch: 036 | Train Loss: 4.0376275 | Grad norm: 0.075829 | Time: 8s205ms
Epoch: 036 | Test Loss: 3.9867945 | Time: 630ms
Epoch: 037 | Train Loss: 4.0413495 | Grad norm: 0.059172 | Time: 8s408ms
Epoch: 037 | Test Loss: 3.9867595 | Time: 555ms
Epoch: 038 | Train Loss: 4.0404543 | Grad norm: 0.057746 | Time: 8s671ms
Epoch: 038 | Test Loss: 3.9867615 | Time: 569ms
Epoch: 039 | Train Loss: 4.0403642 | Grad norm: 0.041588 | Time: 8s380ms
Epoch: 039 | Test Loss: 3.9867569 | Time: 566ms
Epoch: 040 | Train Loss: 4.0429949 | Grad norm: 0.032255 | Time: 8s621ms
Epoch: 040 | Test Loss: 3.9867571 | Time: 555ms
Total time: 5m56s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.70982397 0.70987433]
==> Output transform to be applied to the neural network (trained):
[2.8319 2.8318]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
