==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 1.00
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 8.7522160 | L2 loss: 8.7407389 | Lip loss: 0.0114771 | Grad norm: 4.854507 | Time: 34s140ms
Epoch: 001 | Test Loss: 0.0189864 | Time: 456ms
==> Save the model at epoch 001 with test loss 0.0189864
Epoch: 002 | Loss: 0.0316153 | L2 loss: 0.0103580 | Lip loss: 0.0212573 | Grad norm: 1.288786 | Time: 33s467ms
Epoch: 002 | Test Loss: 0.0087525 | Time: 441ms
==> Save the model at epoch 002 with test loss 0.0087525
Epoch: 003 | Loss: 0.0279891 | L2 loss: 0.0072510 | Lip loss: 0.0207381 | Grad norm: 1.021069 | Time: 33s724ms
Epoch: 003 | Test Loss: 0.0071151 | Time: 444ms
==> Save the model at epoch 003 with test loss 0.0071151
Epoch: 004 | Loss: 0.0272565 | L2 loss: 0.0066559 | Lip loss: 0.0206006 | Grad norm: 0.984574 | Time: 34s310ms
Epoch: 004 | Test Loss: 0.0065286 | Time: 437ms
==> Save the model at epoch 004 with test loss 0.0065286
Epoch: 005 | Loss: 0.0269683 | L2 loss: 0.0063886 | Lip loss: 0.0205797 | Grad norm: 0.949400 | Time: 34s151ms
Epoch: 005 | Test Loss: 0.0064826 | Time: 432ms
==> Save the model at epoch 005 with test loss 0.0064826
Epoch: 006 | Loss: 0.0264501 | L2 loss: 0.0059454 | Lip loss: 0.0205047 | Grad norm: 0.414766 | Time: 34s691ms
Epoch: 006 | Test Loss: 0.0058680 | Time: 506ms
==> Save the model at epoch 006 with test loss 0.0058680
Epoch: 007 | Loss: 0.0263915 | L2 loss: 0.0059128 | Lip loss: 0.0204787 | Grad norm: 0.407301 | Time: 34s433ms
Epoch: 007 | Test Loss: 0.0058465 | Time: 505ms
==> Save the model at epoch 007 with test loss 0.0058465
Epoch: 008 | Loss: 0.0263680 | L2 loss: 0.0059002 | Lip loss: 0.0204678 | Grad norm: 0.396783 | Time: 34s564ms
Epoch: 008 | Test Loss: 0.0058517 | Time: 438ms
Epoch: 009 | Loss: 0.0263866 | L2 loss: 0.0059050 | Lip loss: 0.0204816 | Grad norm: 0.443599 | Time: 34s147ms
Epoch: 009 | Test Loss: 0.0058244 | Time: 433ms
==> Save the model at epoch 009 with test loss 0.0058244
Epoch: 010 | Loss: 0.0263648 | L2 loss: 0.0058807 | Lip loss: 0.0204841 | Grad norm: 0.420836 | Time: 34s464ms
Epoch: 010 | Test Loss: 0.0058681 | Time: 430ms
Epoch: 011 | Loss: 0.0263018 | L2 loss: 0.0058307 | Lip loss: 0.0204711 | Grad norm: 0.308538 | Time: 34s48ms
Epoch: 011 | Test Loss: 0.0058063 | Time: 538ms
==> Save the model at epoch 011 with test loss 0.0058063
Epoch: 012 | Loss: 0.0262853 | L2 loss: 0.0058270 | Lip loss: 0.0204583 | Grad norm: 0.308463 | Time: 34s335ms
Epoch: 012 | Test Loss: 0.0057968 | Time: 441ms
==> Save the model at epoch 012 with test loss 0.0057968
Epoch: 013 | Loss: 0.0262772 | L2 loss: 0.0058241 | Lip loss: 0.0204531 | Grad norm: 0.295893 | Time: 34s716ms
Epoch: 013 | Test Loss: 0.0057933 | Time: 429ms
==> Save the model at epoch 013 with test loss 0.0057933
Epoch: 014 | Loss: 0.0262902 | L2 loss: 0.0058222 | Lip loss: 0.0204680 | Grad norm: 0.300278 | Time: 34s121ms
Epoch: 014 | Test Loss: 0.0058070 | Time: 459ms
Epoch: 015 | Loss: 0.0263019 | L2 loss: 0.0058206 | Lip loss: 0.0204813 | Grad norm: 0.302980 | Time: 34s706ms
Epoch: 015 | Test Loss: 0.0058035 | Time: 501ms
Epoch: 016 | Loss: 0.0263082 | L2 loss: 0.0058166 | Lip loss: 0.0204917 | Grad norm: 0.293661 | Time: 34s694ms
Epoch: 016 | Test Loss: 0.0057866 | Time: 502ms
==> Save the model at epoch 016 with test loss 0.0057866
Epoch: 017 | Loss: 0.0263000 | L2 loss: 0.0058151 | Lip loss: 0.0204849 | Grad norm: 0.290857 | Time: 34s861ms
Epoch: 017 | Test Loss: 0.0057869 | Time: 436ms
Epoch: 018 | Loss: 0.0263149 | L2 loss: 0.0058148 | Lip loss: 0.0205002 | Grad norm: 0.288060 | Time: 34s234ms
Epoch: 018 | Test Loss: 0.0057859 | Time: 447ms
==> Save the model at epoch 018 with test loss 0.0057859
Epoch: 019 | Loss: 0.0262840 | L2 loss: 0.0058145 | Lip loss: 0.0204695 | Grad norm: 0.301035 | Time: 33s713ms
Epoch: 019 | Test Loss: 0.0057853 | Time: 442ms
==> Save the model at epoch 019 with test loss 0.0057853
Epoch: 020 | Loss: 0.0262518 | L2 loss: 0.0058149 | Lip loss: 0.0204369 | Grad norm: 0.289908 | Time: 33s648ms
Epoch: 020 | Test Loss: 0.0057851 | Time: 432ms
==> Save the model at epoch 020 with test loss 0.0057851
Epoch: 021 | Loss: 0.0262689 | L2 loss: 0.0058132 | Lip loss: 0.0204557 | Grad norm: 0.277624 | Time: 33s704ms
Epoch: 021 | Test Loss: 0.0057852 | Time: 436ms
Epoch: 022 | Loss: 0.0262871 | L2 loss: 0.0058131 | Lip loss: 0.0204740 | Grad norm: 0.292051 | Time: 33s198ms
Epoch: 022 | Test Loss: 0.0057853 | Time: 442ms
Epoch: 023 | Loss: 0.0262918 | L2 loss: 0.0058132 | Lip loss: 0.0204786 | Grad norm: 0.279616 | Time: 34s1ms
Epoch: 023 | Test Loss: 0.0057854 | Time: 449ms
Epoch: 024 | Loss: 0.0262809 | L2 loss: 0.0058131 | Lip loss: 0.0204677 | Grad norm: 0.289598 | Time: 34s238ms
Epoch: 024 | Test Loss: 0.0057852 | Time: 519ms
Epoch: 025 | Loss: 0.0263023 | L2 loss: 0.0058131 | Lip loss: 0.0204892 | Grad norm: 0.294805 | Time: 33s792ms
Epoch: 025 | Test Loss: 0.0057853 | Time: 503ms
Epoch: 026 | Loss: 0.0263135 | L2 loss: 0.0058130 | Lip loss: 0.0205005 | Grad norm: 0.293935 | Time: 33s622ms
Epoch: 026 | Test Loss: 0.0057853 | Time: 436ms
Epoch: 027 | Loss: 0.0263102 | L2 loss: 0.0058130 | Lip loss: 0.0204971 | Grad norm: 0.282695 | Time: 34s122ms
Epoch: 027 | Test Loss: 0.0057853 | Time: 430ms
Epoch: 028 | Loss: 0.0262953 | L2 loss: 0.0058130 | Lip loss: 0.0204822 | Grad norm: 0.290845 | Time: 33s513ms
Epoch: 028 | Test Loss: 0.0057853 | Time: 434ms
Epoch: 029 | Loss: 0.0262964 | L2 loss: 0.0058130 | Lip loss: 0.0204834 | Grad norm: 0.291169 | Time: 33s597ms
Epoch: 029 | Test Loss: 0.0057853 | Time: 432ms
Epoch: 030 | Loss: 0.0262699 | L2 loss: 0.0058130 | Lip loss: 0.0204569 | Grad norm: 0.279433 | Time: 33s773ms
Epoch: 030 | Test Loss: 0.0057853 | Time: 430ms
Epoch: 031 | Loss: 0.0262988 | L2 loss: 0.0058130 | Lip loss: 0.0204858 | Grad norm: 0.290093 | Time: 33s530ms
Epoch: 031 | Test Loss: 0.0057853 | Time: 429ms
Epoch: 032 | Loss: 0.0262720 | L2 loss: 0.0058130 | Lip loss: 0.0204590 | Grad norm: 0.287575 | Time: 34s322ms
Epoch: 032 | Test Loss: 0.0057853 | Time: 501ms
Epoch: 033 | Loss: 0.0262753 | L2 loss: 0.0058130 | Lip loss: 0.0204623 | Grad norm: 0.292046 | Time: 34s514ms
Epoch: 033 | Test Loss: 0.0057853 | Time: 429ms
Epoch: 034 | Loss: 0.0262637 | L2 loss: 0.0058130 | Lip loss: 0.0204507 | Grad norm: 0.281145 | Time: 34s161ms
Epoch: 034 | Test Loss: 0.0057853 | Time: 499ms
Epoch: 035 | Loss: 0.0263145 | L2 loss: 0.0058130 | Lip loss: 0.0205015 | Grad norm: 0.283566 | Time: 34s188ms
Epoch: 035 | Test Loss: 0.0057853 | Time: 510ms
Epoch: 036 | Loss: 0.0262845 | L2 loss: 0.0058130 | Lip loss: 0.0204715 | Grad norm: 0.284200 | Time: 34s169ms
Epoch: 036 | Test Loss: 0.0057853 | Time: 432ms
Epoch: 037 | Loss: 0.0263003 | L2 loss: 0.0058130 | Lip loss: 0.0204873 | Grad norm: 0.293838 | Time: 33s964ms
Epoch: 037 | Test Loss: 0.0057853 | Time: 436ms
Epoch: 038 | Loss: 0.0262638 | L2 loss: 0.0058130 | Lip loss: 0.0204508 | Grad norm: 0.282482 | Time: 34s186ms
Epoch: 038 | Test Loss: 0.0057853 | Time: 429ms
Epoch: 039 | Loss: 0.0262640 | L2 loss: 0.0058130 | Lip loss: 0.0204510 | Grad norm: 0.286258 | Time: 34s29ms
Epoch: 039 | Test Loss: 0.0057853 | Time: 452ms
Epoch: 040 | Loss: 0.0262742 | L2 loss: 0.0058130 | Lip loss: 0.0204612 | Grad norm: 0.293087 | Time: 34s20ms
Epoch: 040 | Test Loss: 0.0057853 | Time: 433ms
Total time: 23m2s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
