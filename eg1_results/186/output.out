==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 18.9189925 | L2 loss: 18.9189917 | Lip loss: 0.0000009 | Grad norm: 0.869330 | Time: 7s532ms
Epoch: 001 | Test Loss: 18.9444762 | Time: 368ms
==> Save the model at epoch 001 with test loss 18.9444762
Epoch: 002 | Loss: 17.5465227 | L2 loss: 17.5465056 | Lip loss: 0.0000170 | Grad norm: 4.331184 | Time: 7s28ms
Epoch: 002 | Test Loss: 15.1920580 | Time: 370ms
==> Save the model at epoch 002 with test loss 15.1920580
Epoch: 003 | Loss: 9.7462999 | L2 loss: 9.7461588 | Lip loss: 0.0001410 | Grad norm: 12.258524 | Time: 7s169ms
Epoch: 003 | Test Loss: 3.6366754 | Time: 371ms
==> Save the model at epoch 003 with test loss 3.6366754
Epoch: 004 | Loss: 0.7295703 | L2 loss: 0.7293292 | Lip loss: 0.0002411 | Grad norm: 5.678550 | Time: 7s46ms
Epoch: 004 | Test Loss: 0.0257210 | Time: 360ms
==> Save the model at epoch 004 with test loss 0.0257210
Epoch: 005 | Loss: 0.0118218 | L2 loss: 0.0116064 | Lip loss: 0.0002154 | Grad norm: 1.902037 | Time: 7s101ms
Epoch: 005 | Test Loss: 0.0054274 | Time: 362ms
==> Save the model at epoch 005 with test loss 0.0054274
Epoch: 006 | Loss: 0.0052155 | L2 loss: 0.0050030 | Lip loss: 0.0002125 | Grad norm: 0.490353 | Time: 6s865ms
Epoch: 006 | Test Loss: 0.0048866 | Time: 438ms
==> Save the model at epoch 006 with test loss 0.0048866
Epoch: 007 | Loss: 0.0048528 | L2 loss: 0.0046424 | Lip loss: 0.0002105 | Grad norm: 0.464266 | Time: 7s43ms
Epoch: 007 | Test Loss: 0.0045091 | Time: 360ms
==> Save the model at epoch 007 with test loss 0.0045091
Epoch: 008 | Loss: 0.0044937 | L2 loss: 0.0042833 | Lip loss: 0.0002104 | Grad norm: 0.403782 | Time: 7s212ms
Epoch: 008 | Test Loss: 0.0042149 | Time: 362ms
==> Save the model at epoch 008 with test loss 0.0042149
Epoch: 009 | Loss: 0.0042240 | L2 loss: 0.0040129 | Lip loss: 0.0002111 | Grad norm: 0.413483 | Time: 6s989ms
Epoch: 009 | Test Loss: 0.0039137 | Time: 364ms
==> Save the model at epoch 009 with test loss 0.0039137
Epoch: 010 | Loss: 0.0039840 | L2 loss: 0.0037743 | Lip loss: 0.0002098 | Grad norm: 0.470440 | Time: 7s337ms
Epoch: 010 | Test Loss: 0.0036931 | Time: 363ms
==> Save the model at epoch 010 with test loss 0.0036931
Epoch: 011 | Loss: 0.0038109 | L2 loss: 0.0036014 | Lip loss: 0.0002095 | Grad norm: 0.311165 | Time: 7s156ms
Epoch: 011 | Test Loss: 0.0036460 | Time: 360ms
==> Save the model at epoch 011 with test loss 0.0036460
Epoch: 012 | Loss: 0.0037951 | L2 loss: 0.0035859 | Lip loss: 0.0002092 | Grad norm: 0.335294 | Time: 7s75ms
Epoch: 012 | Test Loss: 0.0036345 | Time: 375ms
==> Save the model at epoch 012 with test loss 0.0036345
Epoch: 013 | Loss: 0.0037629 | L2 loss: 0.0035515 | Lip loss: 0.0002114 | Grad norm: 0.336077 | Time: 6s928ms
Epoch: 013 | Test Loss: 0.0035982 | Time: 427ms
==> Save the model at epoch 013 with test loss 0.0035982
Epoch: 014 | Loss: 0.0037323 | L2 loss: 0.0035226 | Lip loss: 0.0002097 | Grad norm: 0.350725 | Time: 7s168ms
Epoch: 014 | Test Loss: 0.0035775 | Time: 366ms
==> Save the model at epoch 014 with test loss 0.0035775
Epoch: 015 | Loss: 0.0037108 | L2 loss: 0.0035007 | Lip loss: 0.0002101 | Grad norm: 0.318689 | Time: 7s84ms
Epoch: 015 | Test Loss: 0.0035534 | Time: 366ms
==> Save the model at epoch 015 with test loss 0.0035534
Epoch: 016 | Loss: 0.0037120 | L2 loss: 0.0035026 | Lip loss: 0.0002094 | Grad norm: 0.289093 | Time: 7s137ms
Epoch: 016 | Test Loss: 0.0035477 | Time: 357ms
==> Save the model at epoch 016 with test loss 0.0035477
Epoch: 017 | Loss: 0.0036880 | L2 loss: 0.0034787 | Lip loss: 0.0002093 | Grad norm: 0.284497 | Time: 7s362ms
Epoch: 017 | Test Loss: 0.0035458 | Time: 365ms
==> Save the model at epoch 017 with test loss 0.0035458
Epoch: 018 | Loss: 0.0036847 | L2 loss: 0.0034751 | Lip loss: 0.0002097 | Grad norm: 0.283325 | Time: 6s663ms
Epoch: 018 | Test Loss: 0.0035437 | Time: 383ms
==> Save the model at epoch 018 with test loss 0.0035437
Epoch: 019 | Loss: 0.0036919 | L2 loss: 0.0034833 | Lip loss: 0.0002085 | Grad norm: 0.291222 | Time: 6s800ms
Epoch: 019 | Test Loss: 0.0035400 | Time: 358ms
==> Save the model at epoch 019 with test loss 0.0035400
Epoch: 020 | Loss: 0.0037074 | L2 loss: 0.0034974 | Lip loss: 0.0002099 | Grad norm: 0.299608 | Time: 6s977ms
Epoch: 020 | Test Loss: 0.0035387 | Time: 384ms
==> Save the model at epoch 020 with test loss 0.0035387
Epoch: 021 | Loss: 0.0036950 | L2 loss: 0.0034850 | Lip loss: 0.0002100 | Grad norm: 0.309741 | Time: 6s749ms
Epoch: 021 | Test Loss: 0.0035383 | Time: 379ms
==> Save the model at epoch 021 with test loss 0.0035383
Epoch: 022 | Loss: 0.0036965 | L2 loss: 0.0034867 | Lip loss: 0.0002098 | Grad norm: 0.287040 | Time: 6s889ms
Epoch: 022 | Test Loss: 0.0035380 | Time: 361ms
==> Save the model at epoch 022 with test loss 0.0035380
Epoch: 023 | Loss: 0.0036729 | L2 loss: 0.0034626 | Lip loss: 0.0002103 | Grad norm: 0.279079 | Time: 6s905ms
Epoch: 023 | Test Loss: 0.0035378 | Time: 431ms
==> Save the model at epoch 023 with test loss 0.0035378
Epoch: 024 | Loss: 0.0036958 | L2 loss: 0.0034853 | Lip loss: 0.0002105 | Grad norm: 0.296998 | Time: 6s892ms
Epoch: 024 | Test Loss: 0.0035375 | Time: 363ms
==> Save the model at epoch 024 with test loss 0.0035375
Epoch: 025 | Loss: 0.0036856 | L2 loss: 0.0034760 | Lip loss: 0.0002096 | Grad norm: 0.291150 | Time: 6s807ms
Epoch: 025 | Test Loss: 0.0035372 | Time: 377ms
==> Save the model at epoch 025 with test loss 0.0035372
Epoch: 026 | Loss: 0.0036944 | L2 loss: 0.0034843 | Lip loss: 0.0002101 | Grad norm: 0.281684 | Time: 6s773ms
Epoch: 026 | Test Loss: 0.0035372 | Time: 379ms
==> Save the model at epoch 026 with test loss 0.0035372
Epoch: 027 | Loss: 0.0036800 | L2 loss: 0.0034705 | Lip loss: 0.0002094 | Grad norm: 0.281658 | Time: 6s859ms
Epoch: 027 | Test Loss: 0.0035372 | Time: 370ms
==> Save the model at epoch 027 with test loss 0.0035372
Epoch: 028 | Loss: 0.0036699 | L2 loss: 0.0034603 | Lip loss: 0.0002097 | Grad norm: 0.293730 | Time: 7s103ms
Epoch: 028 | Test Loss: 0.0035372 | Time: 366ms
==> Save the model at epoch 028 with test loss 0.0035372
Epoch: 029 | Loss: 0.0037013 | L2 loss: 0.0034911 | Lip loss: 0.0002102 | Grad norm: 0.311083 | Time: 7s86ms
Epoch: 029 | Test Loss: 0.0035372 | Time: 359ms
==> Save the model at epoch 029 with test loss 0.0035372
Epoch: 030 | Loss: 0.0036965 | L2 loss: 0.0034872 | Lip loss: 0.0002093 | Grad norm: 0.290558 | Time: 6s969ms
Epoch: 030 | Test Loss: 0.0035372 | Time: 427ms
==> Save the model at epoch 030 with test loss 0.0035372
Epoch: 031 | Loss: 0.0036904 | L2 loss: 0.0034805 | Lip loss: 0.0002100 | Grad norm: 0.305112 | Time: 7s308ms
Epoch: 031 | Test Loss: 0.0035372 | Time: 364ms
Epoch: 032 | Loss: 0.0037025 | L2 loss: 0.0034926 | Lip loss: 0.0002099 | Grad norm: 0.303515 | Time: 7s28ms
Epoch: 032 | Test Loss: 0.0035372 | Time: 361ms
Epoch: 033 | Loss: 0.0036855 | L2 loss: 0.0034751 | Lip loss: 0.0002104 | Grad norm: 0.280151 | Time: 7s25ms
Epoch: 033 | Test Loss: 0.0035372 | Time: 366ms
Epoch: 034 | Loss: 0.0036834 | L2 loss: 0.0034726 | Lip loss: 0.0002108 | Grad norm: 0.313764 | Time: 6s984ms
Epoch: 034 | Test Loss: 0.0035372 | Time: 368ms
==> Save the model at epoch 034 with test loss 0.0035372
Epoch: 035 | Loss: 0.0036917 | L2 loss: 0.0034816 | Lip loss: 0.0002101 | Grad norm: 0.284406 | Time: 7s227ms
Epoch: 035 | Test Loss: 0.0035372 | Time: 380ms
Epoch: 036 | Loss: 0.0036827 | L2 loss: 0.0034715 | Lip loss: 0.0002112 | Grad norm: 0.297178 | Time: 7s202ms
Epoch: 036 | Test Loss: 0.0035372 | Time: 375ms
Epoch: 037 | Loss: 0.0036808 | L2 loss: 0.0034716 | Lip loss: 0.0002092 | Grad norm: 0.290025 | Time: 7s416ms
Epoch: 037 | Test Loss: 0.0035372 | Time: 359ms
Epoch: 038 | Loss: 0.0037026 | L2 loss: 0.0034927 | Lip loss: 0.0002099 | Grad norm: 0.274060 | Time: 6s955ms
Epoch: 038 | Test Loss: 0.0035372 | Time: 363ms
Epoch: 039 | Loss: 0.0036850 | L2 loss: 0.0034751 | Lip loss: 0.0002099 | Grad norm: 0.307653 | Time: 6s875ms
Epoch: 039 | Test Loss: 0.0035372 | Time: 372ms
Epoch: 040 | Loss: 0.0036894 | L2 loss: 0.0034787 | Lip loss: 0.0002107 | Grad norm: 0.299475 | Time: 7s35ms
Epoch: 040 | Test Loss: 0.0035372 | Time: 365ms
Total time: 4m56s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
