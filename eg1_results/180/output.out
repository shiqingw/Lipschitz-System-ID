==> torch device:  cuda:3
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 20.5387401 | L2 loss: 20.5387401 | Lip loss: 0.0000000 | Grad norm: 1.420485 | Time: 11s195ms
Epoch: 001 | Test Loss: 19.5680102 | Time: 539ms
==> Save the model at epoch 001 with test loss 19.5680102
Epoch: 002 | Loss: 12.4615883 | L2 loss: 12.4615876 | Lip loss: 0.0000008 | Grad norm: 12.774284 | Time: 10s277ms
Epoch: 002 | Test Loss: 1.1284819 | Time: 538ms
==> Save the model at epoch 002 with test loss 1.1284819
Epoch: 003 | Loss: 0.1015131 | L2 loss: 0.1015108 | Lip loss: 0.0000023 | Grad norm: 2.474620 | Time: 10s170ms
Epoch: 003 | Test Loss: 0.0070908 | Time: 548ms
==> Save the model at epoch 003 with test loss 0.0070908
Epoch: 004 | Loss: 0.0055117 | L2 loss: 0.0055096 | Lip loss: 0.0000021 | Grad norm: 1.034283 | Time: 10s247ms
Epoch: 004 | Test Loss: 0.0056963 | Time: 607ms
==> Save the model at epoch 004 with test loss 0.0056963
Epoch: 005 | Loss: 0.0042352 | L2 loss: 0.0042331 | Lip loss: 0.0000021 | Grad norm: 0.766269 | Time: 10s245ms
Epoch: 005 | Test Loss: 0.0040671 | Time: 535ms
==> Save the model at epoch 005 with test loss 0.0040671
Epoch: 006 | Loss: 0.0037692 | L2 loss: 0.0037672 | Lip loss: 0.0000021 | Grad norm: 0.323380 | Time: 10s166ms
Epoch: 006 | Test Loss: 0.0037515 | Time: 530ms
==> Save the model at epoch 006 with test loss 0.0037515
Epoch: 007 | Loss: 0.0037366 | L2 loss: 0.0037345 | Lip loss: 0.0000021 | Grad norm: 0.323553 | Time: 10s299ms
Epoch: 007 | Test Loss: 0.0037313 | Time: 537ms
==> Save the model at epoch 007 with test loss 0.0037313
Epoch: 008 | Loss: 0.0037042 | L2 loss: 0.0037021 | Lip loss: 0.0000021 | Grad norm: 0.285102 | Time: 10s138ms
Epoch: 008 | Test Loss: 0.0037052 | Time: 549ms
==> Save the model at epoch 008 with test loss 0.0037052
Epoch: 009 | Loss: 0.0036793 | L2 loss: 0.0036772 | Lip loss: 0.0000021 | Grad norm: 0.303670 | Time: 10s569ms
Epoch: 009 | Test Loss: 0.0037097 | Time: 549ms
Epoch: 010 | Loss: 0.0036630 | L2 loss: 0.0036609 | Lip loss: 0.0000021 | Grad norm: 0.334041 | Time: 10s240ms
Epoch: 010 | Test Loss: 0.0037015 | Time: 528ms
==> Save the model at epoch 010 with test loss 0.0037015
Epoch: 011 | Loss: 0.0036204 | L2 loss: 0.0036184 | Lip loss: 0.0000021 | Grad norm: 0.249859 | Time: 10s383ms
Epoch: 011 | Test Loss: 0.0036361 | Time: 597ms
==> Save the model at epoch 011 with test loss 0.0036361
Epoch: 012 | Loss: 0.0036143 | L2 loss: 0.0036123 | Lip loss: 0.0000021 | Grad norm: 0.232995 | Time: 10s336ms
Epoch: 012 | Test Loss: 0.0036328 | Time: 534ms
==> Save the model at epoch 012 with test loss 0.0036328
Epoch: 013 | Loss: 0.0036123 | L2 loss: 0.0036102 | Lip loss: 0.0000021 | Grad norm: 0.234737 | Time: 10s258ms
Epoch: 013 | Test Loss: 0.0036307 | Time: 542ms
==> Save the model at epoch 013 with test loss 0.0036307
Epoch: 014 | Loss: 0.0036095 | L2 loss: 0.0036075 | Lip loss: 0.0000021 | Grad norm: 0.217707 | Time: 10s337ms
Epoch: 014 | Test Loss: 0.0036263 | Time: 535ms
==> Save the model at epoch 014 with test loss 0.0036263
Epoch: 015 | Loss: 0.0036096 | L2 loss: 0.0036075 | Lip loss: 0.0000021 | Grad norm: 0.240207 | Time: 10s541ms
Epoch: 015 | Test Loss: 0.0036301 | Time: 544ms
Epoch: 016 | Loss: 0.0035998 | L2 loss: 0.0035977 | Lip loss: 0.0000021 | Grad norm: 0.233895 | Time: 10s287ms
Epoch: 016 | Test Loss: 0.0036225 | Time: 548ms
==> Save the model at epoch 016 with test loss 0.0036225
Epoch: 017 | Loss: 0.0035992 | L2 loss: 0.0035971 | Lip loss: 0.0000021 | Grad norm: 0.238396 | Time: 10s185ms
Epoch: 017 | Test Loss: 0.0036223 | Time: 534ms
==> Save the model at epoch 017 with test loss 0.0036223
Epoch: 018 | Loss: 0.0035997 | L2 loss: 0.0035976 | Lip loss: 0.0000021 | Grad norm: 0.211095 | Time: 10s125ms
Epoch: 018 | Test Loss: 0.0036218 | Time: 536ms
==> Save the model at epoch 018 with test loss 0.0036218
Epoch: 019 | Loss: 0.0036000 | L2 loss: 0.0035980 | Lip loss: 0.0000021 | Grad norm: 0.220560 | Time: 10s146ms
Epoch: 019 | Test Loss: 0.0036213 | Time: 544ms
==> Save the model at epoch 019 with test loss 0.0036213
Epoch: 020 | Loss: 0.0036014 | L2 loss: 0.0035993 | Lip loss: 0.0000021 | Grad norm: 0.235860 | Time: 10s422ms
Epoch: 020 | Test Loss: 0.0036217 | Time: 609ms
Epoch: 021 | Loss: 0.0036012 | L2 loss: 0.0035991 | Lip loss: 0.0000021 | Grad norm: 0.223210 | Time: 10s143ms
Epoch: 021 | Test Loss: 0.0036216 | Time: 546ms
Epoch: 022 | Loss: 0.0036004 | L2 loss: 0.0035983 | Lip loss: 0.0000021 | Grad norm: 0.236596 | Time: 10s399ms
Epoch: 022 | Test Loss: 0.0036214 | Time: 541ms
Epoch: 023 | Loss: 0.0036006 | L2 loss: 0.0035986 | Lip loss: 0.0000021 | Grad norm: 0.226977 | Time: 10s76ms
Epoch: 023 | Test Loss: 0.0036214 | Time: 539ms
Epoch: 024 | Loss: 0.0035991 | L2 loss: 0.0035970 | Lip loss: 0.0000021 | Grad norm: 0.222680 | Time: 10s127ms
Epoch: 024 | Test Loss: 0.0036213 | Time: 553ms
==> Save the model at epoch 024 with test loss 0.0036213
Epoch: 025 | Loss: 0.0036013 | L2 loss: 0.0035992 | Lip loss: 0.0000021 | Grad norm: 0.229116 | Time: 10s13ms
Epoch: 025 | Test Loss: 0.0036212 | Time: 537ms
==> Save the model at epoch 025 with test loss 0.0036212
Epoch: 026 | Loss: 0.0035985 | L2 loss: 0.0035965 | Lip loss: 0.0000021 | Grad norm: 0.220464 | Time: 10s8ms
Epoch: 026 | Test Loss: 0.0036212 | Time: 549ms
==> Save the model at epoch 026 with test loss 0.0036212
Epoch: 027 | Loss: 0.0035983 | L2 loss: 0.0035962 | Lip loss: 0.0000021 | Grad norm: 0.219318 | Time: 10s
Epoch: 027 | Test Loss: 0.0036212 | Time: 541ms
Epoch: 028 | Loss: 0.0035969 | L2 loss: 0.0035948 | Lip loss: 0.0000021 | Grad norm: 0.212057 | Time: 10s109ms
Epoch: 028 | Test Loss: 0.0036212 | Time: 549ms
==> Save the model at epoch 028 with test loss 0.0036212
Epoch: 029 | Loss: 0.0035996 | L2 loss: 0.0035975 | Lip loss: 0.0000021 | Grad norm: 0.217493 | Time: 10s101ms
Epoch: 029 | Test Loss: 0.0036212 | Time: 603ms
==> Save the model at epoch 029 with test loss 0.0036212
Epoch: 030 | Loss: 0.0035982 | L2 loss: 0.0035962 | Lip loss: 0.0000021 | Grad norm: 0.232341 | Time: 9s974ms
Epoch: 030 | Test Loss: 0.0036212 | Time: 534ms
==> Save the model at epoch 030 with test loss 0.0036212
Epoch: 031 | Loss: 0.0035990 | L2 loss: 0.0035970 | Lip loss: 0.0000021 | Grad norm: 0.217692 | Time: 10s202ms
Epoch: 031 | Test Loss: 0.0036212 | Time: 547ms
Epoch: 032 | Loss: 0.0035983 | L2 loss: 0.0035963 | Lip loss: 0.0000021 | Grad norm: 0.218033 | Time: 10s254ms
Epoch: 032 | Test Loss: 0.0036212 | Time: 535ms
Epoch: 033 | Loss: 0.0036000 | L2 loss: 0.0035980 | Lip loss: 0.0000021 | Grad norm: 0.207564 | Time: 9s992ms
Epoch: 033 | Test Loss: 0.0036212 | Time: 530ms
Epoch: 034 | Loss: 0.0036006 | L2 loss: 0.0035986 | Lip loss: 0.0000021 | Grad norm: 0.214615 | Time: 10s606ms
Epoch: 034 | Test Loss: 0.0036212 | Time: 538ms
Epoch: 035 | Loss: 0.0035998 | L2 loss: 0.0035978 | Lip loss: 0.0000021 | Grad norm: 0.217773 | Time: 10s289ms
Epoch: 035 | Test Loss: 0.0036212 | Time: 551ms
Epoch: 036 | Loss: 0.0035990 | L2 loss: 0.0035969 | Lip loss: 0.0000021 | Grad norm: 0.210476 | Time: 10s232ms
Epoch: 036 | Test Loss: 0.0036212 | Time: 539ms
Epoch: 037 | Loss: 0.0035978 | L2 loss: 0.0035958 | Lip loss: 0.0000021 | Grad norm: 0.217122 | Time: 10s272ms
Epoch: 037 | Test Loss: 0.0036212 | Time: 537ms
Epoch: 038 | Loss: 0.0036001 | L2 loss: 0.0035980 | Lip loss: 0.0000021 | Grad norm: 0.214437 | Time: 10s346ms
Epoch: 038 | Test Loss: 0.0036212 | Time: 599ms
Epoch: 039 | Loss: 0.0035963 | L2 loss: 0.0035942 | Lip loss: 0.0000021 | Grad norm: 0.224946 | Time: 10s340ms
Epoch: 039 | Test Loss: 0.0036212 | Time: 535ms
Epoch: 040 | Loss: 0.0035987 | L2 loss: 0.0035967 | Lip loss: 0.0000021 | Grad norm: 0.229326 | Time: 10s123ms
Epoch: 040 | Test Loss: 0.0036212 | Time: 537ms
Total time: 7m12s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
