==> torch device:  cuda:1
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (M): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 0.25
==> Start training...
Epoch: 001 | Loss: 10.2300539 | L2 loss: 10.2300538 | Lip loss: 0.0000002 | Grad norm: 22.650974 | Time: 12s443ms
Epoch: 001 | Test Loss: 0.1294170 | Time: 599ms
==> Save the model at epoch 001 with test loss 0.1294170
Epoch: 002 | Loss: 0.1300356 | L2 loss: 0.1300354 | Lip loss: 0.0000002 | Grad norm: 20.284702 | Time: 12s151ms
Epoch: 002 | Test Loss: 0.2526280 | Time: 599ms
Epoch: 003 | Loss: 0.0653838 | L2 loss: 0.0653836 | Lip loss: 0.0000002 | Grad norm: 12.302292 | Time: 12s726ms
Epoch: 003 | Test Loss: 0.0189409 | Time: 592ms
==> Save the model at epoch 003 with test loss 0.0189409
Epoch: 004 | Loss: 0.0344931 | L2 loss: 0.0344928 | Lip loss: 0.0000002 | Grad norm: 7.408650 | Time: 12s645ms
Epoch: 004 | Test Loss: 0.0583046 | Time: 593ms
Epoch: 005 | Loss: 0.0532099 | L2 loss: 0.0532097 | Lip loss: 0.0000002 | Grad norm: 8.320404 | Time: 12s655ms
Epoch: 005 | Test Loss: 0.0663459 | Time: 594ms
Epoch: 006 | Loss: 0.0119133 | L2 loss: 0.0119131 | Lip loss: 0.0000002 | Grad norm: 1.675346 | Time: 12s705ms
Epoch: 006 | Test Loss: 0.0085256 | Time: 597ms
==> Save the model at epoch 006 with test loss 0.0085256
Epoch: 007 | Loss: 0.0083069 | L2 loss: 0.0083067 | Lip loss: 0.0000002 | Grad norm: 0.697819 | Time: 12s972ms
Epoch: 007 | Test Loss: 0.0085699 | Time: 597ms
Epoch: 008 | Loss: 0.0082869 | L2 loss: 0.0082867 | Lip loss: 0.0000002 | Grad norm: 0.840479 | Time: 12s830ms
Epoch: 008 | Test Loss: 0.0083860 | Time: 651ms
==> Save the model at epoch 008 with test loss 0.0083860
Epoch: 009 | Loss: 0.0082141 | L2 loss: 0.0082139 | Lip loss: 0.0000002 | Grad norm: 0.855962 | Time: 12s780ms
Epoch: 009 | Test Loss: 0.0084599 | Time: 595ms
Epoch: 010 | Loss: 0.0081232 | L2 loss: 0.0081230 | Lip loss: 0.0000002 | Grad norm: 0.811055 | Time: 12s955ms
Epoch: 010 | Test Loss: 0.0081230 | Time: 601ms
==> Save the model at epoch 010 with test loss 0.0081230
Epoch: 011 | Loss: 0.0076884 | L2 loss: 0.0076882 | Lip loss: 0.0000002 | Grad norm: 0.404609 | Time: 12s372ms
Epoch: 011 | Test Loss: 0.0078921 | Time: 589ms
==> Save the model at epoch 011 with test loss 0.0078921
Epoch: 012 | Loss: 0.0076477 | L2 loss: 0.0076475 | Lip loss: 0.0000002 | Grad norm: 0.357686 | Time: 12s387ms
Epoch: 012 | Test Loss: 0.0079265 | Time: 592ms
Epoch: 013 | Loss: 0.0076226 | L2 loss: 0.0076224 | Lip loss: 0.0000002 | Grad norm: 0.344643 | Time: 12s749ms
Epoch: 013 | Test Loss: 0.0078960 | Time: 598ms
Epoch: 014 | Loss: 0.0076467 | L2 loss: 0.0076465 | Lip loss: 0.0000002 | Grad norm: 0.395098 | Time: 12s786ms
Epoch: 014 | Test Loss: 0.0078638 | Time: 597ms
==> Save the model at epoch 014 with test loss 0.0078638
Epoch: 015 | Loss: 0.0076217 | L2 loss: 0.0076215 | Lip loss: 0.0000002 | Grad norm: 0.377333 | Time: 12s714ms
Epoch: 015 | Test Loss: 0.0078589 | Time: 596ms
==> Save the model at epoch 015 with test loss 0.0078589
Epoch: 016 | Loss: 0.0075838 | L2 loss: 0.0075836 | Lip loss: 0.0000002 | Grad norm: 0.351203 | Time: 12s787ms
Epoch: 016 | Test Loss: 0.0078433 | Time: 587ms
==> Save the model at epoch 016 with test loss 0.0078433
Epoch: 017 | Loss: 0.0075820 | L2 loss: 0.0075818 | Lip loss: 0.0000002 | Grad norm: 0.301947 | Time: 11s859ms
Epoch: 017 | Test Loss: 0.0078395 | Time: 586ms
==> Save the model at epoch 017 with test loss 0.0078395
Epoch: 018 | Loss: 0.0075818 | L2 loss: 0.0075816 | Lip loss: 0.0000002 | Grad norm: 0.326049 | Time: 11s830ms
Epoch: 018 | Test Loss: 0.0078404 | Time: 589ms
Epoch: 019 | Loss: 0.0075875 | L2 loss: 0.0075873 | Lip loss: 0.0000002 | Grad norm: 0.354859 | Time: 11s832ms
Epoch: 019 | Test Loss: 0.0078384 | Time: 594ms
==> Save the model at epoch 019 with test loss 0.0078384
Epoch: 020 | Loss: 0.0075832 | L2 loss: 0.0075830 | Lip loss: 0.0000002 | Grad norm: 0.318669 | Time: 11s886ms
Epoch: 020 | Test Loss: 0.0078390 | Time: 588ms
Epoch: 021 | Loss: 0.0075802 | L2 loss: 0.0075800 | Lip loss: 0.0000002 | Grad norm: 0.322517 | Time: 11s821ms
Epoch: 021 | Test Loss: 0.0078375 | Time: 583ms
==> Save the model at epoch 021 with test loss 0.0078375
Epoch: 022 | Loss: 0.0075726 | L2 loss: 0.0075724 | Lip loss: 0.0000002 | Grad norm: 0.308899 | Time: 11s826ms
Epoch: 022 | Test Loss: 0.0078369 | Time: 650ms
==> Save the model at epoch 022 with test loss 0.0078369
Epoch: 023 | Loss: 0.0075764 | L2 loss: 0.0075762 | Lip loss: 0.0000002 | Grad norm: 0.330518 | Time: 11s830ms
Epoch: 023 | Test Loss: 0.0078366 | Time: 589ms
==> Save the model at epoch 023 with test loss 0.0078366
Epoch: 024 | Loss: 0.0075742 | L2 loss: 0.0075740 | Lip loss: 0.0000002 | Grad norm: 0.309449 | Time: 11s853ms
Epoch: 024 | Test Loss: 0.0078367 | Time: 598ms
Epoch: 025 | Loss: 0.0075746 | L2 loss: 0.0075744 | Lip loss: 0.0000002 | Grad norm: 0.314046 | Time: 12s710ms
Epoch: 025 | Test Loss: 0.0078364 | Time: 589ms
==> Save the model at epoch 025 with test loss 0.0078364
Epoch: 026 | Loss: 0.0075753 | L2 loss: 0.0075751 | Lip loss: 0.0000002 | Grad norm: 0.320754 | Time: 13s74ms
Epoch: 026 | Test Loss: 0.0078364 | Time: 599ms
==> Save the model at epoch 026 with test loss 0.0078364
Epoch: 027 | Loss: 0.0075726 | L2 loss: 0.0075724 | Lip loss: 0.0000002 | Grad norm: 0.328297 | Time: 13s220ms
Epoch: 027 | Test Loss: 0.0078364 | Time: 589ms
Epoch: 028 | Loss: 0.0075739 | L2 loss: 0.0075737 | Lip loss: 0.0000002 | Grad norm: 0.285432 | Time: 13s287ms
Epoch: 028 | Test Loss: 0.0078364 | Time: 594ms
Epoch: 029 | Loss: 0.0075752 | L2 loss: 0.0075750 | Lip loss: 0.0000002 | Grad norm: 0.314289 | Time: 13s75ms
Epoch: 029 | Test Loss: 0.0078364 | Time: 601ms
Epoch: 030 | Loss: 0.0075874 | L2 loss: 0.0075872 | Lip loss: 0.0000002 | Grad norm: 0.322658 | Time: 12s717ms
Epoch: 030 | Test Loss: 0.0078364 | Time: 652ms
==> Save the model at epoch 030 with test loss 0.0078364
Epoch: 031 | Loss: 0.0075849 | L2 loss: 0.0075847 | Lip loss: 0.0000002 | Grad norm: 0.323669 | Time: 12s704ms
Epoch: 031 | Test Loss: 0.0078364 | Time: 597ms
==> Save the model at epoch 031 with test loss 0.0078364
Epoch: 032 | Loss: 0.0075713 | L2 loss: 0.0075711 | Lip loss: 0.0000002 | Grad norm: 0.306368 | Time: 12s715ms
Epoch: 032 | Test Loss: 0.0078364 | Time: 599ms
Epoch: 033 | Loss: 0.0075760 | L2 loss: 0.0075758 | Lip loss: 0.0000002 | Grad norm: 0.304871 | Time: 13s363ms
Epoch: 033 | Test Loss: 0.0078364 | Time: 602ms
Epoch: 034 | Loss: 0.0075773 | L2 loss: 0.0075771 | Lip loss: 0.0000002 | Grad norm: 0.326910 | Time: 13s396ms
Epoch: 034 | Test Loss: 0.0078364 | Time: 624ms
Epoch: 035 | Loss: 0.0075736 | L2 loss: 0.0075734 | Lip loss: 0.0000002 | Grad norm: 0.298876 | Time: 12s145ms
Epoch: 035 | Test Loss: 0.0078364 | Time: 656ms
Epoch: 036 | Loss: 0.0075805 | L2 loss: 0.0075803 | Lip loss: 0.0000002 | Grad norm: 0.309804 | Time: 12s441ms
Epoch: 036 | Test Loss: 0.0078364 | Time: 596ms
Epoch: 037 | Loss: 0.0075824 | L2 loss: 0.0075822 | Lip loss: 0.0000002 | Grad norm: 0.333914 | Time: 12s877ms
Epoch: 037 | Test Loss: 0.0078364 | Time: 599ms
Epoch: 038 | Loss: 0.0075739 | L2 loss: 0.0075737 | Lip loss: 0.0000002 | Grad norm: 0.301705 | Time: 13s72ms
Epoch: 038 | Test Loss: 0.0078364 | Time: 668ms
Epoch: 039 | Loss: 0.0075756 | L2 loss: 0.0075754 | Lip loss: 0.0000002 | Grad norm: 0.308644 | Time: 12s185ms
Epoch: 039 | Test Loss: 0.0078364 | Time: 590ms
Epoch: 040 | Loss: 0.0075738 | L2 loss: 0.0075736 | Lip loss: 0.0000002 | Grad norm: 0.324324 | Time: 11s856ms
Epoch: 040 | Test Loss: 0.0078364 | Time: 595ms
Total time: 8m46s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
