==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 18.9223705 | L2 loss: 18.9214915 | Lip loss: 0.0008791 | Grad norm: 0.864542 | Time: 7s377ms
Epoch: 001 | Test Loss: 18.9495443 | Time: 366ms
==> Save the model at epoch 001 with test loss 18.9495443
Epoch: 002 | Loss: 17.5705332 | L2 loss: 17.5530167 | Lip loss: 0.0175164 | Grad norm: 4.269938 | Time: 6s897ms
Epoch: 002 | Test Loss: 15.2012126 | Time: 391ms
==> Save the model at epoch 002 with test loss 15.2012126
Epoch: 003 | Loss: 9.8687932 | L2 loss: 9.7345432 | Lip loss: 0.1342500 | Grad norm: 12.160424 | Time: 7s10ms
Epoch: 003 | Test Loss: 3.5878485 | Time: 377ms
==> Save the model at epoch 003 with test loss 3.5878485
Epoch: 004 | Loss: 0.9467112 | L2 loss: 0.7120441 | Lip loss: 0.2346671 | Grad norm: 5.678979 | Time: 6s902ms
Epoch: 004 | Test Loss: 0.0192287 | Time: 376ms
==> Save the model at epoch 004 with test loss 0.0192287
Epoch: 005 | Loss: 0.2264948 | L2 loss: 0.0146728 | Lip loss: 0.2118221 | Grad norm: 2.791240 | Time: 7s70ms
Epoch: 005 | Test Loss: 0.0097107 | Time: 385ms
==> Save the model at epoch 005 with test loss 0.0097107
Epoch: 006 | Loss: 0.2168660 | L2 loss: 0.0083241 | Lip loss: 0.2085420 | Grad norm: 1.221053 | Time: 7s211ms
Epoch: 006 | Test Loss: 0.0082166 | Time: 441ms
==> Save the model at epoch 006 with test loss 0.0082166
Epoch: 007 | Loss: 0.2154710 | L2 loss: 0.0078554 | Lip loss: 0.2076156 | Grad norm: 1.156989 | Time: 6s928ms
Epoch: 007 | Test Loss: 0.0076688 | Time: 372ms
==> Save the model at epoch 007 with test loss 0.0076688
Epoch: 008 | Loss: 0.2152439 | L2 loss: 0.0075263 | Lip loss: 0.2077176 | Grad norm: 1.172496 | Time: 6s920ms
Epoch: 008 | Test Loss: 0.0077678 | Time: 373ms
Epoch: 009 | Loss: 0.2152823 | L2 loss: 0.0073272 | Lip loss: 0.2079551 | Grad norm: 1.153708 | Time: 7s87ms
Epoch: 009 | Test Loss: 0.0071737 | Time: 377ms
==> Save the model at epoch 009 with test loss 0.0071737
Epoch: 010 | Loss: 0.2150728 | L2 loss: 0.0071088 | Lip loss: 0.2079639 | Grad norm: 1.251281 | Time: 6s936ms
Epoch: 010 | Test Loss: 0.0071845 | Time: 377ms
Epoch: 011 | Loss: 0.2147002 | L2 loss: 0.0069789 | Lip loss: 0.2077213 | Grad norm: 0.945603 | Time: 6s936ms
Epoch: 011 | Test Loss: 0.0070043 | Time: 376ms
==> Save the model at epoch 011 with test loss 0.0070043
Epoch: 012 | Loss: 0.2146669 | L2 loss: 0.0068811 | Lip loss: 0.2077858 | Grad norm: 1.017182 | Time: 6s939ms
Epoch: 012 | Test Loss: 0.0069463 | Time: 373ms
==> Save the model at epoch 012 with test loss 0.0069463
Epoch: 013 | Loss: 0.2134269 | L2 loss: 0.0068100 | Lip loss: 0.2066169 | Grad norm: 0.951671 | Time: 7s18ms
Epoch: 013 | Test Loss: 0.0068887 | Time: 456ms
==> Save the model at epoch 013 with test loss 0.0068887
Epoch: 014 | Loss: 0.2138352 | L2 loss: 0.0067980 | Lip loss: 0.2070372 | Grad norm: 1.037625 | Time: 6s895ms
Epoch: 014 | Test Loss: 0.0068997 | Time: 379ms
Epoch: 015 | Loss: 0.2143969 | L2 loss: 0.0067886 | Lip loss: 0.2076083 | Grad norm: 0.970433 | Time: 6s917ms
Epoch: 015 | Test Loss: 0.0068546 | Time: 375ms
==> Save the model at epoch 015 with test loss 0.0068546
Epoch: 016 | Loss: 0.2136810 | L2 loss: 0.0067680 | Lip loss: 0.2069130 | Grad norm: 0.979234 | Time: 6s903ms
Epoch: 016 | Test Loss: 0.0068321 | Time: 379ms
==> Save the model at epoch 016 with test loss 0.0068321
Epoch: 017 | Loss: 0.2147289 | L2 loss: 0.0067141 | Lip loss: 0.2080148 | Grad norm: 0.977065 | Time: 6s991ms
Epoch: 017 | Test Loss: 0.0068305 | Time: 389ms
==> Save the model at epoch 017 with test loss 0.0068305
Epoch: 018 | Loss: 0.2138595 | L2 loss: 0.0067346 | Lip loss: 0.2071248 | Grad norm: 0.909107 | Time: 6s740ms
Epoch: 018 | Test Loss: 0.0068410 | Time: 387ms
Epoch: 019 | Loss: 0.2145383 | L2 loss: 0.0067790 | Lip loss: 0.2077593 | Grad norm: 0.954688 | Time: 7s191ms
Epoch: 019 | Test Loss: 0.0068428 | Time: 376ms
Epoch: 020 | Loss: 0.2145410 | L2 loss: 0.0067616 | Lip loss: 0.2077794 | Grad norm: 1.041342 | Time: 6s982ms
Epoch: 020 | Test Loss: 0.0068399 | Time: 374ms
Epoch: 021 | Loss: 0.2141128 | L2 loss: 0.0067573 | Lip loss: 0.2073555 | Grad norm: 1.052315 | Time: 6s912ms
Epoch: 021 | Test Loss: 0.0068391 | Time: 392ms
Epoch: 022 | Loss: 0.2146234 | L2 loss: 0.0067434 | Lip loss: 0.2078800 | Grad norm: 0.984272 | Time: 6s987ms
Epoch: 022 | Test Loss: 0.0068382 | Time: 375ms
Epoch: 023 | Loss: 0.2136943 | L2 loss: 0.0067230 | Lip loss: 0.2069713 | Grad norm: 0.910924 | Time: 6s804ms
Epoch: 023 | Test Loss: 0.0068372 | Time: 454ms
Epoch: 024 | Loss: 0.2136619 | L2 loss: 0.0067362 | Lip loss: 0.2069257 | Grad norm: 1.010767 | Time: 6s935ms
Epoch: 024 | Test Loss: 0.0068362 | Time: 381ms
Epoch: 025 | Loss: 0.2141335 | L2 loss: 0.0067371 | Lip loss: 0.2073965 | Grad norm: 0.991299 | Time: 6s964ms
Epoch: 025 | Test Loss: 0.0068358 | Time: 386ms
Epoch: 026 | Loss: 0.2146160 | L2 loss: 0.0067624 | Lip loss: 0.2078536 | Grad norm: 0.983819 | Time: 6s767ms
Epoch: 026 | Test Loss: 0.0068358 | Time: 377ms
Epoch: 027 | Loss: 0.2147759 | L2 loss: 0.0067418 | Lip loss: 0.2080342 | Grad norm: 0.982953 | Time: 6s958ms
Epoch: 027 | Test Loss: 0.0068358 | Time: 383ms
Epoch: 028 | Loss: 0.2135320 | L2 loss: 0.0067203 | Lip loss: 0.2068117 | Grad norm: 0.970437 | Time: 6s742ms
Epoch: 028 | Test Loss: 0.0068358 | Time: 387ms
Epoch: 029 | Loss: 0.2142112 | L2 loss: 0.0067622 | Lip loss: 0.2074490 | Grad norm: 0.998201 | Time: 6s533ms
Epoch: 029 | Test Loss: 0.0068358 | Time: 378ms
Epoch: 030 | Loss: 0.2135688 | L2 loss: 0.0067662 | Lip loss: 0.2068026 | Grad norm: 0.985773 | Time: 6s929ms
Epoch: 030 | Test Loss: 0.0068358 | Time: 446ms
Epoch: 031 | Loss: 0.2157641 | L2 loss: 0.0067406 | Lip loss: 0.2090235 | Grad norm: 1.081413 | Time: 6s933ms
Epoch: 031 | Test Loss: 0.0068358 | Time: 375ms
Epoch: 032 | Loss: 0.2142902 | L2 loss: 0.0067831 | Lip loss: 0.2075071 | Grad norm: 1.027345 | Time: 6s856ms
Epoch: 032 | Test Loss: 0.0068358 | Time: 380ms
Epoch: 033 | Loss: 0.2142834 | L2 loss: 0.0067355 | Lip loss: 0.2075479 | Grad norm: 0.980506 | Time: 6s765ms
Epoch: 033 | Test Loss: 0.0068358 | Time: 380ms
Epoch: 034 | Loss: 0.2140463 | L2 loss: 0.0067454 | Lip loss: 0.2073009 | Grad norm: 0.994665 | Time: 7s116ms
Epoch: 034 | Test Loss: 0.0068358 | Time: 400ms
Epoch: 035 | Loss: 0.2141256 | L2 loss: 0.0067444 | Lip loss: 0.2073812 | Grad norm: 0.991567 | Time: 7s277ms
Epoch: 035 | Test Loss: 0.0068358 | Time: 377ms
Epoch: 036 | Loss: 0.2144932 | L2 loss: 0.0067515 | Lip loss: 0.2077417 | Grad norm: 0.990303 | Time: 6s904ms
Epoch: 036 | Test Loss: 0.0068358 | Time: 376ms
Epoch: 037 | Loss: 0.2141333 | L2 loss: 0.0067379 | Lip loss: 0.2073954 | Grad norm: 0.968709 | Time: 7s375ms
Epoch: 037 | Test Loss: 0.0068358 | Time: 390ms
Epoch: 038 | Loss: 0.2142912 | L2 loss: 0.0067778 | Lip loss: 0.2075134 | Grad norm: 0.938843 | Time: 6s959ms
Epoch: 038 | Test Loss: 0.0068358 | Time: 377ms
Epoch: 039 | Loss: 0.2136690 | L2 loss: 0.0067631 | Lip loss: 0.2069059 | Grad norm: 0.948516 | Time: 7s245ms
Epoch: 039 | Test Loss: 0.0068358 | Time: 386ms
Epoch: 040 | Loss: 0.2136145 | L2 loss: 0.0067426 | Lip loss: 0.2068720 | Grad norm: 0.978373 | Time: 7s74ms
Epoch: 040 | Test Loss: 0.0068358 | Time: 404ms
Total time: 4m54s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
