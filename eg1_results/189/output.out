==> torch device:  cuda:0
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[1. 1.]
==> Ouput transform to be applied to the neural network:
[1. 1.]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─LeakyReLU: 2-2                    [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─LeakyReLU: 2-4                    [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─LeakyReLU: 2-6                    [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─LeakyReLU: 2-8                    [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─LeakyReLU: 2-10                   [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─LeakyReLU: 2-12                   [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─LeakyReLU: 2-14                   [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.03
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.10
Estimated Total Size (MB): 0.10
==========================================================================================
==> Saving initial model weights...
==> Start training...
Epoch: 001 | Loss: 18.8576709 | L2 loss: 18.8576551 | Lip loss: 0.0000157 | Grad norm: 0.931223 | Time: 7s3ms
Epoch: 001 | Test Loss: 18.5681320 | Time: 369ms
==> Save the model at epoch 001 with test loss 18.5681320
Epoch: 002 | Loss: 17.5019067 | L2 loss: 17.5016636 | Lip loss: 0.0002431 | Grad norm: 4.070120 | Time: 6s826ms
Epoch: 002 | Test Loss: 15.4486694 | Time: 377ms
==> Save the model at epoch 002 with test loss 15.4486694
Epoch: 003 | Loss: 10.9475076 | L2 loss: 10.9460016 | Lip loss: 0.0015060 | Grad norm: 10.999326 | Time: 6s885ms
Epoch: 003 | Test Loss: 6.3067439 | Time: 374ms
==> Save the model at epoch 003 with test loss 6.3067439
Epoch: 004 | Loss: 2.6236435 | L2 loss: 2.6211619 | Lip loss: 0.0024816 | Grad norm: 7.822019 | Time: 6s957ms
Epoch: 004 | Test Loss: 0.0847696 | Time: 371ms
==> Save the model at epoch 004 with test loss 0.0847696
Epoch: 005 | Loss: 0.0356393 | L2 loss: 0.0333302 | Lip loss: 0.0023091 | Grad norm: 2.175979 | Time: 6s964ms
Epoch: 005 | Test Loss: 0.0164733 | Time: 443ms
==> Save the model at epoch 005 with test loss 0.0164733
Epoch: 006 | Loss: 0.0162114 | L2 loss: 0.0140022 | Lip loss: 0.0022092 | Grad norm: 0.872457 | Time: 7s244ms
Epoch: 006 | Test Loss: 0.0132456 | Time: 452ms
==> Save the model at epoch 006 with test loss 0.0132456
Epoch: 007 | Loss: 0.0146953 | L2 loss: 0.0124937 | Lip loss: 0.0022017 | Grad norm: 0.612656 | Time: 7s264ms
Epoch: 007 | Test Loss: 0.0120714 | Time: 391ms
==> Save the model at epoch 007 with test loss 0.0120714
Epoch: 008 | Loss: 0.0135994 | L2 loss: 0.0114168 | Lip loss: 0.0021827 | Grad norm: 0.535162 | Time: 7s56ms
Epoch: 008 | Test Loss: 0.0110151 | Time: 378ms
==> Save the model at epoch 008 with test loss 0.0110151
Epoch: 009 | Loss: 0.0127507 | L2 loss: 0.0105793 | Lip loss: 0.0021713 | Grad norm: 0.642950 | Time: 6s924ms
Epoch: 009 | Test Loss: 0.0102155 | Time: 377ms
==> Save the model at epoch 009 with test loss 0.0102155
Epoch: 010 | Loss: 0.0119547 | L2 loss: 0.0097865 | Lip loss: 0.0021683 | Grad norm: 0.547313 | Time: 7s122ms
Epoch: 010 | Test Loss: 0.0095302 | Time: 374ms
==> Save the model at epoch 010 with test loss 0.0095302
Epoch: 011 | Loss: 0.0115766 | L2 loss: 0.0094023 | Lip loss: 0.0021743 | Grad norm: 0.436153 | Time: 7s22ms
Epoch: 011 | Test Loss: 0.0094443 | Time: 376ms
==> Save the model at epoch 011 with test loss 0.0094443
Epoch: 012 | Loss: 0.0114813 | L2 loss: 0.0093251 | Lip loss: 0.0021562 | Grad norm: 0.417070 | Time: 6s692ms
Epoch: 012 | Test Loss: 0.0093728 | Time: 375ms
==> Save the model at epoch 012 with test loss 0.0093728
Epoch: 013 | Loss: 0.0114313 | L2 loss: 0.0092591 | Lip loss: 0.0021722 | Grad norm: 0.389941 | Time: 6s905ms
Epoch: 013 | Test Loss: 0.0093043 | Time: 449ms
==> Save the model at epoch 013 with test loss 0.0093043
Epoch: 014 | Loss: 0.0113769 | L2 loss: 0.0092159 | Lip loss: 0.0021610 | Grad norm: 0.413793 | Time: 6s944ms
Epoch: 014 | Test Loss: 0.0092481 | Time: 382ms
==> Save the model at epoch 014 with test loss 0.0092481
Epoch: 015 | Loss: 0.0112842 | L2 loss: 0.0091260 | Lip loss: 0.0021581 | Grad norm: 0.444741 | Time: 6s984ms
Epoch: 015 | Test Loss: 0.0091960 | Time: 387ms
==> Save the model at epoch 015 with test loss 0.0091960
Epoch: 016 | Loss: 0.0112453 | L2 loss: 0.0090640 | Lip loss: 0.0021813 | Grad norm: 0.378629 | Time: 6s992ms
Epoch: 016 | Test Loss: 0.0091705 | Time: 370ms
==> Save the model at epoch 016 with test loss 0.0091705
Epoch: 017 | Loss: 0.0112576 | L2 loss: 0.0090652 | Lip loss: 0.0021924 | Grad norm: 0.408050 | Time: 6s960ms
Epoch: 017 | Test Loss: 0.0091640 | Time: 370ms
==> Save the model at epoch 017 with test loss 0.0091640
Epoch: 018 | Loss: 0.0112489 | L2 loss: 0.0090820 | Lip loss: 0.0021669 | Grad norm: 0.372396 | Time: 6s841ms
Epoch: 018 | Test Loss: 0.0091568 | Time: 369ms
==> Save the model at epoch 018 with test loss 0.0091568
Epoch: 019 | Loss: 0.0112538 | L2 loss: 0.0090683 | Lip loss: 0.0021855 | Grad norm: 0.403393 | Time: 6s903ms
Epoch: 019 | Test Loss: 0.0091501 | Time: 372ms
==> Save the model at epoch 019 with test loss 0.0091501
Epoch: 020 | Loss: 0.0112398 | L2 loss: 0.0090834 | Lip loss: 0.0021564 | Grad norm: 0.454631 | Time: 6s956ms
Epoch: 020 | Test Loss: 0.0091433 | Time: 381ms
==> Save the model at epoch 020 with test loss 0.0091433
Epoch: 021 | Loss: 0.0111943 | L2 loss: 0.0090283 | Lip loss: 0.0021660 | Grad norm: 0.387993 | Time: 6s861ms
Epoch: 021 | Test Loss: 0.0091427 | Time: 372ms
==> Save the model at epoch 021 with test loss 0.0091427
Epoch: 022 | Loss: 0.0112077 | L2 loss: 0.0090458 | Lip loss: 0.0021619 | Grad norm: 0.408541 | Time: 6s911ms
Epoch: 022 | Test Loss: 0.0091422 | Time: 377ms
==> Save the model at epoch 022 with test loss 0.0091422
Epoch: 023 | Loss: 0.0112158 | L2 loss: 0.0090548 | Lip loss: 0.0021610 | Grad norm: 0.337213 | Time: 6s874ms
Epoch: 023 | Test Loss: 0.0091416 | Time: 444ms
==> Save the model at epoch 023 with test loss 0.0091416
Epoch: 024 | Loss: 0.0112282 | L2 loss: 0.0090602 | Lip loss: 0.0021680 | Grad norm: 0.384038 | Time: 6s947ms
Epoch: 024 | Test Loss: 0.0091410 | Time: 381ms
==> Save the model at epoch 024 with test loss 0.0091410
Epoch: 025 | Loss: 0.0112030 | L2 loss: 0.0090467 | Lip loss: 0.0021563 | Grad norm: 0.385236 | Time: 6s828ms
Epoch: 025 | Test Loss: 0.0091404 | Time: 385ms
==> Save the model at epoch 025 with test loss 0.0091404
Epoch: 026 | Loss: 0.0111865 | L2 loss: 0.0090492 | Lip loss: 0.0021373 | Grad norm: 0.419971 | Time: 6s922ms
Epoch: 026 | Test Loss: 0.0091404 | Time: 375ms
==> Save the model at epoch 026 with test loss 0.0091404
Epoch: 027 | Loss: 0.0112061 | L2 loss: 0.0090491 | Lip loss: 0.0021570 | Grad norm: 0.409159 | Time: 6s892ms
Epoch: 027 | Test Loss: 0.0091404 | Time: 373ms
==> Save the model at epoch 027 with test loss 0.0091404
Epoch: 028 | Loss: 0.0112087 | L2 loss: 0.0090504 | Lip loss: 0.0021583 | Grad norm: 0.402183 | Time: 6s918ms
Epoch: 028 | Test Loss: 0.0091404 | Time: 375ms
==> Save the model at epoch 028 with test loss 0.0091404
Epoch: 029 | Loss: 0.0111682 | L2 loss: 0.0090173 | Lip loss: 0.0021510 | Grad norm: 0.387804 | Time: 6s737ms
Epoch: 029 | Test Loss: 0.0091404 | Time: 373ms
==> Save the model at epoch 029 with test loss 0.0091404
Epoch: 030 | Loss: 0.0112502 | L2 loss: 0.0090791 | Lip loss: 0.0021712 | Grad norm: 0.409931 | Time: 6s627ms
Epoch: 030 | Test Loss: 0.0091404 | Time: 438ms
==> Save the model at epoch 030 with test loss 0.0091404
Epoch: 031 | Loss: 0.0111861 | L2 loss: 0.0090311 | Lip loss: 0.0021549 | Grad norm: 0.378258 | Time: 6s908ms
Epoch: 031 | Test Loss: 0.0091404 | Time: 381ms
Epoch: 032 | Loss: 0.0112161 | L2 loss: 0.0090553 | Lip loss: 0.0021608 | Grad norm: 0.411561 | Time: 6s896ms
Epoch: 032 | Test Loss: 0.0091404 | Time: 379ms
Epoch: 033 | Loss: 0.0112365 | L2 loss: 0.0090492 | Lip loss: 0.0021873 | Grad norm: 0.385796 | Time: 6s933ms
Epoch: 033 | Test Loss: 0.0091404 | Time: 370ms
Epoch: 034 | Loss: 0.0111841 | L2 loss: 0.0090218 | Lip loss: 0.0021622 | Grad norm: 0.399556 | Time: 6s870ms
Epoch: 034 | Test Loss: 0.0091404 | Time: 373ms
Epoch: 035 | Loss: 0.0112058 | L2 loss: 0.0090423 | Lip loss: 0.0021635 | Grad norm: 0.380797 | Time: 6s805ms
Epoch: 035 | Test Loss: 0.0091404 | Time: 383ms
Epoch: 036 | Loss: 0.0112408 | L2 loss: 0.0090890 | Lip loss: 0.0021517 | Grad norm: 0.400988 | Time: 6s790ms
Epoch: 036 | Test Loss: 0.0091404 | Time: 374ms
Epoch: 037 | Loss: 0.0112309 | L2 loss: 0.0090631 | Lip loss: 0.0021678 | Grad norm: 0.392838 | Time: 6s835ms
Epoch: 037 | Test Loss: 0.0091404 | Time: 370ms
Epoch: 038 | Loss: 0.0112103 | L2 loss: 0.0090372 | Lip loss: 0.0021731 | Grad norm: 0.385570 | Time: 6s743ms
Epoch: 038 | Test Loss: 0.0091404 | Time: 374ms
Epoch: 039 | Loss: 0.0111854 | L2 loss: 0.0090462 | Lip loss: 0.0021392 | Grad norm: 0.380155 | Time: 6s862ms
Epoch: 039 | Test Loss: 0.0091404 | Time: 394ms
Epoch: 040 | Loss: 0.0111983 | L2 loss: 0.0090437 | Lip loss: 0.0021545 | Grad norm: 0.369605 | Time: 6s748ms
Epoch: 040 | Test Loss: 0.0091404 | Time: 369ms
Total time: 4m51s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[1. 1.]
==> Output transform to be applied to the neural network (trained):
[1. 1.]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Drawing l2 loss...
==> Drawing lip loss...
==> Process finished.
