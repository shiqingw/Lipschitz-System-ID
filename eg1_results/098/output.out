==> torch device:  cuda:1
==> Test-Train split: test_ratio = 0.20
==> Test-Train split seed:  None
==> Further split: further_train_ratio = 0.25
==> Further split seed:  None
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.70982397 0.70987433]
==> Ouput transform to be applied to the neural network:
[2.8319 2.8318]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─Linear: 2-1                       [1, 64]                   192
│    └─ReLU: 2-2                         [1, 64]                   --
│    └─Linear: 2-3                       [1, 64]                   4,160
│    └─ReLU: 2-4                         [1, 64]                   --
│    └─Linear: 2-5                       [1, 64]                   4,160
│    └─ReLU: 2-6                         [1, 64]                   --
│    └─Linear: 2-7                       [1, 64]                   4,160
│    └─ReLU: 2-8                         [1, 64]                   --
│    └─Linear: 2-9                       [1, 64]                   4,160
│    └─ReLU: 2-10                        [1, 64]                   --
│    └─Linear: 2-11                      [1, 64]                   4,160
│    └─ReLU: 2-12                        [1, 64]                   --
│    └─Linear: 2-13                      [1, 64]                   4,160
│    └─ReLU: 2-14                        [1, 64]                   --
│    └─Linear: 2-15                      [1, 2]                    130
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─Linear: 2-16                      [1, 64]                   (recursive)
│    └─ReLU: 2-17                        [1, 64]                   --
│    └─Linear: 2-18                      [1, 64]                   (recursive)
│    └─ReLU: 2-19                        [1, 64]                   --
│    └─Linear: 2-20                      [1, 64]                   (recursive)
│    └─ReLU: 2-21                        [1, 64]                   --
│    └─Linear: 2-22                      [1, 64]                   (recursive)
│    └─ReLU: 2-23                        [1, 64]                   --
│    └─Linear: 2-24                      [1, 64]                   (recursive)
│    └─ReLU: 2-25                        [1, 64]                   --
│    └─Linear: 2-26                      [1, 64]                   (recursive)
│    └─ReLU: 2-27                        [1, 64]                   --
│    └─Linear: 2-28                      [1, 64]                   (recursive)
│    └─ReLU: 2-29                        [1, 64]                   --
│    └─Linear: 2-30                      [1, 2]                    (recursive)
==========================================================================================
Total params: 25,288
Trainable params: 25,282
Non-trainable params: 6
Total mult-adds (Units.MEGABYTES): 0.05
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.10
Estimated Total Size (MB): 0.11
==========================================================================================
==> Saving initial model weights...
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 15.8972881 | Grad norm: 0.336606 | Time: 1s225ms
Epoch: 001 | Test Loss: 15.8376202 | Time: 469ms
==> Save the model at epoch 001 with test loss 15.8376202
Epoch: 002 | Train Loss: 3.9786082 | Grad norm: 6.844533 | Time: 1s12ms
Epoch: 002 | Test Loss: 0.0128314 | Time: 452ms
==> Save the model at epoch 002 with test loss 0.0128314
Epoch: 003 | Train Loss: 0.0415932 | Grad norm: 2.806084 | Time: 1s27ms
Epoch: 003 | Test Loss: 0.0211336 | Time: 539ms
Epoch: 004 | Train Loss: 0.0420684 | Grad norm: 2.764768 | Time: 1s74ms
Epoch: 004 | Test Loss: 0.0364255 | Time: 526ms
Epoch: 005 | Train Loss: 0.0616480 | Grad norm: 3.163947 | Time: 1s119ms
Epoch: 005 | Test Loss: 0.0135553 | Time: 459ms
Epoch: 006 | Train Loss: 0.0921580 | Grad norm: 3.786581 | Time: 958ms
Epoch: 006 | Test Loss: 0.1704147 | Time: 457ms
Epoch: 007 | Train Loss: 0.1187865 | Grad norm: 4.819508 | Time: 1s26ms
Epoch: 007 | Test Loss: 0.0895552 | Time: 521ms
Epoch: 008 | Train Loss: 0.0963248 | Grad norm: 4.214602 | Time: 1s43ms
Epoch: 008 | Test Loss: 0.1220736 | Time: 467ms
Epoch: 009 | Train Loss: 0.1405114 | Grad norm: 5.302535 | Time: 1s57ms
Epoch: 009 | Test Loss: 0.0987797 | Time: 461ms
Epoch: 010 | Train Loss: 0.0920821 | Grad norm: 4.243877 | Time: 1s19ms
Epoch: 010 | Test Loss: 0.0620790 | Time: 454ms
Epoch: 011 | Train Loss: 0.0888082 | Grad norm: 4.305933 | Time: 965ms
Epoch: 011 | Test Loss: 0.2494749 | Time: 527ms
Epoch: 012 | Train Loss: 0.1066546 | Grad norm: 4.886237 | Time: 992ms
Epoch: 012 | Test Loss: 0.0757058 | Time: 454ms
Epoch: 013 | Train Loss: 0.0774833 | Grad norm: 4.245813 | Time: 1s85ms
Epoch: 013 | Test Loss: 0.1150956 | Time: 457ms
Epoch: 014 | Train Loss: 0.0608831 | Grad norm: 3.766318 | Time: 981ms
Epoch: 014 | Test Loss: 0.0713074 | Time: 455ms
Epoch: 015 | Train Loss: 0.0729166 | Grad norm: 4.124440 | Time: 964ms
Epoch: 015 | Test Loss: 0.0393723 | Time: 528ms
Epoch: 016 | Train Loss: 0.0600861 | Grad norm: 3.517235 | Time: 945ms
Epoch: 016 | Test Loss: 0.0951142 | Time: 467ms
Epoch: 017 | Train Loss: 0.0314906 | Grad norm: 2.608239 | Time: 1s19ms
Epoch: 017 | Test Loss: 0.0166875 | Time: 453ms
Epoch: 018 | Train Loss: 0.0316151 | Grad norm: 2.707104 | Time: 1s5ms
Epoch: 018 | Test Loss: 0.0484793 | Time: 454ms
Epoch: 019 | Train Loss: 0.0332942 | Grad norm: 2.801240 | Time: 1s99ms
Epoch: 019 | Test Loss: 0.0556299 | Time: 521ms
Epoch: 020 | Train Loss: 0.0266845 | Grad norm: 2.412259 | Time: 1s32ms
Epoch: 020 | Test Loss: 0.0989473 | Time: 453ms
Epoch: 021 | Train Loss: 0.0327917 | Grad norm: 2.711021 | Time: 958ms
Epoch: 021 | Test Loss: 0.0153442 | Time: 471ms
Epoch: 022 | Train Loss: 0.0239861 | Grad norm: 2.327707 | Time: 1s70ms
Epoch: 022 | Test Loss: 0.0174992 | Time: 453ms
Epoch: 023 | Train Loss: 0.0133681 | Grad norm: 1.637452 | Time: 1s45ms
Epoch: 023 | Test Loss: 0.0185068 | Time: 524ms
Epoch: 024 | Train Loss: 0.0120150 | Grad norm: 1.418394 | Time: 978ms
Epoch: 024 | Test Loss: 0.0247850 | Time: 455ms
Epoch: 025 | Train Loss: 0.0102803 | Grad norm: 1.326149 | Time: 1s28ms
Epoch: 025 | Test Loss: 0.0086050 | Time: 480ms
==> Save the model at epoch 025 with test loss 0.0086050
Epoch: 026 | Train Loss: 0.0091588 | Grad norm: 1.269252 | Time: 952ms
Epoch: 026 | Test Loss: 0.0101904 | Time: 469ms
Epoch: 027 | Train Loss: 0.0075674 | Grad norm: 0.960227 | Time: 959ms
Epoch: 027 | Test Loss: 0.0121931 | Time: 526ms
Epoch: 028 | Train Loss: 0.0067289 | Grad norm: 1.078134 | Time: 985ms
Epoch: 028 | Test Loss: 0.0066368 | Time: 468ms
==> Save the model at epoch 028 with test loss 0.0066368
Epoch: 029 | Train Loss: 0.0055590 | Grad norm: 0.773511 | Time: 928ms
Epoch: 029 | Test Loss: 0.0073055 | Time: 453ms
Epoch: 030 | Train Loss: 0.0046222 | Grad norm: 0.518467 | Time: 998ms
Epoch: 030 | Test Loss: 0.0044969 | Time: 463ms
==> Save the model at epoch 030 with test loss 0.0044969
Epoch: 031 | Train Loss: 0.0038546 | Grad norm: 0.380267 | Time: 1s7ms
Epoch: 031 | Test Loss: 0.0056884 | Time: 588ms
Epoch: 032 | Train Loss: 0.0044394 | Grad norm: 0.511604 | Time: 1s99ms
Epoch: 032 | Test Loss: 0.0049026 | Time: 457ms
Epoch: 033 | Train Loss: 0.0040528 | Grad norm: 0.368309 | Time: 1s2ms
Epoch: 033 | Test Loss: 0.0046195 | Time: 470ms
Epoch: 034 | Train Loss: 0.0038957 | Grad norm: 0.318437 | Time: 1s123ms
Epoch: 034 | Test Loss: 0.0039074 | Time: 455ms
==> Save the model at epoch 034 with test loss 0.0039074
Epoch: 035 | Train Loss: 0.0037181 | Grad norm: 0.372725 | Time: 1s38ms
Epoch: 035 | Test Loss: 0.0042761 | Time: 541ms
Epoch: 036 | Train Loss: 0.0036683 | Grad norm: 0.255317 | Time: 1s63ms
Epoch: 036 | Test Loss: 0.0046523 | Time: 454ms
Epoch: 037 | Train Loss: 0.0037610 | Grad norm: 0.303318 | Time: 1s14ms
Epoch: 037 | Test Loss: 0.0043358 | Time: 454ms
Epoch: 038 | Train Loss: 0.0036433 | Grad norm: 0.216059 | Time: 1s27ms
Epoch: 038 | Test Loss: 0.0044765 | Time: 452ms
Epoch: 039 | Train Loss: 0.0034967 | Grad norm: 0.170511 | Time: 1s5ms
Epoch: 039 | Test Loss: 0.0042970 | Time: 544ms
Epoch: 040 | Train Loss: 0.0034845 | Grad norm: 0.133771 | Time: 984ms
Epoch: 040 | Test Loss: 0.0043027 | Time: 455ms
Total time: 1m169ms
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.70982397 0.70987433]
==> Output transform to be applied to the neural network (trained):
[2.8319 2.8318]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
