==> torch device:  cuda:0
==> Lipschitz constant: 1.00
==> Input bias to be applied to the neural network:
[0. 0.]
==> Input transform to be applied to the neural network:
[0.5090095 0.5090095]
==> Ouput transform to be applied to the neural network:
[3.9366 3.9363]
==> Evaluating model...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
NeuralNetwork                            [1, 2]                    6
├─Sequential: 1-1                        [1, 2]                    --
│    └─SandwichFc: 2-1                   [1, 128]                  16,897
│    └─SandwichFc: 2-2                   [1, 128]                  33,025
│    └─SandwichFc: 2-3                   [1, 128]                  33,025
│    └─SandwichFc: 2-4                   [1, 128]                  33,025
│    └─SandwichFc: 2-5                   [1, 128]                  33,025
│    └─SandwichFc: 2-6                   [1, 128]                  33,025
│    └─SandwichFc: 2-7                   [1, 128]                  33,025
│    └─SandwichLin: 2-8                  [1, 2]                    263
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─SandwichFc: 2-9                   [1, 128]                  (recursive)
│    └─SandwichFc: 2-10                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-11                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-12                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-13                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-14                  [1, 128]                  (recursive)
│    └─SandwichFc: 2-15                  [1, 128]                  (recursive)
│    └─SandwichLin: 2-16                 [1, 2]                    (recursive)
==========================================================================================
Total params: 215,316
Trainable params: 215,310
Non-trainable params: 6
Total mult-adds (M): 0.43
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.86
Estimated Total Size (MB): 0.88
==========================================================================================
==> Saving initial model weights...
==> Test-Train split: test_ratio = 0.20
==> Further split: further_train_ratio = 1.00
==> Start training...
==> Number of param_groups in optimizer: 1
Epoch: 001 | Train Loss: 30.9722701 | Grad norm: 7.665089 | Time: 21s918ms
Epoch: 001 | Test Loss: 31.0111956 | Time: 782ms
==> Save the model at epoch 001 with test loss 31.0111956
Epoch: 002 | Train Loss: 6.3853506 | Grad norm: 12.271258 | Time: 21s948ms
Epoch: 002 | Test Loss: 3.0614422 | Time: 763ms
==> Save the model at epoch 002 with test loss 3.0614422
Epoch: 003 | Train Loss: 1.6319980 | Grad norm: 7.819825 | Time: 17s514ms
Epoch: 003 | Test Loss: 0.8376076 | Time: 782ms
==> Save the model at epoch 003 with test loss 0.8376076
Epoch: 004 | Train Loss: 0.5306479 | Grad norm: 9.641097 | Time: 17s824ms
Epoch: 004 | Test Loss: 0.3180935 | Time: 787ms
==> Save the model at epoch 004 with test loss 0.3180935
Epoch: 005 | Train Loss: 0.2269573 | Grad norm: 10.408798 | Time: 17s937ms
Epoch: 005 | Test Loss: 0.1580010 | Time: 774ms
==> Save the model at epoch 005 with test loss 0.1580010
Epoch: 006 | Train Loss: 0.1248659 | Grad norm: 10.851734 | Time: 17s957ms
Epoch: 006 | Test Loss: 0.0902755 | Time: 755ms
==> Save the model at epoch 006 with test loss 0.0902755
Epoch: 007 | Train Loss: 0.0848243 | Grad norm: 10.835872 | Time: 17s346ms
Epoch: 007 | Test Loss: 0.0734578 | Time: 757ms
==> Save the model at epoch 007 with test loss 0.0734578
Epoch: 008 | Train Loss: 0.0661575 | Grad norm: 10.504433 | Time: 17s777ms
Epoch: 008 | Test Loss: 0.0516911 | Time: 793ms
==> Save the model at epoch 008 with test loss 0.0516911
Epoch: 009 | Train Loss: 0.0552106 | Grad norm: 9.901739 | Time: 17s362ms
Epoch: 009 | Test Loss: 0.0633820 | Time: 789ms
Epoch: 010 | Train Loss: 0.0483231 | Grad norm: 9.237067 | Time: 17s144ms
Epoch: 010 | Test Loss: 0.0309238 | Time: 792ms
==> Save the model at epoch 010 with test loss 0.0309238
Epoch: 011 | Train Loss: 0.0435649 | Grad norm: 8.487642 | Time: 18s70ms
Epoch: 011 | Test Loss: 0.0225789 | Time: 790ms
==> Save the model at epoch 011 with test loss 0.0225789
Epoch: 012 | Train Loss: 0.0389388 | Grad norm: 7.171881 | Time: 17s881ms
Epoch: 012 | Test Loss: 0.0555755 | Time: 756ms
Epoch: 013 | Train Loss: 0.0323925 | Grad norm: 6.211593 | Time: 17s578ms
Epoch: 013 | Test Loss: 0.0358940 | Time: 758ms
Epoch: 014 | Train Loss: 0.0265752 | Grad norm: 5.460171 | Time: 17s820ms
Epoch: 014 | Test Loss: 0.0195436 | Time: 786ms
==> Save the model at epoch 014 with test loss 0.0195436
Epoch: 015 | Train Loss: 0.0311873 | Grad norm: 5.261848 | Time: 20s436ms
Epoch: 015 | Test Loss: 0.0375168 | Time: 793ms
Epoch: 016 | Train Loss: 0.0206587 | Grad norm: 4.286879 | Time: 20s441ms
Epoch: 016 | Test Loss: 0.0147332 | Time: 767ms
==> Save the model at epoch 016 with test loss 0.0147332
Epoch: 017 | Train Loss: 0.0168823 | Grad norm: 3.821537 | Time: 21s711ms
Epoch: 017 | Test Loss: 0.0095067 | Time: 1s42ms
==> Save the model at epoch 017 with test loss 0.0095067
Epoch: 018 | Train Loss: 0.0162715 | Grad norm: 3.498006 | Time: 21s275ms
Epoch: 018 | Test Loss: 0.0096501 | Time: 813ms
Epoch: 019 | Train Loss: 0.0150796 | Grad norm: 3.206989 | Time: 20s536ms
Epoch: 019 | Test Loss: 0.0126069 | Time: 794ms
Epoch: 020 | Train Loss: 0.0133034 | Grad norm: 2.952371 | Time: 19s998ms
Epoch: 020 | Test Loss: 0.0176606 | Time: 801ms
Epoch: 021 | Train Loss: 0.0131189 | Grad norm: 2.712512 | Time: 19s535ms
Epoch: 021 | Test Loss: 0.0110878 | Time: 787ms
Epoch: 022 | Train Loss: 0.0117534 | Grad norm: 2.520752 | Time: 17s844ms
Epoch: 022 | Test Loss: 0.0109962 | Time: 789ms
Epoch: 023 | Train Loss: 0.0110725 | Grad norm: 2.304732 | Time: 20s35ms
Epoch: 023 | Test Loss: 0.0108596 | Time: 749ms
Epoch: 024 | Train Loss: 0.0107799 | Grad norm: 2.165904 | Time: 20s854ms
Epoch: 024 | Test Loss: 0.0107173 | Time: 753ms
Epoch: 025 | Train Loss: 0.0101720 | Grad norm: 1.987222 | Time: 20s590ms
Epoch: 025 | Test Loss: 0.0102807 | Time: 757ms
Epoch: 026 | Train Loss: 0.0097688 | Grad norm: 1.833031 | Time: 21s127ms
Epoch: 026 | Test Loss: 0.0101770 | Time: 768ms
Epoch: 027 | Train Loss: 0.0097889 | Grad norm: 1.763256 | Time: 17s306ms
Epoch: 027 | Test Loss: 0.0113411 | Time: 759ms
Epoch: 028 | Train Loss: 0.0112178 | Grad norm: 1.780152 | Time: 17s86ms
Epoch: 028 | Test Loss: 0.0076119 | Time: 759ms
==> Save the model at epoch 028 with test loss 0.0076119
Epoch: 029 | Train Loss: 0.0129058 | Grad norm: 1.738953 | Time: 17s621ms
Epoch: 029 | Test Loss: 0.0079817 | Time: 777ms
Epoch: 030 | Train Loss: 0.0085018 | Grad norm: 0.977115 | Time: 17s112ms
Epoch: 030 | Test Loss: 0.0073993 | Time: 752ms
==> Save the model at epoch 030 with test loss 0.0073993
Epoch: 031 | Train Loss: 0.0098976 | Grad norm: 1.464662 | Time: 18s81ms
Epoch: 031 | Test Loss: 0.0138038 | Time: 792ms
Epoch: 032 | Train Loss: 0.0099313 | Grad norm: 1.473991 | Time: 20s842ms
Epoch: 032 | Test Loss: 0.0113851 | Time: 754ms
Epoch: 033 | Train Loss: 0.0084114 | Grad norm: 0.907634 | Time: 21s584ms
Epoch: 033 | Test Loss: 0.0090497 | Time: 773ms
Epoch: 034 | Train Loss: 0.0078708 | Grad norm: 0.760391 | Time: 20s502ms
Epoch: 034 | Test Loss: 0.0086318 | Time: 856ms
Epoch: 035 | Train Loss: 0.0078620 | Grad norm: 0.784632 | Time: 20s347ms
Epoch: 035 | Test Loss: 0.0076048 | Time: 764ms
Epoch: 036 | Train Loss: 0.0074982 | Grad norm: 0.513342 | Time: 20s135ms
Epoch: 036 | Test Loss: 0.0074927 | Time: 757ms
Epoch: 037 | Train Loss: 0.0083294 | Grad norm: 0.833985 | Time: 20s627ms
Epoch: 037 | Test Loss: 0.0089952 | Time: 855ms
Epoch: 038 | Train Loss: 0.0075519 | Grad norm: 0.564697 | Time: 21s123ms
Epoch: 038 | Test Loss: 0.0078589 | Time: 790ms
Epoch: 039 | Train Loss: 0.0074234 | Grad norm: 0.464496 | Time: 22s269ms
Epoch: 039 | Test Loss: 0.0074141 | Time: 787ms
Epoch: 040 | Train Loss: 0.0076403 | Grad norm: 0.585141 | Time: 20s391ms
Epoch: 040 | Test Loss: 0.0072424 | Time: 848ms
==> Save the model at epoch 040 with test loss 0.0072424
Epoch: 041 | Train Loss: 0.0073794 | Grad norm: 0.443743 | Time: 17s781ms
Epoch: 041 | Test Loss: 0.0075068 | Time: 757ms
Epoch: 042 | Train Loss: 0.0073211 | Grad norm: 0.378741 | Time: 17s448ms
Epoch: 042 | Test Loss: 0.0073409 | Time: 782ms
Epoch: 043 | Train Loss: 0.0073637 | Grad norm: 0.412462 | Time: 17s947ms
Epoch: 043 | Test Loss: 0.0074846 | Time: 817ms
Epoch: 044 | Train Loss: 0.0073945 | Grad norm: 0.448008 | Time: 17s617ms
Epoch: 044 | Test Loss: 0.0077089 | Time: 813ms
Epoch: 045 | Train Loss: 0.0073248 | Grad norm: 0.368758 | Time: 22s159ms
Epoch: 045 | Test Loss: 0.0074108 | Time: 811ms
Epoch: 046 | Train Loss: 0.0072803 | Grad norm: 0.343660 | Time: 20s115ms
Epoch: 046 | Test Loss: 0.0075448 | Time: 850ms
Epoch: 047 | Train Loss: 0.0072397 | Grad norm: 0.299928 | Time: 17s858ms
Epoch: 047 | Test Loss: 0.0073866 | Time: 791ms
Epoch: 048 | Train Loss: 0.0073201 | Grad norm: 0.376376 | Time: 21s333ms
Epoch: 048 | Test Loss: 0.0072688 | Time: 788ms
Epoch: 049 | Train Loss: 0.0072412 | Grad norm: 0.295177 | Time: 17s907ms
Epoch: 049 | Test Loss: 0.0072740 | Time: 851ms
Epoch: 050 | Train Loss: 0.0072122 | Grad norm: 0.250252 | Time: 18s624ms
Epoch: 050 | Test Loss: 0.0072591 | Time: 784ms
Epoch: 051 | Train Loss: 0.0072242 | Grad norm: 0.269737 | Time: 18s
Epoch: 051 | Test Loss: 0.0075431 | Time: 760ms
Epoch: 052 | Train Loss: 0.0071967 | Grad norm: 0.230007 | Time: 18s202ms
Epoch: 052 | Test Loss: 0.0072351 | Time: 886ms
==> Save the model at epoch 052 with test loss 0.0072351
Epoch: 053 | Train Loss: 0.0071787 | Grad norm: 0.206100 | Time: 18s9ms
Epoch: 053 | Test Loss: 0.0073086 | Time: 789ms
Epoch: 054 | Train Loss: 0.0071856 | Grad norm: 0.224816 | Time: 18s585ms
Epoch: 054 | Test Loss: 0.0072433 | Time: 789ms
Epoch: 055 | Train Loss: 0.0071568 | Grad norm: 0.168246 | Time: 18s164ms
Epoch: 055 | Test Loss: 0.0073094 | Time: 792ms
Epoch: 056 | Train Loss: 0.0071541 | Grad norm: 0.168785 | Time: 18s79ms
Epoch: 056 | Test Loss: 0.0072376 | Time: 764ms
Epoch: 057 | Train Loss: 0.0071456 | Grad norm: 0.145959 | Time: 17s734ms
Epoch: 057 | Test Loss: 0.0072589 | Time: 788ms
Epoch: 058 | Train Loss: 0.0071420 | Grad norm: 0.143069 | Time: 17s81ms
Epoch: 058 | Test Loss: 0.0072356 | Time: 757ms
Epoch: 059 | Train Loss: 0.0071362 | Grad norm: 0.120228 | Time: 20s65ms
Epoch: 059 | Test Loss: 0.0072348 | Time: 790ms
==> Save the model at epoch 059 with test loss 0.0072348
Epoch: 060 | Train Loss: 0.0071324 | Grad norm: 0.109495 | Time: 21s810ms
Epoch: 060 | Test Loss: 0.0072347 | Time: 791ms
==> Save the model at epoch 060 with test loss 0.0072347
Total time: 19m57s
==> Saving trained model weights...
==> Input bias to be applied to the neural network (trained):
[0. 0.]
==> Input transform to be applied to the neural network (trained):
[0.5090095 0.5090095]
==> Output transform to be applied to the neural network (trained):
[3.9366 3.9363]
==> Saving training info...
==> Drawing training loss...
==> Drawing grad norm...
==> Drawing testing loss...
==> Process finished.
